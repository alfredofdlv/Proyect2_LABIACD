{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stock Portfolio Forecasting and Optimization on S&P500 Using Machine Learning and Search Methods**\n",
    "<img src=\"images/pr.jpg\" style=\"width:100%; height:100%;\">\n",
    "\n",
    "## **Abstract**  \n",
    "Stock market forecasting and portfolio optimization are pivotal challenges in financial decision-making, driven by the volatility and complexity of markets like the S&P500. This project aims to develop an efficient system that predicts stock price trends and optimizes investment portfolios, leveraging historical S&P500 data from 2010 to 2023.  \n",
    "\n",
    "The solution integrates traditional Machine Learning algorithms (e.g., Random Forest, SVM) for accurate stock price prediction. Furthermore, optimization techniques such as Monte Carlo simulations and genetic algorithms are employed to construct portfolios that maximize returns or minimize risks over a short-term investment horizon.  \n",
    "\n",
    "By combining predictive analytics with optimization methodologies, this work provides insight for data-driven financial decision-making, promoting effective investment strategies. Ethical considerations and regulatory implications are also addressed, ensuring responsible and practical application in real-world financial contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "### **Context of the Problem**\n",
    "The stock market is inherently volatile and complex, making it challenging for investors to forecast stock price trends and optimize investment decisions. Within the S&P500 index, the ability to predict stock movements and construct efficient portfolios is crucial for maximizing returns while managing risks. Traditional methods often fall short in addressing the rapid fluctuations and interdependencies inherent in financial markets, necessitating data-driven, adaptive solutions.\n",
    "\n",
    "Machine Learning (ML) and Deep Learning (DL) have shown significant potential in addressing these challenges by analyzing large historical datasets, identifying patterns, and predicting trends with improved accuracy. Additionally, portfolio optimization techniques, such as Monte Carlo simulations and genetic algorithms, can help select investments that maximize returns or minimize risks based on forecasted trends. Combining these approaches can streamline decision-making, reduce human bias, and enhance financial performance.\n",
    "\n",
    "### **Objective of the work**\n",
    "This project aims to develop a robust framework for predicting stock price trends and optimizing investment portfolios within the S&P500 index. By leveraging ML algorithms models, the objective is to build accurate forecasting systems. These predictions will feed into optimization techniques to create well-balanced investment portfolios over a one-month horizon.\n",
    "\n",
    "### **Structure of the work**\n",
    "This notebook is structured as follows:\n",
    "- Data Acquisition and EDA : We will collect historical price data for all S&P500 stocks from 2010 to January 2024 to build predictive models. Additionally, we will extract relevant financial indices such as moving averages, volatility, and trading volume, which can provide valuable insights into market trends. An Exploratory Data Analysis (EDA) will be conducted to study correlations, detect trends, and evaluate patterns within the data for the predictive modeling process.\n",
    "\n",
    "- Stock Market Forecasting : This section focuses on developing multi-output forecasting models using traditional Machine Learning techniques such as Random Forest and Support Vector Machines (SVM). These models aim to predict the daily prices of all 500 stocks within the S&P500 index. The performance of each model will be compared using regression metrics such as Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) to determine their effectiveness.\n",
    "\n",
    "- Model Hyperparameter Tuning : After identifying the most promising models, we will perform hyperparameter tuning to enhance their predictive accuracy further. Techniques such as grid search and randomized search will be employed, targeting optimal configurations for regression metrics. \n",
    "\n",
    "- Stock Portfolio Optimization: Building on the forecasting outputs, we will employ advanced optimization techniques, including Monte Carlo simulations and genetic algorithms, to construct efficient investment portfolios. The Monte Carlo simulations will model various potential scenarios to evaluate risk and return distributions, while genetic algorithms will optimize portfolio allocation by iteratively evolving toward the most profitable or least risky configurations. These methods will account for constraints such as diversification, transaction costs, and risk tolerance.\n",
    "\n",
    "- Discussion and Future Work :  Finally, we will analyze the results, discussing the strengths and limitations of our methodology. This section will provide insights into the practical applications of our approach and suggest potential refinements, such as incorporating alternative datasets (e.g., economic indicators or sentiment analysis) or exploring novel optimization algorithms. Future research directions will aim to enhance both predictive accuracy and portfolio optimization for real-world financial challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Decomposition\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "\n",
    "# Regressors\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, \n",
    "    ExtraTreesRegressor, BaggingRegressor, HistGradientBoostingRegressor\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, ARDRegression, \n",
    "    HuberRegressor, PassiveAggressiveRegressor, TheilSenRegressor, SGDRegressor, \n",
    "    PoissonRegressor, TweedieRegressor\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error,make_scorer\n",
    ")\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "\n",
    "from utils import (\n",
    "    extract_sp500_companies, extract_stock, process_data, \n",
    "    join_stock_data, join_macro, join_technical_indicators,mean_positive_error,\n",
    "    transformar_a_tensor_3d, crear_ventanas_temporales\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Acquisition**\n",
    "As mentioned above, we will extract historical price data for all **current S&P500 stocks**. The list of stocks will be sourced from the S&P500 Wikipedia page, ensuring it reflects the most up-to-date composition of the index. Subsequently, we will utilize the Yahoo Finance API to retrieve the historical data for these stocks, covering the period from 2010 to January 2024. This dataset will include essential information such as daily opening and closing prices, trading volumes, and additional technical indicators. These data points will serve as the foundation for both the forecasting models and the portfolio optimization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting SP500 Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Símbolo</th>\n",
       "      <th>Seguridad</th>\n",
       "      <th>Presentación ante la SEC</th>\n",
       "      <th>Sector GICS</th>\n",
       "      <th>Sub-industria GICS</th>\n",
       "      <th>Ubicación de la sede</th>\n",
       "      <th>Fecha de incorporación</th>\n",
       "      <th>Clave de índice central</th>\n",
       "      <th>Fundada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>reports</td>\n",
       "      <td>Industriales</td>\n",
       "      <td>Conglomerados Industriales</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1976-08-09</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>reports</td>\n",
       "      <td>Industriales</td>\n",
       "      <td>Productos de Edificio</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>91142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>reports</td>\n",
       "      <td>Cuidado de Salud</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1964-03-31</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A123</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>reports</td>\n",
       "      <td>Cuidado de Salud</td>\n",
       "      <td>Farmacéuticos</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>reports</td>\n",
       "      <td>Tecnología Informática</td>\n",
       "      <td>Consultoría Informática y Otros Servicios</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Símbolo    Seguridad Presentación ante la SEC             Sector GICS  \\\n",
       "0     MMM           3M                  reports            Industriales   \n",
       "1     AOS  A. O. Smith                  reports            Industriales   \n",
       "2     ABT       Abbott                  reports        Cuidado de Salud   \n",
       "3    A123       AbbVie                  reports        Cuidado de Salud   \n",
       "4     ACN    Accenture                  reports  Tecnología Informática   \n",
       "\n",
       "                          Sub-industria GICS     Ubicación de la sede  \\\n",
       "0                 Conglomerados Industriales    Saint Paul, Minnesota   \n",
       "1                      Productos de Edificio     Milwaukee, Wisconsin   \n",
       "2                      Health Care Equipment  North Chicago, Illinois   \n",
       "3                              Farmacéuticos  North Chicago, Illinois   \n",
       "4  Consultoría Informática y Otros Servicios          Dublin, Ireland   \n",
       "\n",
       "  Fecha de incorporación  Clave de índice central      Fundada  \n",
       "0             1976-08-09                    66740         1902  \n",
       "1             2017-07-26                    91142         1916  \n",
       "2             1964-03-31                     1800         1888  \n",
       "3             2012-12-31                  1551152  2013 (1888)  \n",
       "4             2011-07-06                  1467373         1989  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL de la página de Wikipedia\n",
    "df = extract_sp500_companies()\n",
    "companies = df['Símbolo']\n",
    "df.head(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['A123']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for A123.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['ATVI']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for ATVI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['ABC']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for ABC.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['BRK.B']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for BRK.B.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['CDAY']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for CDAY.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['DISH']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for DISH.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['RE']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for RE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['FRC']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for FRC.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['FLT']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for FLT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['PEAK']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for PEAK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['PKI']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for PKI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['PXD']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for PXD.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['SBNY']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2010-01-01 -> 2024-01-31) (Yahoo error = \"Data doesn\\'t exist for startDate = 1262322000, endDate = 1706677200\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for SBNY.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['SIVB']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2010-01-01 -> 2024-01-31) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for SIVB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['WRK']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for WRK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AdjustedClose</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-01 00:00:00+00:00</td>\n",
       "      <td>28.409882</td>\n",
       "      <td>31.010000</td>\n",
       "      <td>31.74</td>\n",
       "      <td>30.469999</td>\n",
       "      <td>31.50</td>\n",
       "      <td>66789100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-04 00:00:00+00:00</td>\n",
       "      <td>28.419044</td>\n",
       "      <td>31.020000</td>\n",
       "      <td>31.99</td>\n",
       "      <td>30.760000</td>\n",
       "      <td>31.09</td>\n",
       "      <td>7695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-05 00:00:00+00:00</td>\n",
       "      <td>28.437363</td>\n",
       "      <td>31.040001</td>\n",
       "      <td>31.98</td>\n",
       "      <td>30.850000</td>\n",
       "      <td>31.25</td>\n",
       "      <td>5013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-06 00:00:00+00:00</td>\n",
       "      <td>28.428202</td>\n",
       "      <td>31.030001</td>\n",
       "      <td>31.43</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>30.98</td>\n",
       "      <td>2126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-07 00:00:00+00:00</td>\n",
       "      <td>29.316872</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.73</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3800800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date  AdjustedClose      Close   High        Low  \\\n",
       "0 2013-02-01 00:00:00+00:00      28.409882  31.010000  31.74  30.469999   \n",
       "1 2013-02-04 00:00:00+00:00      28.419044  31.020000  31.99  30.760000   \n",
       "2 2013-02-05 00:00:00+00:00      28.437363  31.040001  31.98  30.850000   \n",
       "3 2013-02-06 00:00:00+00:00      28.428202  31.030001  31.43  30.750000   \n",
       "4 2013-02-07 00:00:00+00:00      29.316872  32.000000  32.73  31.000000   \n",
       "\n",
       "    Open    Volume  \n",
       "0  31.50  66789100  \n",
       "1  31.09   7695400  \n",
       "2  31.25   5013200  \n",
       "3  30.98   2126100  \n",
       "4  31.00   3800800  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for stock in companies : \n",
    "    raw_stock_data = extract_stock(stock)\n",
    "    if raw_stock_data is not None : \n",
    "        raw_stock_data.to_csv(f\"data/raw_stocks/{stock}.csv\")\n",
    "        stock_data = process_data(f\"data/raw_stocks/{stock}.csv\")\n",
    "        stock_data.to_csv(f\"data/stocks/{stock}.csv\")\n",
    "    \n",
    "stock_data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Files Retrieved : 500, Processed Files : 500\n",
      "Companies without data : ['A123', 'ATVI', 'ABC', 'BRK.B', 'CDAY', 'DISH', 'RE', 'FRC', 'FLT', 'PEAK', 'PKI', 'PXD', 'SBNY', 'SIVB', 'WRK']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Raw Files Retrieved : {len(os.listdir('data/raw_stocks/'))}, Processed Files : {len(os.listdir('data/stocks/'))}\")\n",
    "raw_folder = \"data/raw_stocks/\"\n",
    "raw_files = [os.path.splitext(file)[0] for file in os.listdir(raw_folder) if file.endswith('.csv')]\n",
    "missing_companies = [company for company in companies if company not in raw_files]\n",
    "print(f\"Companies without data : {missing_companies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 15 companies have been identified as requiring the use of alternative tickers, as the existing tickers are no longer available on the Yahoo Finance website. The following challenges were encountered:\n",
    "\n",
    "- Berkshire Hathaway operates two distinct types of stocks, one targeting high-net-worth investors and another designed for those with more limited financial resources. Nevertheless, both types of stocks have been included in the dataset due to the critical importance of both stock classes to the company's financial structure.\n",
    "\n",
    "- In contrast, First Republic Bank ceased operations in May 2023 due to a banking crisis, and thus, it will not be included in the dataset. Similarly, SVB Financial Group experienced a banking crisis and was removed from the index in March 2023, so it will also be excluded. \n",
    "\n",
    "- EchoStar Corporation acquired DISH Network Corporation on December 31, 2023. Following this merger, DISH shares were converted into EchoStar shares, and the \"DISH\" ticker symbol was removed from S&P500 listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "from utils import process_data\n",
    "missing_companies_ticker=['ABBV','AIY.DE','COR','BRKA.VI','BRKB.VI','DAY','EG','CPAY','DOC','VST','BG','SWR.L']\n",
    "for missing_stock in missing_companies_ticker: \n",
    "    raw_stock_data = extract_stock(missing_stock)\n",
    "    if raw_stock_data is not None : \n",
    "        raw_stock_data.to_csv(f\"data/raw_stocks/{missing_stock}.csv\")\n",
    "        stock_data = process_data(f\"data/raw_stocks/{missing_stock}.csv\")\n",
    "        stock_data.to_csv(f\"data/stocks/{missing_stock}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merging Stock data and Macroeconomic data**\n",
    "\n",
    "After obtaining individual CSV files containing data for each stock, our next step was to construct a unified, comprehensive dataset. Initially, we considered an approach where each company’s data would be arranged in contiguous blocks, with repeated date entries for each firm and only six columns : *Date*, *Close*, *AdjustedClose*, *Open*, *High*, and *Volume* in the original_data dataframe. While this method would keep the dataset structure relatively simple, we identified several drawbacks. First, this vertical stacking of time-series data for each company could prevent the model from capturing important **inter-company relationships**. Moreover, implementing such an arrangement would complicate the programming process and significantly increase computational demands.\n",
    "\n",
    "Example of the vertical stacking : \n",
    "\n",
    "| Date       | Close  | AdjustedClose | Open   | High   | Volume   | Company |\n",
    "|------------|---------|--------------|--------|--------|----------|---------|\n",
    "| 2020-01-01 | 300.35 | 300.32       | 299.80 | 303.00 | 31,000   | AAPL    |\n",
    "| 2020-01-02 | 302.20 | 302.18       | 301.50 | 305.10 | 25,500   | AAPL    |\n",
    "| 2020-01-03 | 298.90 | 298.87       | 297.30 | 301.00 | 28,000   | AAPL    |\n",
    "| ...        | ...    | ...          | ...    | ...    | ...      | AAPL    |\n",
    "| 2020-01-01 | 150.10 | 150.08       | 149.70 | 151.50 | 20,000   | MSFT    |\n",
    "| 2020-01-02 | 152.50 | 152.45       | 151.90 | 153.00 | 22,000   | MSFT    |\n",
    "| 2020-01-03 | 149.90 | 149.85       | 149.00 | 150.80 | 18,500   | MSFT    |\n",
    "| ...        | ...    | ...          | ...    | ...    | ...      | MSFT    |\n",
    "\n",
    "\n",
    "To address these challenges, we opted to merge the CSV files for each stock **horizontally**, aligning their data by dates. In other words, we combined the datasets as additional columns rather than stacking them by rows. This horizontal integration enables the model to simultaneously analyze the time-series behavior of multiple stocks, thereby facilitating the discovery of cross-company correlations and interdependencies. By leveraging a richer information structure, the model is better positioned to uncover complex patterns and improve predictive accuracy. Ultimately, this approach not only enhances the model’s explanatory power but also reduces the computational overhead associated with more convoluted dataset configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\Proyect2_LABIACD\\utils.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_dataset.reset_index(inplace=True)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      2010-01-04\n",
       "1      2010-01-05\n",
       "2      2010-01-06\n",
       "3      2010-01-07\n",
       "4      2010-01-08\n",
       "          ...    \n",
       "3632   2024-01-24\n",
       "3633   2024-01-25\n",
       "3634   2024-01-26\n",
       "3635   2024-01-29\n",
       "3636   2024-01-30\n",
       "Name: Date, Length: 3637, dtype: datetime64[ns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_data     = join_stock_data(r\"data/stocks\",axis=True)\n",
    "original_data['Date'] = pd.to_datetime(original_data['Date']).dt.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AdjustedClose_A</th>\n",
       "      <th>Close_A</th>\n",
       "      <th>High_A</th>\n",
       "      <th>Low_A</th>\n",
       "      <th>Open_A</th>\n",
       "      <th>Volume_A</th>\n",
       "      <th>AdjustedClose_AAL</th>\n",
       "      <th>Close_AAL</th>\n",
       "      <th>High_AAL</th>\n",
       "      <th>...</th>\n",
       "      <th>^FTSE</th>\n",
       "      <th>CL=F</th>\n",
       "      <th>SI=F</th>\n",
       "      <th>GC=F</th>\n",
       "      <th>^HSI</th>\n",
       "      <th>NG=F</th>\n",
       "      <th>ZC=F</th>\n",
       "      <th>EURUSD=X</th>\n",
       "      <th>BTC-USD</th>\n",
       "      <th>HO=F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>20.053032</td>\n",
       "      <td>22.389128</td>\n",
       "      <td>22.625179</td>\n",
       "      <td>22.267525</td>\n",
       "      <td>22.453505</td>\n",
       "      <td>3815561.0</td>\n",
       "      <td>4.496876</td>\n",
       "      <td>4.77</td>\n",
       "      <td>4.94</td>\n",
       "      <td>...</td>\n",
       "      <td>5500.299805</td>\n",
       "      <td>81.510002</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>1117.699951</td>\n",
       "      <td>21823.279297</td>\n",
       "      <td>5.884</td>\n",
       "      <td>418.50</td>\n",
       "      <td>1.442398</td>\n",
       "      <td>457.334015</td>\n",
       "      <td>2.1905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>19.835201</td>\n",
       "      <td>22.145924</td>\n",
       "      <td>22.331903</td>\n",
       "      <td>22.002861</td>\n",
       "      <td>22.324751</td>\n",
       "      <td>4186031.0</td>\n",
       "      <td>5.005958</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.37</td>\n",
       "      <td>...</td>\n",
       "      <td>5522.500000</td>\n",
       "      <td>81.769997</td>\n",
       "      <td>17.781000</td>\n",
       "      <td>1118.099976</td>\n",
       "      <td>22279.580078</td>\n",
       "      <td>5.637</td>\n",
       "      <td>418.75</td>\n",
       "      <td>1.436596</td>\n",
       "      <td>457.334015</td>\n",
       "      <td>2.1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>19.764729</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>22.174536</td>\n",
       "      <td>22.002861</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>3243779.0</td>\n",
       "      <td>4.798555</td>\n",
       "      <td>5.09</td>\n",
       "      <td>5.38</td>\n",
       "      <td>...</td>\n",
       "      <td>5530.000000</td>\n",
       "      <td>83.180000</td>\n",
       "      <td>18.163000</td>\n",
       "      <td>1135.900024</td>\n",
       "      <td>22416.669922</td>\n",
       "      <td>6.009</td>\n",
       "      <td>421.75</td>\n",
       "      <td>1.440403</td>\n",
       "      <td>457.334015</td>\n",
       "      <td>2.2032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>19.739101</td>\n",
       "      <td>22.038628</td>\n",
       "      <td>22.045780</td>\n",
       "      <td>21.816881</td>\n",
       "      <td>22.017166</td>\n",
       "      <td>3095172.0</td>\n",
       "      <td>4.939965</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.43</td>\n",
       "      <td>...</td>\n",
       "      <td>5526.700195</td>\n",
       "      <td>82.660004</td>\n",
       "      <td>18.333000</td>\n",
       "      <td>1133.099976</td>\n",
       "      <td>22269.449219</td>\n",
       "      <td>5.806</td>\n",
       "      <td>417.50</td>\n",
       "      <td>1.431803</td>\n",
       "      <td>457.334015</td>\n",
       "      <td>2.1836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>19.732687</td>\n",
       "      <td>22.031473</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>21.745352</td>\n",
       "      <td>21.917025</td>\n",
       "      <td>3733918.0</td>\n",
       "      <td>4.845691</td>\n",
       "      <td>5.14</td>\n",
       "      <td>5.43</td>\n",
       "      <td>...</td>\n",
       "      <td>5534.200195</td>\n",
       "      <td>82.750000</td>\n",
       "      <td>18.458000</td>\n",
       "      <td>1138.199951</td>\n",
       "      <td>22296.750000</td>\n",
       "      <td>5.749</td>\n",
       "      <td>423.00</td>\n",
       "      <td>1.441109</td>\n",
       "      <td>457.334015</td>\n",
       "      <td>2.2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3632</th>\n",
       "      <td>2024-01-24</td>\n",
       "      <td>129.123932</td>\n",
       "      <td>129.779999</td>\n",
       "      <td>132.800003</td>\n",
       "      <td>129.429993</td>\n",
       "      <td>132.410004</td>\n",
       "      <td>1141900.0</td>\n",
       "      <td>13.930000</td>\n",
       "      <td>13.93</td>\n",
       "      <td>14.18</td>\n",
       "      <td>...</td>\n",
       "      <td>7527.700195</td>\n",
       "      <td>75.089996</td>\n",
       "      <td>22.754999</td>\n",
       "      <td>2013.900024</td>\n",
       "      <td>15899.870117</td>\n",
       "      <td>2.641</td>\n",
       "      <td>452.25</td>\n",
       "      <td>1.085788</td>\n",
       "      <td>40077.074219</td>\n",
       "      <td>2.6818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633</th>\n",
       "      <td>2024-01-25</td>\n",
       "      <td>129.223419</td>\n",
       "      <td>129.880005</td>\n",
       "      <td>131.429993</td>\n",
       "      <td>129.429993</td>\n",
       "      <td>131.110001</td>\n",
       "      <td>1120900.0</td>\n",
       "      <td>15.360000</td>\n",
       "      <td>15.36</td>\n",
       "      <td>15.46</td>\n",
       "      <td>...</td>\n",
       "      <td>7529.700195</td>\n",
       "      <td>77.360001</td>\n",
       "      <td>22.808001</td>\n",
       "      <td>2016.900024</td>\n",
       "      <td>16211.959961</td>\n",
       "      <td>2.571</td>\n",
       "      <td>451.75</td>\n",
       "      <td>1.088175</td>\n",
       "      <td>39933.808594</td>\n",
       "      <td>2.7954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>2024-01-26</td>\n",
       "      <td>130.327835</td>\n",
       "      <td>130.990005</td>\n",
       "      <td>134.229996</td>\n",
       "      <td>130.639999</td>\n",
       "      <td>133.710007</td>\n",
       "      <td>1319800.0</td>\n",
       "      <td>15.130000</td>\n",
       "      <td>15.13</td>\n",
       "      <td>15.84</td>\n",
       "      <td>...</td>\n",
       "      <td>7635.100098</td>\n",
       "      <td>78.010002</td>\n",
       "      <td>22.754999</td>\n",
       "      <td>2016.800049</td>\n",
       "      <td>15952.230469</td>\n",
       "      <td>2.712</td>\n",
       "      <td>446.25</td>\n",
       "      <td>1.084705</td>\n",
       "      <td>41816.871094</td>\n",
       "      <td>2.8434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635</th>\n",
       "      <td>2024-01-29</td>\n",
       "      <td>132.158524</td>\n",
       "      <td>132.830002</td>\n",
       "      <td>132.899994</td>\n",
       "      <td>131.279999</td>\n",
       "      <td>131.750000</td>\n",
       "      <td>1407200.0</td>\n",
       "      <td>14.940000</td>\n",
       "      <td>14.94</td>\n",
       "      <td>15.39</td>\n",
       "      <td>...</td>\n",
       "      <td>7632.700195</td>\n",
       "      <td>76.779999</td>\n",
       "      <td>23.138000</td>\n",
       "      <td>2025.199951</td>\n",
       "      <td>16077.240234</td>\n",
       "      <td>2.490</td>\n",
       "      <td>440.25</td>\n",
       "      <td>1.084352</td>\n",
       "      <td>43288.246094</td>\n",
       "      <td>2.8339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3636</th>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>133.601181</td>\n",
       "      <td>134.279999</td>\n",
       "      <td>134.520004</td>\n",
       "      <td>132.690002</td>\n",
       "      <td>132.830002</td>\n",
       "      <td>1428700.0</td>\n",
       "      <td>14.530000</td>\n",
       "      <td>14.53</td>\n",
       "      <td>14.88</td>\n",
       "      <td>...</td>\n",
       "      <td>7666.299805</td>\n",
       "      <td>77.820000</td>\n",
       "      <td>23.107000</td>\n",
       "      <td>2031.500000</td>\n",
       "      <td>15703.450195</td>\n",
       "      <td>2.077</td>\n",
       "      <td>447.75</td>\n",
       "      <td>1.083447</td>\n",
       "      <td>42952.609375</td>\n",
       "      <td>2.8068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3637 rows × 3031 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AdjustedClose_A     Close_A      High_A       Low_A  \\\n",
       "0     2010-01-04        20.053032   22.389128   22.625179   22.267525   \n",
       "1     2010-01-05        19.835201   22.145924   22.331903   22.002861   \n",
       "2     2010-01-06        19.764729   22.067240   22.174536   22.002861   \n",
       "3     2010-01-07        19.739101   22.038628   22.045780   21.816881   \n",
       "4     2010-01-08        19.732687   22.031473   22.067240   21.745352   \n",
       "...          ...              ...         ...         ...         ...   \n",
       "3632  2024-01-24       129.123932  129.779999  132.800003  129.429993   \n",
       "3633  2024-01-25       129.223419  129.880005  131.429993  129.429993   \n",
       "3634  2024-01-26       130.327835  130.990005  134.229996  130.639999   \n",
       "3635  2024-01-29       132.158524  132.830002  132.899994  131.279999   \n",
       "3636  2024-01-30       133.601181  134.279999  134.520004  132.690002   \n",
       "\n",
       "          Open_A   Volume_A  AdjustedClose_AAL  Close_AAL  High_AAL  ...  \\\n",
       "0      22.453505  3815561.0           4.496876       4.77      4.94  ...   \n",
       "1      22.324751  4186031.0           5.005958       5.31      5.37  ...   \n",
       "2      22.067240  3243779.0           4.798555       5.09      5.38  ...   \n",
       "3      22.017166  3095172.0           4.939965       5.24      5.43  ...   \n",
       "4      21.917025  3733918.0           4.845691       5.14      5.43  ...   \n",
       "...          ...        ...                ...        ...       ...  ...   \n",
       "3632  132.410004  1141900.0          13.930000      13.93     14.18  ...   \n",
       "3633  131.110001  1120900.0          15.360000      15.36     15.46  ...   \n",
       "3634  133.710007  1319800.0          15.130000      15.13     15.84  ...   \n",
       "3635  131.750000  1407200.0          14.940000      14.94     15.39  ...   \n",
       "3636  132.830002  1428700.0          14.530000      14.53     14.88  ...   \n",
       "\n",
       "            ^FTSE       CL=F       SI=F         GC=F          ^HSI   NG=F  \\\n",
       "0     5500.299805  81.510002  17.440001  1117.699951  21823.279297  5.884   \n",
       "1     5522.500000  81.769997  17.781000  1118.099976  22279.580078  5.637   \n",
       "2     5530.000000  83.180000  18.163000  1135.900024  22416.669922  6.009   \n",
       "3     5526.700195  82.660004  18.333000  1133.099976  22269.449219  5.806   \n",
       "4     5534.200195  82.750000  18.458000  1138.199951  22296.750000  5.749   \n",
       "...           ...        ...        ...          ...           ...    ...   \n",
       "3632  7527.700195  75.089996  22.754999  2013.900024  15899.870117  2.641   \n",
       "3633  7529.700195  77.360001  22.808001  2016.900024  16211.959961  2.571   \n",
       "3634  7635.100098  78.010002  22.754999  2016.800049  15952.230469  2.712   \n",
       "3635  7632.700195  76.779999  23.138000  2025.199951  16077.240234  2.490   \n",
       "3636  7666.299805  77.820000  23.107000  2031.500000  15703.450195  2.077   \n",
       "\n",
       "        ZC=F  EURUSD=X       BTC-USD    HO=F  \n",
       "0     418.50  1.442398    457.334015  2.1905  \n",
       "1     418.75  1.436596    457.334015  2.1941  \n",
       "2     421.75  1.440403    457.334015  2.2032  \n",
       "3     417.50  1.431803    457.334015  2.1836  \n",
       "4     423.00  1.441109    457.334015  2.2003  \n",
       "...      ...       ...           ...     ...  \n",
       "3632  452.25  1.085788  40077.074219  2.6818  \n",
       "3633  451.75  1.088175  39933.808594  2.7954  \n",
       "3634  446.25  1.084705  41816.871094  2.8434  \n",
       "3635  440.25  1.084352  43288.246094  2.8339  \n",
       "3636  447.75  1.083447  42952.609375  2.8068  \n",
       "\n",
       "[3637 rows x 3031 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = '2010-01-01'\n",
    "end = '2024-01-31'\n",
    "macro_data=join_macro(start,end)\n",
    "raw_data = pd.merge(original_data, macro_data, on='Date', how='inner')\n",
    "raw_data = raw_data.sort_values(by=[ 'Date']).reset_index(drop=True)\n",
    "raw_data=raw_data[raw_data['Date'].notna()]\n",
    "\n",
    "\n",
    "\n",
    "raw_data.bfill( inplace=True)\n",
    "raw_data.ffill(inplace=True)\n",
    "\n",
    "\n",
    "df = raw_data\n",
    "# Iterar sobre todas las columnas menos 'Date'\n",
    "for columna in df.columns:\n",
    "    if columna not in ['Date'] :\n",
    "        # Verificar valores no numéricos en la columna\n",
    "        non_numeric_values = df[columna][~df[columna].apply(lambda x: isinstance(x, (int, float)))]\n",
    "        # Convertir la columna a tipo numérico, forzando los errores a NaN\n",
    "        df[columna] = pd.to_numeric(df[columna], errors='coerce')\n",
    "        \n",
    "\n",
    "\n",
    "volume_columns = [col for col in df.columns if \"Volume\" in col]\n",
    "for col in volume_columns: \n",
    "    df[col]=np.log1p(df[col])\n",
    "\n",
    "display(df)\n",
    "print('Size of Horizontal Structure',raw_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Obtaining Technical Indicators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores Nulos en Horizontal Series([], dtype: int64)\n",
      "Duplicados:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AdjustedClose_A</th>\n",
       "      <th>High_A</th>\n",
       "      <th>Low_A</th>\n",
       "      <th>Open_A</th>\n",
       "      <th>Volume_A</th>\n",
       "      <th>AdjustedClose_AAL</th>\n",
       "      <th>High_AAL</th>\n",
       "      <th>Low_AAL</th>\n",
       "      <th>Open_AAL</th>\n",
       "      <th>...</th>\n",
       "      <th>MTM6_ZTS</th>\n",
       "      <th>MTM12_ZTS</th>\n",
       "      <th>ROC_ZTS</th>\n",
       "      <th>SMI_ZTS</th>\n",
       "      <th>WVAD_ZTS</th>\n",
       "      <th>RSI_ZTS</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>20.053032</td>\n",
       "      <td>22.625179</td>\n",
       "      <td>22.267525</td>\n",
       "      <td>22.453505</td>\n",
       "      <td>15.154599</td>\n",
       "      <td>4.496876</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>19.835201</td>\n",
       "      <td>22.331903</td>\n",
       "      <td>22.002861</td>\n",
       "      <td>22.324751</td>\n",
       "      <td>15.247264</td>\n",
       "      <td>5.005958</td>\n",
       "      <td>5.37</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.79</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>19.764729</td>\n",
       "      <td>22.174536</td>\n",
       "      <td>22.002861</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>14.992250</td>\n",
       "      <td>4.798555</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>19.739101</td>\n",
       "      <td>22.045780</td>\n",
       "      <td>21.816881</td>\n",
       "      <td>22.017166</td>\n",
       "      <td>14.945354</td>\n",
       "      <td>4.939965</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>19.732687</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>21.745352</td>\n",
       "      <td>21.917025</td>\n",
       "      <td>15.132969</td>\n",
       "      <td>4.845691</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.06</td>\n",
       "      <td>5.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>19.745510</td>\n",
       "      <td>22.210300</td>\n",
       "      <td>21.938484</td>\n",
       "      <td>22.088697</td>\n",
       "      <td>15.380282</td>\n",
       "      <td>4.751417</td>\n",
       "      <td>5.23</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>19.508453</td>\n",
       "      <td>21.924177</td>\n",
       "      <td>21.616594</td>\n",
       "      <td>21.859800</td>\n",
       "      <td>14.870197</td>\n",
       "      <td>4.789126</td>\n",
       "      <td>5.15</td>\n",
       "      <td>4.96</td>\n",
       "      <td>5.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>19.662220</td>\n",
       "      <td>22.017166</td>\n",
       "      <td>21.494993</td>\n",
       "      <td>21.795422</td>\n",
       "      <td>15.044844</td>\n",
       "      <td>5.166223</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.02</td>\n",
       "      <td>5.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-01-14</td>\n",
       "      <td>19.956928</td>\n",
       "      <td>22.346209</td>\n",
       "      <td>21.816881</td>\n",
       "      <td>21.881260</td>\n",
       "      <td>15.634201</td>\n",
       "      <td>5.269925</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>19.495636</td>\n",
       "      <td>22.432047</td>\n",
       "      <td>21.695278</td>\n",
       "      <td>22.331903</td>\n",
       "      <td>15.347351</td>\n",
       "      <td>5.185079</td>\n",
       "      <td>5.84</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010-01-18</td>\n",
       "      <td>19.732687</td>\n",
       "      <td>22.052933</td>\n",
       "      <td>21.709585</td>\n",
       "      <td>21.716738</td>\n",
       "      <td>15.086294</td>\n",
       "      <td>5.317063</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2010-01-19</td>\n",
       "      <td>19.732687</td>\n",
       "      <td>22.052933</td>\n",
       "      <td>21.709585</td>\n",
       "      <td>21.716738</td>\n",
       "      <td>15.086294</td>\n",
       "      <td>5.317063</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010-01-20</td>\n",
       "      <td>19.623779</td>\n",
       "      <td>21.938484</td>\n",
       "      <td>21.595137</td>\n",
       "      <td>21.838341</td>\n",
       "      <td>15.339189</td>\n",
       "      <td>5.411336</td>\n",
       "      <td>5.89</td>\n",
       "      <td>5.61</td>\n",
       "      <td>5.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2010-01-21</td>\n",
       "      <td>19.553305</td>\n",
       "      <td>22.253220</td>\n",
       "      <td>21.587982</td>\n",
       "      <td>22.174536</td>\n",
       "      <td>15.620752</td>\n",
       "      <td>5.147368</td>\n",
       "      <td>5.96</td>\n",
       "      <td>5.42</td>\n",
       "      <td>5.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2010-01-22</td>\n",
       "      <td>18.688395</td>\n",
       "      <td>21.709585</td>\n",
       "      <td>20.808298</td>\n",
       "      <td>21.709585</td>\n",
       "      <td>15.265498</td>\n",
       "      <td>4.939965</td>\n",
       "      <td>5.52</td>\n",
       "      <td>5.11</td>\n",
       "      <td>5.48</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2010-01-25</td>\n",
       "      <td>18.867786</td>\n",
       "      <td>21.208870</td>\n",
       "      <td>20.908442</td>\n",
       "      <td>21.044350</td>\n",
       "      <td>15.098808</td>\n",
       "      <td>4.921111</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2010-01-26</td>\n",
       "      <td>18.765278</td>\n",
       "      <td>21.101574</td>\n",
       "      <td>20.729614</td>\n",
       "      <td>21.008583</td>\n",
       "      <td>14.807607</td>\n",
       "      <td>4.760844</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2010-01-27</td>\n",
       "      <td>18.688395</td>\n",
       "      <td>20.951359</td>\n",
       "      <td>20.550787</td>\n",
       "      <td>20.886980</td>\n",
       "      <td>15.288482</td>\n",
       "      <td>4.581723</td>\n",
       "      <td>5.17</td>\n",
       "      <td>4.47</td>\n",
       "      <td>5.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2010-01-28</td>\n",
       "      <td>18.380880</td>\n",
       "      <td>21.008583</td>\n",
       "      <td>20.379112</td>\n",
       "      <td>20.836910</td>\n",
       "      <td>15.412304</td>\n",
       "      <td>4.845691</td>\n",
       "      <td>5.22</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2010-01-29</td>\n",
       "      <td>17.958031</td>\n",
       "      <td>20.865522</td>\n",
       "      <td>19.971388</td>\n",
       "      <td>20.693848</td>\n",
       "      <td>15.594362</td>\n",
       "      <td>5.005958</td>\n",
       "      <td>5.61</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 9535 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AdjustedClose_A     High_A      Low_A     Open_A   Volume_A  \\\n",
       "0  2010-01-04        20.053032  22.625179  22.267525  22.453505  15.154599   \n",
       "1  2010-01-05        19.835201  22.331903  22.002861  22.324751  15.247264   \n",
       "2  2010-01-06        19.764729  22.174536  22.002861  22.067240  14.992250   \n",
       "3  2010-01-07        19.739101  22.045780  21.816881  22.017166  14.945354   \n",
       "4  2010-01-08        19.732687  22.067240  21.745352  21.917025  15.132969   \n",
       "5  2010-01-11        19.745510  22.210300  21.938484  22.088697  15.380282   \n",
       "6  2010-01-12        19.508453  21.924177  21.616594  21.859800  14.870197   \n",
       "7  2010-01-13        19.662220  22.017166  21.494993  21.795422  15.044844   \n",
       "8  2010-01-14        19.956928  22.346209  21.816881  21.881260  15.634201   \n",
       "9  2010-01-15        19.495636  22.432047  21.695278  22.331903  15.347351   \n",
       "10 2010-01-18        19.732687  22.052933  21.709585  21.716738  15.086294   \n",
       "11 2010-01-19        19.732687  22.052933  21.709585  21.716738  15.086294   \n",
       "12 2010-01-20        19.623779  21.938484  21.595137  21.838341  15.339189   \n",
       "13 2010-01-21        19.553305  22.253220  21.587982  22.174536  15.620752   \n",
       "14 2010-01-22        18.688395  21.709585  20.808298  21.709585  15.265498   \n",
       "15 2010-01-25        18.867786  21.208870  20.908442  21.044350  15.098808   \n",
       "16 2010-01-26        18.765278  21.101574  20.729614  21.008583  14.807607   \n",
       "17 2010-01-27        18.688395  20.951359  20.550787  20.886980  15.288482   \n",
       "18 2010-01-28        18.380880  21.008583  20.379112  20.836910  15.412304   \n",
       "19 2010-01-29        17.958031  20.865522  19.971388  20.693848  15.594362   \n",
       "\n",
       "    AdjustedClose_AAL  High_AAL  Low_AAL  Open_AAL  ...  MTM6_ZTS  MTM12_ZTS  \\\n",
       "0            4.496876      4.94     4.66      4.84  ...       0.0        0.0   \n",
       "1            5.005958      5.37     4.71      4.79  ...       0.0        0.0   \n",
       "2            4.798555      5.38     5.00      5.19  ...       0.0        0.0   \n",
       "3            4.939965      5.43     5.05      5.06  ...       0.0        0.0   \n",
       "4            4.845691      5.43     5.06      5.27  ...       0.0        0.0   \n",
       "5            4.751417      5.23     4.94      5.13  ...       0.0        0.0   \n",
       "6            4.789126      5.15     4.96      5.06  ...       0.0        0.0   \n",
       "7            5.166223      5.50     5.02      5.12  ...       0.0        0.0   \n",
       "8            5.269925      5.71     5.41      5.46  ...       0.0        0.0   \n",
       "9            5.185079      5.84     5.43      5.64  ...       0.0        0.0   \n",
       "10           5.317063      5.73     5.50      5.50  ...       0.0        0.0   \n",
       "11           5.317063      5.73     5.50      5.50  ...       0.0        0.0   \n",
       "12           5.411336      5.89     5.61      5.65  ...       0.0        0.0   \n",
       "13           5.147368      5.96     5.42      5.88  ...       0.0        0.0   \n",
       "14           4.939965      5.52     5.11      5.48  ...       0.0        0.0   \n",
       "15           4.921111      5.50     5.15      5.34  ...       0.0        0.0   \n",
       "16           4.760844      5.28     5.05      5.21  ...       0.0        0.0   \n",
       "17           4.581723      5.17     4.47      5.10  ...       0.0        0.0   \n",
       "18           4.845691      5.22     4.71      4.99  ...       0.0        0.0   \n",
       "19           5.005958      5.61     5.17      5.23  ...       0.0        0.0   \n",
       "\n",
       "    ROC_ZTS    SMI_ZTS  WVAD_ZTS  RSI_ZTS   day  month    year  day_of_week  \n",
       "0       0.0  42.519742 -6.951455    100.0   4.0    1.0  2010.0          0.0  \n",
       "1       0.0  42.519742 -6.951455    100.0   5.0    1.0  2010.0          1.0  \n",
       "2       0.0  42.519742 -6.951455    100.0   6.0    1.0  2010.0          2.0  \n",
       "3       0.0  42.519742 -6.951455    100.0   7.0    1.0  2010.0          3.0  \n",
       "4       0.0  42.519742 -6.951455    100.0   8.0    1.0  2010.0          4.0  \n",
       "5       0.0  42.519742 -6.951455    100.0  11.0    1.0  2010.0          0.0  \n",
       "6       0.0  42.519742 -6.951455    100.0  12.0    1.0  2010.0          1.0  \n",
       "7       0.0  42.519742 -6.951455    100.0  13.0    1.0  2010.0          2.0  \n",
       "8       0.0  42.519742 -6.951455    100.0  14.0    1.0  2010.0          3.0  \n",
       "9       0.0  42.519742 -6.951455    100.0  15.0    1.0  2010.0          4.0  \n",
       "10      0.0  42.519742 -6.951455    100.0  18.0    1.0  2010.0          0.0  \n",
       "11      0.0  42.519742 -6.951455    100.0  19.0    1.0  2010.0          1.0  \n",
       "12      0.0  42.519742 -6.951455    100.0  20.0    1.0  2010.0          2.0  \n",
       "13      0.0  42.519742 -6.951455    100.0  21.0    1.0  2010.0          3.0  \n",
       "14      0.0  42.519742 -6.951455    100.0  22.0    1.0  2010.0          4.0  \n",
       "15      0.0  42.519742 -6.951455    100.0  25.0    1.0  2010.0          0.0  \n",
       "16      0.0  42.519742 -6.951455    100.0  26.0    1.0  2010.0          1.0  \n",
       "17      0.0  42.519742 -6.951455    100.0  27.0    1.0  2010.0          2.0  \n",
       "18      0.0  42.519742 -6.951455    100.0  28.0    1.0  2010.0          3.0  \n",
       "19      0.0  42.519742 -6.951455    100.0  29.0    1.0  2010.0          4.0  \n",
       "\n",
       "[20 rows x 9535 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3616   2024-01-02\n",
       "3617   2024-01-03\n",
       "3618   2024-01-04\n",
       "3619   2024-01-05\n",
       "3620   2024-01-08\n",
       "3621   2024-01-09\n",
       "3622   2024-01-10\n",
       "3623   2024-01-11\n",
       "3624   2024-01-12\n",
       "3625   2024-01-15\n",
       "3626   2024-01-16\n",
       "3627   2024-01-17\n",
       "3628   2024-01-18\n",
       "3629   2024-01-19\n",
       "3630   2024-01-22\n",
       "3631   2024-01-23\n",
       "3632   2024-01-24\n",
       "3633   2024-01-25\n",
       "3634   2024-01-26\n",
       "3635   2024-01-29\n",
       "3636   2024-01-30\n",
       "Name: Date, dtype: datetime64[ns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(utils)\n",
    "\n",
    "df2=df.copy()\n",
    "\n",
    "df2['Date']=pd.to_datetime(df2['Date'],format=\"ISO8601\")\n",
    "\n",
    "datos_tecnicos=join_technical_indicators(database=df2,folder = r\"data/stocks\",axis=True)\n",
    "\n",
    "df2 = pd.merge(df2, datos_tecnicos, on='Date', how='outer')\n",
    "df2 = df2.sort_values(by=[ 'Date']).reset_index(drop=True)\n",
    "\n",
    "df2['day']          = df2['Date'].dt.day.astype('float64')\n",
    "df2['month']        = df2['Date'].dt.month.astype('float64')\n",
    "df2['year']         = df2['Date'].dt.year.astype('float64')\n",
    "df2['day_of_week']  = df2['Date'].dt.dayofweek.astype('float64')\n",
    "\n",
    "\n",
    "# Filtrar y mostrar columnas con valores nulos en df2\n",
    "null_columns_df2 = df2.isnull().sum()\n",
    "print('Valores Nulos en Horizontal',null_columns_df2[null_columns_df2 > 0])\n",
    "\n",
    "print('Duplicados: ',df2.duplicated().sum())\n",
    "\n",
    "columnas_a_eliminar = [col for col in df2.columns if \"Close_\" in col and \"AdjustedClose_\" not in col]\n",
    "df2 = df2.drop(columns=columnas_a_eliminar)\n",
    "display(df2.head(20))\n",
    "\n",
    "\n",
    "train=df2[df2['Date']<'2024-01-01']\n",
    "test=df2[df2['Date']>='2024-01-01']\n",
    "\n",
    "display(test['Date'])\n",
    "\n",
    "train.to_csv('train.csv')\n",
    "test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de train : (3616, 9535)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Información de train :'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3616 entries, 0 to 3615\n",
      "Columns: 9535 entries, Date to day_of_week\n",
      "dtypes: datetime64[ns](1), float64(9534)\n",
      "memory usage: 263.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'AdjustedClose_A', 'High_A', 'Low_A', 'Open_A', 'Volume_A',\n",
       "       'AdjustedClose_AAL', 'High_AAL', 'Low_AAL', 'Open_AAL',\n",
       "       ...\n",
       "       'MTM6_ZTS', 'MTM12_ZTS', 'ROC_ZTS', 'SMI_ZTS', 'WVAD_ZTS', 'RSI_ZTS',\n",
       "       'day', 'month', 'year', 'day_of_week'],\n",
       "      dtype='object', length=9535)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de test : (21, 9535)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Información de test :'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21 entries, 3616 to 3636\n",
      "Columns: 9535 entries, Date to day_of_week\n",
      "dtypes: datetime64[ns](1), float64(9534)\n",
      "memory usage: 1.5 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'AdjustedClose_A', 'High_A', 'Low_A', 'Open_A', 'Volume_A',\n",
       "       'AdjustedClose_AAL', 'High_AAL', 'Low_AAL', 'Open_AAL',\n",
       "       ...\n",
       "       'MTM6_ZTS', 'MTM12_ZTS', 'ROC_ZTS', 'SMI_ZTS', 'WVAD_ZTS', 'RSI_ZTS',\n",
       "       'day', 'month', 'year', 'day_of_week'],\n",
       "      dtype='object', length=9535)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Dimensión de train :',train.shape)\n",
    "display('Información de train :')\n",
    "train.info()\n",
    "display(train.columns)\n",
    "\n",
    "print('Dimensión de test :',test.shape)\n",
    "display('Información de test :')\n",
    "test.info()\n",
    "display(test.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODIGO NÚMERO DE STOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared all files in destination folder: data/probar\n",
      "Copied: IT.csv\n",
      "Copied: BRO.csv\n",
      "Copied: WY.csv\n",
      "Copied: BALL.csv\n",
      "Copied: NKE.csv\n",
      "Copied: MU.csv\n",
      "Copied: SBAC.csv\n",
      "Copied: LUMN.csv\n",
      "Copied: ADBE.csv\n",
      "Copied: ALK.csv\n",
      "Successfully copied 10 files to the folder: data/probar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Horizontal Structure (3637, 3031)\n",
      "Valores Nulos en Horizontal Series([], dtype: int64)\n",
      "Duplicados:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AdjustedClose_A</th>\n",
       "      <th>High_A</th>\n",
       "      <th>Low_A</th>\n",
       "      <th>Open_A</th>\n",
       "      <th>Volume_A</th>\n",
       "      <th>AdjustedClose_AAL</th>\n",
       "      <th>High_AAL</th>\n",
       "      <th>Low_AAL</th>\n",
       "      <th>Open_AAL</th>\n",
       "      <th>...</th>\n",
       "      <th>MTM6_WY</th>\n",
       "      <th>MTM12_WY</th>\n",
       "      <th>ROC_WY</th>\n",
       "      <th>SMI_WY</th>\n",
       "      <th>WVAD_WY</th>\n",
       "      <th>RSI_WY</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>20.053032</td>\n",
       "      <td>22.625179</td>\n",
       "      <td>22.267525</td>\n",
       "      <td>22.453505</td>\n",
       "      <td>15.154599</td>\n",
       "      <td>4.496876</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.84</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>5.681036</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>19.835201</td>\n",
       "      <td>22.331903</td>\n",
       "      <td>22.002861</td>\n",
       "      <td>22.324751</td>\n",
       "      <td>15.247264</td>\n",
       "      <td>5.005958</td>\n",
       "      <td>5.37</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>8.616255</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>19.764729</td>\n",
       "      <td>22.174536</td>\n",
       "      <td>22.002861</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>14.992250</td>\n",
       "      <td>4.798555</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.19</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>4.571395</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>19.739101</td>\n",
       "      <td>22.045780</td>\n",
       "      <td>21.816881</td>\n",
       "      <td>22.017166</td>\n",
       "      <td>14.945354</td>\n",
       "      <td>4.939965</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-0.321977</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>19.732687</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>21.745352</td>\n",
       "      <td>21.917025</td>\n",
       "      <td>15.132969</td>\n",
       "      <td>4.845691</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.06</td>\n",
       "      <td>5.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-2.629289</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>19.745510</td>\n",
       "      <td>22.210300</td>\n",
       "      <td>21.938484</td>\n",
       "      <td>22.088697</td>\n",
       "      <td>15.380282</td>\n",
       "      <td>4.751417</td>\n",
       "      <td>5.23</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>5.099675</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>19.508453</td>\n",
       "      <td>21.924177</td>\n",
       "      <td>21.616594</td>\n",
       "      <td>21.859800</td>\n",
       "      <td>14.870197</td>\n",
       "      <td>4.789126</td>\n",
       "      <td>5.15</td>\n",
       "      <td>4.96</td>\n",
       "      <td>5.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-1.794944</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>19.662220</td>\n",
       "      <td>22.017166</td>\n",
       "      <td>21.494993</td>\n",
       "      <td>21.795422</td>\n",
       "      <td>15.044844</td>\n",
       "      <td>5.166223</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.02</td>\n",
       "      <td>5.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020787</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>4.820256</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-01-14</td>\n",
       "      <td>19.956928</td>\n",
       "      <td>22.346209</td>\n",
       "      <td>21.816881</td>\n",
       "      <td>21.881260</td>\n",
       "      <td>15.634201</td>\n",
       "      <td>5.269925</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.46</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019231</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-2.992770</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>19.495636</td>\n",
       "      <td>22.432047</td>\n",
       "      <td>21.695278</td>\n",
       "      <td>22.331903</td>\n",
       "      <td>15.347351</td>\n",
       "      <td>5.185079</td>\n",
       "      <td>5.84</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020889</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-5.878726</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010-01-18</td>\n",
       "      <td>19.732687</td>\n",
       "      <td>22.052933</td>\n",
       "      <td>21.709585</td>\n",
       "      <td>21.716738</td>\n",
       "      <td>15.086294</td>\n",
       "      <td>5.317063</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013829</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>12.507607</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2010-01-19</td>\n",
       "      <td>19.732687</td>\n",
       "      <td>22.052933</td>\n",
       "      <td>21.709585</td>\n",
       "      <td>21.716738</td>\n",
       "      <td>15.086294</td>\n",
       "      <td>5.317063</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>12.507607</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010-01-20</td>\n",
       "      <td>19.623779</td>\n",
       "      <td>21.938484</td>\n",
       "      <td>21.595137</td>\n",
       "      <td>21.838341</td>\n",
       "      <td>15.339189</td>\n",
       "      <td>5.411336</td>\n",
       "      <td>5.89</td>\n",
       "      <td>5.61</td>\n",
       "      <td>5.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-5.324586</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2010-01-21</td>\n",
       "      <td>19.553305</td>\n",
       "      <td>22.253220</td>\n",
       "      <td>21.587982</td>\n",
       "      <td>22.174536</td>\n",
       "      <td>15.620752</td>\n",
       "      <td>5.147368</td>\n",
       "      <td>5.96</td>\n",
       "      <td>5.42</td>\n",
       "      <td>5.88</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052168</td>\n",
       "      <td>-0.071871</td>\n",
       "      <td>-7.187085</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-13.167614</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2010-01-22</td>\n",
       "      <td>18.688395</td>\n",
       "      <td>21.709585</td>\n",
       "      <td>20.808298</td>\n",
       "      <td>21.709585</td>\n",
       "      <td>15.265498</td>\n",
       "      <td>4.939965</td>\n",
       "      <td>5.52</td>\n",
       "      <td>5.11</td>\n",
       "      <td>5.48</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059280</td>\n",
       "      <td>-0.077370</td>\n",
       "      <td>-7.737037</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-6.636597</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2010-01-25</td>\n",
       "      <td>18.867786</td>\n",
       "      <td>21.208870</td>\n",
       "      <td>20.908442</td>\n",
       "      <td>21.044350</td>\n",
       "      <td>15.098808</td>\n",
       "      <td>4.921111</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.34</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049094</td>\n",
       "      <td>-0.068958</td>\n",
       "      <td>-6.895776</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-4.929280</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2010-01-26</td>\n",
       "      <td>18.765278</td>\n",
       "      <td>21.101574</td>\n",
       "      <td>20.729614</td>\n",
       "      <td>21.008583</td>\n",
       "      <td>14.807607</td>\n",
       "      <td>4.760844</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.21</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070662</td>\n",
       "      <td>-0.057810</td>\n",
       "      <td>-5.781000</td>\n",
       "      <td>9.853272</td>\n",
       "      <td>4.783835</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2010-01-27</td>\n",
       "      <td>18.688395</td>\n",
       "      <td>20.951359</td>\n",
       "      <td>20.550787</td>\n",
       "      <td>20.886980</td>\n",
       "      <td>15.288482</td>\n",
       "      <td>4.581723</td>\n",
       "      <td>5.17</td>\n",
       "      <td>4.47</td>\n",
       "      <td>5.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077594</td>\n",
       "      <td>-0.070946</td>\n",
       "      <td>-7.094598</td>\n",
       "      <td>15.076942</td>\n",
       "      <td>-1.855560</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2010-01-28</td>\n",
       "      <td>18.380880</td>\n",
       "      <td>21.008583</td>\n",
       "      <td>20.379112</td>\n",
       "      <td>20.836910</td>\n",
       "      <td>15.412304</td>\n",
       "      <td>4.845691</td>\n",
       "      <td>5.22</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.99</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079582</td>\n",
       "      <td>-0.077484</td>\n",
       "      <td>-7.748408</td>\n",
       "      <td>13.981684</td>\n",
       "      <td>-9.578624</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2010-01-29</td>\n",
       "      <td>17.958031</td>\n",
       "      <td>20.865522</td>\n",
       "      <td>19.971388</td>\n",
       "      <td>20.693848</td>\n",
       "      <td>15.594362</td>\n",
       "      <td>5.005958</td>\n",
       "      <td>5.61</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.23</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049321</td>\n",
       "      <td>-0.098916</td>\n",
       "      <td>-9.891593</td>\n",
       "      <td>8.896018</td>\n",
       "      <td>-8.294642</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 2675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AdjustedClose_A     High_A      Low_A     Open_A   Volume_A  \\\n",
       "0  2010-01-04        20.053032  22.625179  22.267525  22.453505  15.154599   \n",
       "1  2010-01-05        19.835201  22.331903  22.002861  22.324751  15.247264   \n",
       "2  2010-01-06        19.764729  22.174536  22.002861  22.067240  14.992250   \n",
       "3  2010-01-07        19.739101  22.045780  21.816881  22.017166  14.945354   \n",
       "4  2010-01-08        19.732687  22.067240  21.745352  21.917025  15.132969   \n",
       "5  2010-01-11        19.745510  22.210300  21.938484  22.088697  15.380282   \n",
       "6  2010-01-12        19.508453  21.924177  21.616594  21.859800  14.870197   \n",
       "7  2010-01-13        19.662220  22.017166  21.494993  21.795422  15.044844   \n",
       "8  2010-01-14        19.956928  22.346209  21.816881  21.881260  15.634201   \n",
       "9  2010-01-15        19.495636  22.432047  21.695278  22.331903  15.347351   \n",
       "10 2010-01-18        19.732687  22.052933  21.709585  21.716738  15.086294   \n",
       "11 2010-01-19        19.732687  22.052933  21.709585  21.716738  15.086294   \n",
       "12 2010-01-20        19.623779  21.938484  21.595137  21.838341  15.339189   \n",
       "13 2010-01-21        19.553305  22.253220  21.587982  22.174536  15.620752   \n",
       "14 2010-01-22        18.688395  21.709585  20.808298  21.709585  15.265498   \n",
       "15 2010-01-25        18.867786  21.208870  20.908442  21.044350  15.098808   \n",
       "16 2010-01-26        18.765278  21.101574  20.729614  21.008583  14.807607   \n",
       "17 2010-01-27        18.688395  20.951359  20.550787  20.886980  15.288482   \n",
       "18 2010-01-28        18.380880  21.008583  20.379112  20.836910  15.412304   \n",
       "19 2010-01-29        17.958031  20.865522  19.971388  20.693848  15.594362   \n",
       "\n",
       "    AdjustedClose_AAL  High_AAL  Low_AAL  Open_AAL  ...   MTM6_WY  MTM12_WY  \\\n",
       "0            4.496876      4.94     4.66      4.84  ... -0.009033 -0.006775   \n",
       "1            5.005958      5.37     4.71      4.79  ... -0.009033 -0.006775   \n",
       "2            4.798555      5.38     5.00      5.19  ... -0.009033 -0.006775   \n",
       "3            4.939965      5.43     5.05      5.06  ... -0.009033 -0.006775   \n",
       "4            4.845691      5.43     5.06      5.27  ... -0.009033 -0.006775   \n",
       "5            4.751417      5.23     4.94      5.13  ... -0.009033 -0.006775   \n",
       "6            4.789126      5.15     4.96      5.06  ... -0.009033 -0.006775   \n",
       "7            5.166223      5.50     5.02      5.12  ... -0.020787 -0.006775   \n",
       "8            5.269925      5.71     5.41      5.46  ... -0.019231 -0.006775   \n",
       "9            5.185079      5.84     5.43      5.64  ... -0.020889 -0.006775   \n",
       "10           5.317063      5.73     5.50      5.50  ...  0.013829 -0.006775   \n",
       "11           5.317063      5.73     5.50      5.50  ...  0.007207 -0.006775   \n",
       "12           5.411336      5.89     5.61      5.65  ...  0.002279 -0.006775   \n",
       "13           5.147368      5.96     5.42      5.88  ... -0.052168 -0.071871   \n",
       "14           4.939965      5.52     5.11      5.48  ... -0.059280 -0.077370   \n",
       "15           4.921111      5.50     5.15      5.34  ... -0.049094 -0.068958   \n",
       "16           4.760844      5.28     5.05      5.21  ... -0.070662 -0.057810   \n",
       "17           4.581723      5.17     4.47      5.10  ... -0.077594 -0.070946   \n",
       "18           4.845691      5.22     4.71      4.99  ... -0.079582 -0.077484   \n",
       "19           5.005958      5.61     5.17      5.23  ... -0.049321 -0.098916   \n",
       "\n",
       "      ROC_WY     SMI_WY    WVAD_WY     RSI_WY   day  month    year  \\\n",
       "0  -0.677505   4.663898   5.681036  24.237716   4.0    1.0  2010.0   \n",
       "1  -0.677505   4.663898   8.616255  24.237716   5.0    1.0  2010.0   \n",
       "2  -0.677505   4.663898   4.571395  24.237716   6.0    1.0  2010.0   \n",
       "3  -0.677505   4.663898  -0.321977  24.237716   7.0    1.0  2010.0   \n",
       "4  -0.677505   4.663898  -2.629289  24.237716   8.0    1.0  2010.0   \n",
       "5  -0.677505   4.663898   5.099675  24.237716  11.0    1.0  2010.0   \n",
       "6  -0.677505   4.663898  -1.794944  24.237716  12.0    1.0  2010.0   \n",
       "7  -0.677505   4.663898   4.820256  24.237716  13.0    1.0  2010.0   \n",
       "8  -0.677505   4.663898  -2.992770  24.237716  14.0    1.0  2010.0   \n",
       "9  -0.677505   4.663898  -5.878726  24.237716  15.0    1.0  2010.0   \n",
       "10 -0.677505   4.663898  12.507607  24.237716  18.0    1.0  2010.0   \n",
       "11 -0.677505   4.663898  12.507607  24.237716  19.0    1.0  2010.0   \n",
       "12 -0.677505   4.663898  -5.324586  24.237716  20.0    1.0  2010.0   \n",
       "13 -7.187085   4.663898 -13.167614  24.237716  21.0    1.0  2010.0   \n",
       "14 -7.737037   4.663898  -6.636597  24.237716  22.0    1.0  2010.0   \n",
       "15 -6.895776   4.663898  -4.929280  24.237716  25.0    1.0  2010.0   \n",
       "16 -5.781000   9.853272   4.783835  24.237716  26.0    1.0  2010.0   \n",
       "17 -7.094598  15.076942  -1.855560  24.237716  27.0    1.0  2010.0   \n",
       "18 -7.748408  13.981684  -9.578624  24.237716  28.0    1.0  2010.0   \n",
       "19 -9.891593   8.896018  -8.294642  24.237716  29.0    1.0  2010.0   \n",
       "\n",
       "    day_of_week  \n",
       "0           0.0  \n",
       "1           1.0  \n",
       "2           2.0  \n",
       "3           3.0  \n",
       "4           4.0  \n",
       "5           0.0  \n",
       "6           1.0  \n",
       "7           2.0  \n",
       "8           3.0  \n",
       "9           4.0  \n",
       "10          0.0  \n",
       "11          1.0  \n",
       "12          2.0  \n",
       "13          3.0  \n",
       "14          4.0  \n",
       "15          0.0  \n",
       "16          1.0  \n",
       "17          2.0  \n",
       "18          3.0  \n",
       "19          4.0  \n",
       "\n",
       "[20 rows x 2675 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3616   2024-01-02\n",
       "3617   2024-01-03\n",
       "3618   2024-01-04\n",
       "3619   2024-01-05\n",
       "3620   2024-01-08\n",
       "3621   2024-01-09\n",
       "3622   2024-01-10\n",
       "3623   2024-01-11\n",
       "3624   2024-01-12\n",
       "3625   2024-01-15\n",
       "3626   2024-01-16\n",
       "3627   2024-01-17\n",
       "3628   2024-01-18\n",
       "3629   2024-01-19\n",
       "3630   2024-01-22\n",
       "3631   2024-01-23\n",
       "3632   2024-01-24\n",
       "3633   2024-01-25\n",
       "3634   2024-01-26\n",
       "3635   2024-01-29\n",
       "3636   2024-01-30\n",
       "Name: Date, dtype: datetime64[ns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AdjustedClose_A</th>\n",
       "      <th>High_A</th>\n",
       "      <th>Low_A</th>\n",
       "      <th>Open_A</th>\n",
       "      <th>Volume_A</th>\n",
       "      <th>AdjustedClose_AAL</th>\n",
       "      <th>High_AAL</th>\n",
       "      <th>Low_AAL</th>\n",
       "      <th>Open_AAL</th>\n",
       "      <th>...</th>\n",
       "      <th>MTM6_WY</th>\n",
       "      <th>MTM12_WY</th>\n",
       "      <th>ROC_WY</th>\n",
       "      <th>SMI_WY</th>\n",
       "      <th>WVAD_WY</th>\n",
       "      <th>RSI_WY</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>20.053032</td>\n",
       "      <td>22.625179</td>\n",
       "      <td>22.267525</td>\n",
       "      <td>22.453505</td>\n",
       "      <td>15.154599</td>\n",
       "      <td>4.496876</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.84</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>5.681036</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>19.835201</td>\n",
       "      <td>22.331903</td>\n",
       "      <td>22.002861</td>\n",
       "      <td>22.324751</td>\n",
       "      <td>15.247264</td>\n",
       "      <td>5.005958</td>\n",
       "      <td>5.37</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>8.616255</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>19.764729</td>\n",
       "      <td>22.174536</td>\n",
       "      <td>22.002861</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>14.992250</td>\n",
       "      <td>4.798555</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.19</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>4.571395</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>19.739101</td>\n",
       "      <td>22.045780</td>\n",
       "      <td>21.816881</td>\n",
       "      <td>22.017166</td>\n",
       "      <td>14.945354</td>\n",
       "      <td>4.939965</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-0.321977</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>19.732687</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>21.745352</td>\n",
       "      <td>21.917025</td>\n",
       "      <td>15.132969</td>\n",
       "      <td>4.845691</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.06</td>\n",
       "      <td>5.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-2.629289</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>19.745510</td>\n",
       "      <td>22.210300</td>\n",
       "      <td>21.938484</td>\n",
       "      <td>22.088697</td>\n",
       "      <td>15.380282</td>\n",
       "      <td>4.751417</td>\n",
       "      <td>5.23</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>5.099675</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>19.508453</td>\n",
       "      <td>21.924177</td>\n",
       "      <td>21.616594</td>\n",
       "      <td>21.859800</td>\n",
       "      <td>14.870197</td>\n",
       "      <td>4.789126</td>\n",
       "      <td>5.15</td>\n",
       "      <td>4.96</td>\n",
       "      <td>5.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-1.794944</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>19.662220</td>\n",
       "      <td>22.017166</td>\n",
       "      <td>21.494993</td>\n",
       "      <td>21.795422</td>\n",
       "      <td>15.044844</td>\n",
       "      <td>5.166223</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.02</td>\n",
       "      <td>5.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020787</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>4.820256</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-01-14</td>\n",
       "      <td>19.956928</td>\n",
       "      <td>22.346209</td>\n",
       "      <td>21.816881</td>\n",
       "      <td>21.881260</td>\n",
       "      <td>15.634201</td>\n",
       "      <td>5.269925</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.46</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019231</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-2.992770</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>19.495636</td>\n",
       "      <td>22.432047</td>\n",
       "      <td>21.695278</td>\n",
       "      <td>22.331903</td>\n",
       "      <td>15.347351</td>\n",
       "      <td>5.185079</td>\n",
       "      <td>5.84</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020889</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.677505</td>\n",
       "      <td>4.663898</td>\n",
       "      <td>-5.878726</td>\n",
       "      <td>24.237716</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AdjustedClose_A     High_A      Low_A     Open_A   Volume_A  \\\n",
       "0 2010-01-04        20.053032  22.625179  22.267525  22.453505  15.154599   \n",
       "1 2010-01-05        19.835201  22.331903  22.002861  22.324751  15.247264   \n",
       "2 2010-01-06        19.764729  22.174536  22.002861  22.067240  14.992250   \n",
       "3 2010-01-07        19.739101  22.045780  21.816881  22.017166  14.945354   \n",
       "4 2010-01-08        19.732687  22.067240  21.745352  21.917025  15.132969   \n",
       "5 2010-01-11        19.745510  22.210300  21.938484  22.088697  15.380282   \n",
       "6 2010-01-12        19.508453  21.924177  21.616594  21.859800  14.870197   \n",
       "7 2010-01-13        19.662220  22.017166  21.494993  21.795422  15.044844   \n",
       "8 2010-01-14        19.956928  22.346209  21.816881  21.881260  15.634201   \n",
       "9 2010-01-15        19.495636  22.432047  21.695278  22.331903  15.347351   \n",
       "\n",
       "   AdjustedClose_AAL  High_AAL  Low_AAL  Open_AAL  ...   MTM6_WY  MTM12_WY  \\\n",
       "0           4.496876      4.94     4.66      4.84  ... -0.009033 -0.006775   \n",
       "1           5.005958      5.37     4.71      4.79  ... -0.009033 -0.006775   \n",
       "2           4.798555      5.38     5.00      5.19  ... -0.009033 -0.006775   \n",
       "3           4.939965      5.43     5.05      5.06  ... -0.009033 -0.006775   \n",
       "4           4.845691      5.43     5.06      5.27  ... -0.009033 -0.006775   \n",
       "5           4.751417      5.23     4.94      5.13  ... -0.009033 -0.006775   \n",
       "6           4.789126      5.15     4.96      5.06  ... -0.009033 -0.006775   \n",
       "7           5.166223      5.50     5.02      5.12  ... -0.020787 -0.006775   \n",
       "8           5.269925      5.71     5.41      5.46  ... -0.019231 -0.006775   \n",
       "9           5.185079      5.84     5.43      5.64  ... -0.020889 -0.006775   \n",
       "\n",
       "     ROC_WY    SMI_WY   WVAD_WY     RSI_WY   day  month    year  day_of_week  \n",
       "0 -0.677505  4.663898  5.681036  24.237716   4.0    1.0  2010.0          0.0  \n",
       "1 -0.677505  4.663898  8.616255  24.237716   5.0    1.0  2010.0          1.0  \n",
       "2 -0.677505  4.663898  4.571395  24.237716   6.0    1.0  2010.0          2.0  \n",
       "3 -0.677505  4.663898 -0.321977  24.237716   7.0    1.0  2010.0          3.0  \n",
       "4 -0.677505  4.663898 -2.629289  24.237716   8.0    1.0  2010.0          4.0  \n",
       "5 -0.677505  4.663898  5.099675  24.237716  11.0    1.0  2010.0          0.0  \n",
       "6 -0.677505  4.663898 -1.794944  24.237716  12.0    1.0  2010.0          1.0  \n",
       "7 -0.677505  4.663898  4.820256  24.237716  13.0    1.0  2010.0          2.0  \n",
       "8 -0.677505  4.663898 -2.992770  24.237716  14.0    1.0  2010.0          3.0  \n",
       "9 -0.677505  4.663898 -5.878726  24.237716  15.0    1.0  2010.0          4.0  \n",
       "\n",
       "[10 rows x 2675 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AdjustedClose_A</th>\n",
       "      <th>High_A</th>\n",
       "      <th>Low_A</th>\n",
       "      <th>Open_A</th>\n",
       "      <th>Volume_A</th>\n",
       "      <th>AdjustedClose_AAL</th>\n",
       "      <th>High_AAL</th>\n",
       "      <th>Low_AAL</th>\n",
       "      <th>Open_AAL</th>\n",
       "      <th>...</th>\n",
       "      <th>MTM6_WY</th>\n",
       "      <th>MTM12_WY</th>\n",
       "      <th>ROC_WY</th>\n",
       "      <th>SMI_WY</th>\n",
       "      <th>WVAD_WY</th>\n",
       "      <th>RSI_WY</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3616</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>138.048584</td>\n",
       "      <td>140.589996</td>\n",
       "      <td>137.910004</td>\n",
       "      <td>138.190002</td>\n",
       "      <td>14.181265</td>\n",
       "      <td>13.44</td>\n",
       "      <td>13.72</td>\n",
       "      <td>13.39</td>\n",
       "      <td>13.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019936</td>\n",
       "      <td>0.087188</td>\n",
       "      <td>8.718753</td>\n",
       "      <td>94.492764</td>\n",
       "      <td>6.200617</td>\n",
       "      <td>65.849168</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3617</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>130.496964</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>131.070007</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>14.545231</td>\n",
       "      <td>12.95</td>\n",
       "      <td>13.17</td>\n",
       "      <td>12.89</td>\n",
       "      <td>13.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008731</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.353573</td>\n",
       "      <td>86.728948</td>\n",
       "      <td>-6.769792</td>\n",
       "      <td>59.556994</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>130.337784</td>\n",
       "      <td>131.500000</td>\n",
       "      <td>130.190002</td>\n",
       "      <td>130.550003</td>\n",
       "      <td>14.710210</td>\n",
       "      <td>13.09</td>\n",
       "      <td>13.25</td>\n",
       "      <td>12.95</td>\n",
       "      <td>13.01</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030032</td>\n",
       "      <td>0.007499</td>\n",
       "      <td>0.749850</td>\n",
       "      <td>68.988493</td>\n",
       "      <td>-9.609413</td>\n",
       "      <td>55.934694</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>129.899979</td>\n",
       "      <td>131.960007</td>\n",
       "      <td>128.619995</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>14.147689</td>\n",
       "      <td>13.60</td>\n",
       "      <td>13.67</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031070</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.387485</td>\n",
       "      <td>48.568743</td>\n",
       "      <td>8.194373</td>\n",
       "      <td>56.468369</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>132.705734</td>\n",
       "      <td>133.570007</td>\n",
       "      <td>129.809998</td>\n",
       "      <td>130.139999</td>\n",
       "      <td>14.086607</td>\n",
       "      <td>14.58</td>\n",
       "      <td>14.67</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.03</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029915</td>\n",
       "      <td>-0.011611</td>\n",
       "      <td>-1.161107</td>\n",
       "      <td>38.898834</td>\n",
       "      <td>10.745312</td>\n",
       "      <td>58.636243</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>130.019363</td>\n",
       "      <td>135.649994</td>\n",
       "      <td>130.009995</td>\n",
       "      <td>132.270004</td>\n",
       "      <td>14.176676</td>\n",
       "      <td>14.38</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.36</td>\n",
       "      <td>14.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029623</td>\n",
       "      <td>-0.010557</td>\n",
       "      <td>-1.055709</td>\n",
       "      <td>36.556243</td>\n",
       "      <td>5.075470</td>\n",
       "      <td>56.169283</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>130.427307</td>\n",
       "      <td>131.160004</td>\n",
       "      <td>128.360001</td>\n",
       "      <td>130.580002</td>\n",
       "      <td>14.097904</td>\n",
       "      <td>14.35</td>\n",
       "      <td>14.41</td>\n",
       "      <td>14.20</td>\n",
       "      <td>14.34</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027594</td>\n",
       "      <td>-0.008209</td>\n",
       "      <td>-0.820870</td>\n",
       "      <td>38.211434</td>\n",
       "      <td>5.756601</td>\n",
       "      <td>56.725683</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>2024-01-11</td>\n",
       "      <td>129.024429</td>\n",
       "      <td>130.679993</td>\n",
       "      <td>127.900002</td>\n",
       "      <td>130.580002</td>\n",
       "      <td>14.538460</td>\n",
       "      <td>14.59</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.24</td>\n",
       "      <td>14.48</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006166</td>\n",
       "      <td>-0.014843</td>\n",
       "      <td>-1.484290</td>\n",
       "      <td>34.959387</td>\n",
       "      <td>6.400425</td>\n",
       "      <td>56.853781</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>129.880066</td>\n",
       "      <td>131.610001</td>\n",
       "      <td>129.639999</td>\n",
       "      <td>130.309998</td>\n",
       "      <td>14.066426</td>\n",
       "      <td>13.21</td>\n",
       "      <td>13.98</td>\n",
       "      <td>13.19</td>\n",
       "      <td>13.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>-0.028877</td>\n",
       "      <td>-2.887670</td>\n",
       "      <td>33.170759</td>\n",
       "      <td>-11.580531</td>\n",
       "      <td>54.969396</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>2024-01-15</td>\n",
       "      <td>129.870132</td>\n",
       "      <td>130.809998</td>\n",
       "      <td>128.600006</td>\n",
       "      <td>129.139999</td>\n",
       "      <td>14.139115</td>\n",
       "      <td>13.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>12.78</td>\n",
       "      <td>13.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018112</td>\n",
       "      <td>-0.048619</td>\n",
       "      <td>-4.861907</td>\n",
       "      <td>22.378487</td>\n",
       "      <td>-6.658657</td>\n",
       "      <td>50.485790</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>129.870132</td>\n",
       "      <td>130.809998</td>\n",
       "      <td>128.600006</td>\n",
       "      <td>129.139999</td>\n",
       "      <td>14.139115</td>\n",
       "      <td>13.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>12.78</td>\n",
       "      <td>13.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028781</td>\n",
       "      <td>-0.057835</td>\n",
       "      <td>-5.783473</td>\n",
       "      <td>11.261067</td>\n",
       "      <td>-6.658657</td>\n",
       "      <td>50.485790</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3627</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>127.472313</td>\n",
       "      <td>130.610001</td>\n",
       "      <td>126.739998</td>\n",
       "      <td>129.979996</td>\n",
       "      <td>14.044557</td>\n",
       "      <td>12.93</td>\n",
       "      <td>13.21</td>\n",
       "      <td>12.87</td>\n",
       "      <td>12.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037641</td>\n",
       "      <td>-0.066149</td>\n",
       "      <td>-6.614896</td>\n",
       "      <td>5.018061</td>\n",
       "      <td>-4.444947</td>\n",
       "      <td>46.028723</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3628</th>\n",
       "      <td>2024-01-18</td>\n",
       "      <td>129.800507</td>\n",
       "      <td>130.729996</td>\n",
       "      <td>127.529999</td>\n",
       "      <td>128.020004</td>\n",
       "      <td>14.155406</td>\n",
       "      <td>13.82</td>\n",
       "      <td>13.87</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036654</td>\n",
       "      <td>-0.063237</td>\n",
       "      <td>-6.323658</td>\n",
       "      <td>8.214634</td>\n",
       "      <td>3.498207</td>\n",
       "      <td>47.013524</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>2024-01-19</td>\n",
       "      <td>130.556656</td>\n",
       "      <td>131.990005</td>\n",
       "      <td>129.210007</td>\n",
       "      <td>130.169998</td>\n",
       "      <td>14.236390</td>\n",
       "      <td>13.66</td>\n",
       "      <td>13.83</td>\n",
       "      <td>13.41</td>\n",
       "      <td>13.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029542</td>\n",
       "      <td>-0.035526</td>\n",
       "      <td>-3.552563</td>\n",
       "      <td>14.684890</td>\n",
       "      <td>3.863881</td>\n",
       "      <td>49.130487</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3630</th>\n",
       "      <td>2024-01-22</td>\n",
       "      <td>131.879929</td>\n",
       "      <td>132.729996</td>\n",
       "      <td>131.250000</td>\n",
       "      <td>131.500000</td>\n",
       "      <td>14.229143</td>\n",
       "      <td>13.62</td>\n",
       "      <td>14.08</td>\n",
       "      <td>13.54</td>\n",
       "      <td>13.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005650</td>\n",
       "      <td>-0.004466</td>\n",
       "      <td>-0.446566</td>\n",
       "      <td>31.107599</td>\n",
       "      <td>9.441117</td>\n",
       "      <td>53.562257</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>130.974533</td>\n",
       "      <td>133.940002</td>\n",
       "      <td>130.330002</td>\n",
       "      <td>133.639999</td>\n",
       "      <td>13.658741</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.47</td>\n",
       "      <td>13.93</td>\n",
       "      <td>14.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>-0.011580</td>\n",
       "      <td>-1.157955</td>\n",
       "      <td>43.414234</td>\n",
       "      <td>-9.534326</td>\n",
       "      <td>52.341924</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3632</th>\n",
       "      <td>2024-01-24</td>\n",
       "      <td>129.123932</td>\n",
       "      <td>132.800003</td>\n",
       "      <td>129.429993</td>\n",
       "      <td>132.410004</td>\n",
       "      <td>13.948205</td>\n",
       "      <td>13.93</td>\n",
       "      <td>14.18</td>\n",
       "      <td>13.85</td>\n",
       "      <td>14.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>-0.027019</td>\n",
       "      <td>-2.701904</td>\n",
       "      <td>49.989428</td>\n",
       "      <td>-11.523481</td>\n",
       "      <td>51.036324</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633</th>\n",
       "      <td>2024-01-25</td>\n",
       "      <td>129.223419</td>\n",
       "      <td>131.429993</td>\n",
       "      <td>129.429993</td>\n",
       "      <td>131.110001</td>\n",
       "      <td>13.929643</td>\n",
       "      <td>15.36</td>\n",
       "      <td>15.46</td>\n",
       "      <td>14.52</td>\n",
       "      <td>14.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026178</td>\n",
       "      <td>-0.012448</td>\n",
       "      <td>-1.244819</td>\n",
       "      <td>48.233518</td>\n",
       "      <td>-8.601594</td>\n",
       "      <td>52.516821</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>2024-01-26</td>\n",
       "      <td>130.327835</td>\n",
       "      <td>134.229996</td>\n",
       "      <td>130.639999</td>\n",
       "      <td>133.710007</td>\n",
       "      <td>14.092992</td>\n",
       "      <td>15.13</td>\n",
       "      <td>15.84</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015956</td>\n",
       "      <td>-0.021283</td>\n",
       "      <td>-2.128292</td>\n",
       "      <td>47.819333</td>\n",
       "      <td>5.171565</td>\n",
       "      <td>50.732143</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635</th>\n",
       "      <td>2024-01-29</td>\n",
       "      <td>132.158524</td>\n",
       "      <td>132.899994</td>\n",
       "      <td>131.279999</td>\n",
       "      <td>131.750000</td>\n",
       "      <td>14.157113</td>\n",
       "      <td>14.94</td>\n",
       "      <td>15.39</td>\n",
       "      <td>14.86</td>\n",
       "      <td>15.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004262</td>\n",
       "      <td>-0.025406</td>\n",
       "      <td>-2.540611</td>\n",
       "      <td>48.243485</td>\n",
       "      <td>-1.784449</td>\n",
       "      <td>49.715910</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3636</th>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>133.601181</td>\n",
       "      <td>134.520004</td>\n",
       "      <td>132.690002</td>\n",
       "      <td>132.830002</td>\n",
       "      <td>14.172276</td>\n",
       "      <td>14.53</td>\n",
       "      <td>14.88</td>\n",
       "      <td>14.53</td>\n",
       "      <td>14.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010466</td>\n",
       "      <td>-0.016057</td>\n",
       "      <td>-1.605712</td>\n",
       "      <td>47.098577</td>\n",
       "      <td>9.754775</td>\n",
       "      <td>50.584207</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 2675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  AdjustedClose_A      High_A       Low_A      Open_A  \\\n",
       "3616 2024-01-02       138.048584  140.589996  137.910004  138.190002   \n",
       "3617 2024-01-03       130.496964  138.000000  131.070007  138.000000   \n",
       "3618 2024-01-04       130.337784  131.500000  130.190002  130.550003   \n",
       "3619 2024-01-05       129.899979  131.960007  128.619995  130.000000   \n",
       "3620 2024-01-08       132.705734  133.570007  129.809998  130.139999   \n",
       "3621 2024-01-09       130.019363  135.649994  130.009995  132.270004   \n",
       "3622 2024-01-10       130.427307  131.160004  128.360001  130.580002   \n",
       "3623 2024-01-11       129.024429  130.679993  127.900002  130.580002   \n",
       "3624 2024-01-12       129.880066  131.610001  129.639999  130.309998   \n",
       "3625 2024-01-15       129.870132  130.809998  128.600006  129.139999   \n",
       "3626 2024-01-16       129.870132  130.809998  128.600006  129.139999   \n",
       "3627 2024-01-17       127.472313  130.610001  126.739998  129.979996   \n",
       "3628 2024-01-18       129.800507  130.729996  127.529999  128.020004   \n",
       "3629 2024-01-19       130.556656  131.990005  129.210007  130.169998   \n",
       "3630 2024-01-22       131.879929  132.729996  131.250000  131.500000   \n",
       "3631 2024-01-23       130.974533  133.940002  130.330002  133.639999   \n",
       "3632 2024-01-24       129.123932  132.800003  129.429993  132.410004   \n",
       "3633 2024-01-25       129.223419  131.429993  129.429993  131.110001   \n",
       "3634 2024-01-26       130.327835  134.229996  130.639999  133.710007   \n",
       "3635 2024-01-29       132.158524  132.899994  131.279999  131.750000   \n",
       "3636 2024-01-30       133.601181  134.520004  132.690002  132.830002   \n",
       "\n",
       "       Volume_A  AdjustedClose_AAL  High_AAL  Low_AAL  Open_AAL  ...  \\\n",
       "3616  14.181265              13.44     13.72    13.39     13.65  ...   \n",
       "3617  14.545231              12.95     13.17    12.89     13.15  ...   \n",
       "3618  14.710210              13.09     13.25    12.95     13.01  ...   \n",
       "3619  14.147689              13.60     13.67    13.01     13.05  ...   \n",
       "3620  14.086607              14.58     14.67    14.02     14.03  ...   \n",
       "3621  14.176676              14.38     14.63    14.36     14.51  ...   \n",
       "3622  14.097904              14.35     14.41    14.20     14.34  ...   \n",
       "3623  14.538460              14.59     14.63    14.24     14.48  ...   \n",
       "3624  14.066426              13.21     13.98    13.19     13.93  ...   \n",
       "3625  14.139115              13.19     13.20    12.78     13.08  ...   \n",
       "3626  14.139115              13.19     13.20    12.78     13.08  ...   \n",
       "3627  14.044557              12.93     13.21    12.87     12.96  ...   \n",
       "3628  14.155406              13.82     13.87    13.01     13.06  ...   \n",
       "3629  14.236390              13.66     13.83    13.41     13.83  ...   \n",
       "3630  14.229143              13.62     14.08    13.54     13.96  ...   \n",
       "3631  13.658741              14.00     14.47    13.93     14.14  ...   \n",
       "3632  13.948205              13.93     14.18    13.85     14.16  ...   \n",
       "3633  13.929643              15.36     15.46    14.52     14.59  ...   \n",
       "3634  14.092992              15.13     15.84    15.00     15.55  ...   \n",
       "3635  14.157113              14.94     15.39    14.86     15.37  ...   \n",
       "3636  14.172276              14.53     14.88    14.53     14.79  ...   \n",
       "\n",
       "       MTM6_WY  MTM12_WY    ROC_WY     SMI_WY    WVAD_WY     RSI_WY   day  \\\n",
       "3616  0.019936  0.087188  8.718753  94.492764   6.200617  65.849168   2.0   \n",
       "3617 -0.008731  0.003536  0.353573  86.728948  -6.769792  59.556994   3.0   \n",
       "3618 -0.030032  0.007499  0.749850  68.988493  -9.609413  55.934694   4.0   \n",
       "3619 -0.031070  0.003875  0.387485  48.568743   8.194373  56.468369   5.0   \n",
       "3620 -0.029915 -0.011611 -1.161107  38.898834  10.745312  58.636243   8.0   \n",
       "3621 -0.029623 -0.010557 -1.055709  36.556243   5.075470  56.169283   9.0   \n",
       "3622 -0.027594 -0.008209 -0.820870  38.211434   5.756601  56.725683  10.0   \n",
       "3623 -0.006166 -0.014843 -1.484290  34.959387   6.400425  56.853781  11.0   \n",
       "3624  0.001191 -0.028877 -2.887670  33.170759 -11.580531  54.969396  12.0   \n",
       "3625 -0.018112 -0.048619 -4.861907  22.378487  -6.658657  50.485790  15.0   \n",
       "3626 -0.028781 -0.057835 -5.783473  11.261067  -6.658657  50.485790  16.0   \n",
       "3627 -0.037641 -0.066149 -6.614896   5.018061  -4.444947  46.028723  17.0   \n",
       "3628 -0.036654 -0.063237 -6.323658   8.214634   3.498207  47.013524  18.0   \n",
       "3629 -0.029542 -0.035526 -3.552563  14.684890   3.863881  49.130487  19.0   \n",
       "3630 -0.005650 -0.004466 -0.446566  31.107599   9.441117  53.562257  22.0   \n",
       "3631  0.006653 -0.011580 -1.157955  43.414234  -9.534326  52.341924  23.0   \n",
       "3632  0.001814 -0.027019 -2.701904  49.989428 -11.523481  51.036324  24.0   \n",
       "3633  0.026178 -0.012448 -1.244819  48.233518  -8.601594  52.516821  25.0   \n",
       "3634  0.015956 -0.021283 -2.128292  47.819333   5.171565  50.732143  26.0   \n",
       "3635  0.004262 -0.025406 -2.540611  48.243485  -1.784449  49.715910  29.0   \n",
       "3636 -0.010466 -0.016057 -1.605712  47.098577   9.754775  50.584207  30.0   \n",
       "\n",
       "      month    year  day_of_week  \n",
       "3616    1.0  2024.0          1.0  \n",
       "3617    1.0  2024.0          2.0  \n",
       "3618    1.0  2024.0          3.0  \n",
       "3619    1.0  2024.0          4.0  \n",
       "3620    1.0  2024.0          0.0  \n",
       "3621    1.0  2024.0          1.0  \n",
       "3622    1.0  2024.0          2.0  \n",
       "3623    1.0  2024.0          3.0  \n",
       "3624    1.0  2024.0          4.0  \n",
       "3625    1.0  2024.0          0.0  \n",
       "3626    1.0  2024.0          1.0  \n",
       "3627    1.0  2024.0          2.0  \n",
       "3628    1.0  2024.0          3.0  \n",
       "3629    1.0  2024.0          4.0  \n",
       "3630    1.0  2024.0          0.0  \n",
       "3631    1.0  2024.0          1.0  \n",
       "3632    1.0  2024.0          2.0  \n",
       "3633    1.0  2024.0          3.0  \n",
       "3634    1.0  2024.0          4.0  \n",
       "3635    1.0  2024.0          0.0  \n",
       "3636    1.0  2024.0          1.0  \n",
       "\n",
       "[21 rows x 2675 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importar y recargar utilidades\n",
    "reload(utils)\n",
    "from utils import copy_files\n",
    "\n",
    "# Configuración de carpetas y parámetros\n",
    "carpeta_origen = r'data/stocks'\n",
    "carpeta_destino = r'data/probar'\n",
    "num_archivos = 10\n",
    "aleatorio = True\n",
    "\n",
    "# Copiar archivos\n",
    "copy_files(carpeta_origen, carpeta_destino, num_archivos, aleatorio)\n",
    "\n",
    "# Cargar y procesar datos originales\n",
    "reload(utils)\n",
    "\n",
    "original_data     = join_stock_data(r\"data/probar\",axis=True)\n",
    "original_data['Date'] = pd.to_datetime(original_data['Date']).dt.tz_localize(None)\n",
    "start = '2010-01-01'\n",
    "end = '2024-01-31'\n",
    "\n",
    "macro_data=join_macro(start,end)\n",
    "raw_data = pd.merge(original_data, macro_data, on='Date', how='inner')\n",
    "raw_data = raw_data.sort_values(by=[ 'Date']).reset_index(drop=True)\n",
    "raw_data=raw_data[raw_data['Date'].notna()]\n",
    "\n",
    "\n",
    "\n",
    "raw_data.bfill( inplace=True)\n",
    "raw_data.ffill(inplace=True)\n",
    "\n",
    "\n",
    "df = raw_data\n",
    "print('Size of Horizontal Structure',raw_data.shape)\n",
    "\n",
    "\n",
    "# Iterar sobre todas las columnas menos 'Date'\n",
    "for columna in df.columns:\n",
    "    if columna not in ['Date'] :\n",
    "        # Verificar valores no numéricos en la columna\n",
    "        non_numeric_values = df[columna][~df[columna].apply(lambda x: isinstance(x, (int, float)))]\n",
    "        # Convertir la columna a tipo numérico, forzando los errores a NaN\n",
    "        df[columna] = pd.to_numeric(df[columna], errors='coerce')\n",
    "        \n",
    "\n",
    "\n",
    "volume_columns = [col for col in df.columns if \"Volume\" in col]\n",
    "for col in volume_columns: \n",
    "    df[col]=np.log1p(df[col])\n",
    "\n",
    "\n",
    "reload(utils)\n",
    "\n",
    "df2=df.copy()\n",
    "\n",
    "df2['Date']=pd.to_datetime(df2['Date'],format=\"ISO8601\")\n",
    "\n",
    "datos_tecnicos=join_technical_indicators(database=df2,folder = r\"data/probar\",axis=True)\n",
    "\n",
    "df2 = pd.merge(df2, datos_tecnicos, on='Date', how='outer')\n",
    "df2 = df2.sort_values(by=[ 'Date']).reset_index(drop=True)\n",
    "\n",
    "df2['day']          = df2['Date'].dt.day.astype('float64')\n",
    "df2['month']        = df2['Date'].dt.month.astype('float64')\n",
    "df2['year']         = df2['Date'].dt.year.astype('float64')\n",
    "df2['day_of_week']  = df2['Date'].dt.dayofweek.astype('float64')\n",
    "\n",
    "\n",
    "# Filtrar y mostrar columnas con valores nulos en df2\n",
    "null_columns_df2 = df2.isnull().sum()\n",
    "print('Valores Nulos en Horizontal',null_columns_df2[null_columns_df2 > 0])\n",
    "\n",
    "print('Duplicados: ',df2.duplicated().sum())\n",
    "\n",
    "columnas_a_eliminar = [col for col in df2.columns if \"Close_\" in col and \"AdjustedClose_\" not in col]\n",
    "df2 = df2.drop(columns=columnas_a_eliminar)\n",
    "display(df2.head(20))\n",
    "\n",
    "\n",
    "train=df2[df2['Date']<'2024-01-01']\n",
    "test=df2[df2['Date']>='2024-01-01']\n",
    "\n",
    "display(test['Date'])\n",
    "    \n",
    "\n",
    "train = train.loc[:, ~train.columns.str.contains('^Unnamed')]\n",
    "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]\n",
    "\n",
    "\n",
    "display(train.head(10))\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AdjustedClose_A</th>\n",
       "      <th>High_A</th>\n",
       "      <th>Low_A</th>\n",
       "      <th>Open_A</th>\n",
       "      <th>Volume_A</th>\n",
       "      <th>AdjustedClose_AAL</th>\n",
       "      <th>High_AAL</th>\n",
       "      <th>Low_AAL</th>\n",
       "      <th>Open_AAL</th>\n",
       "      <th>...</th>\n",
       "      <th>MTM6_ZTS</th>\n",
       "      <th>MTM12_ZTS</th>\n",
       "      <th>ROC_ZTS</th>\n",
       "      <th>SMI_ZTS</th>\n",
       "      <th>WVAD_ZTS</th>\n",
       "      <th>RSI_ZTS</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>20.053032</td>\n",
       "      <td>22.625179</td>\n",
       "      <td>22.267525</td>\n",
       "      <td>22.453505</td>\n",
       "      <td>15.154599</td>\n",
       "      <td>4.496876</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>19.835201</td>\n",
       "      <td>22.331903</td>\n",
       "      <td>22.002861</td>\n",
       "      <td>22.324751</td>\n",
       "      <td>15.247264</td>\n",
       "      <td>5.005958</td>\n",
       "      <td>5.37</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.79</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>19.764729</td>\n",
       "      <td>22.174536</td>\n",
       "      <td>22.002861</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>14.992250</td>\n",
       "      <td>4.798555</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>19.739101</td>\n",
       "      <td>22.045780</td>\n",
       "      <td>21.816881</td>\n",
       "      <td>22.017166</td>\n",
       "      <td>14.945354</td>\n",
       "      <td>4.939965</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>19.732687</td>\n",
       "      <td>22.067240</td>\n",
       "      <td>21.745352</td>\n",
       "      <td>21.917025</td>\n",
       "      <td>15.132969</td>\n",
       "      <td>4.845691</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.06</td>\n",
       "      <td>5.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>19.745510</td>\n",
       "      <td>22.210300</td>\n",
       "      <td>21.938484</td>\n",
       "      <td>22.088697</td>\n",
       "      <td>15.380282</td>\n",
       "      <td>4.751417</td>\n",
       "      <td>5.23</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>19.508453</td>\n",
       "      <td>21.924177</td>\n",
       "      <td>21.616594</td>\n",
       "      <td>21.859800</td>\n",
       "      <td>14.870197</td>\n",
       "      <td>4.789126</td>\n",
       "      <td>5.15</td>\n",
       "      <td>4.96</td>\n",
       "      <td>5.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>19.662220</td>\n",
       "      <td>22.017166</td>\n",
       "      <td>21.494993</td>\n",
       "      <td>21.795422</td>\n",
       "      <td>15.044844</td>\n",
       "      <td>5.166223</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.02</td>\n",
       "      <td>5.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-01-14</td>\n",
       "      <td>19.956928</td>\n",
       "      <td>22.346209</td>\n",
       "      <td>21.816881</td>\n",
       "      <td>21.881260</td>\n",
       "      <td>15.634201</td>\n",
       "      <td>5.269925</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>19.495636</td>\n",
       "      <td>22.432047</td>\n",
       "      <td>21.695278</td>\n",
       "      <td>22.331903</td>\n",
       "      <td>15.347351</td>\n",
       "      <td>5.185079</td>\n",
       "      <td>5.84</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.519742</td>\n",
       "      <td>-6.951455</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 9535 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AdjustedClose_A     High_A      Low_A     Open_A   Volume_A  \\\n",
       "0  2010-01-04        20.053032  22.625179  22.267525  22.453505  15.154599   \n",
       "1  2010-01-05        19.835201  22.331903  22.002861  22.324751  15.247264   \n",
       "2  2010-01-06        19.764729  22.174536  22.002861  22.067240  14.992250   \n",
       "3  2010-01-07        19.739101  22.045780  21.816881  22.017166  14.945354   \n",
       "4  2010-01-08        19.732687  22.067240  21.745352  21.917025  15.132969   \n",
       "5  2010-01-11        19.745510  22.210300  21.938484  22.088697  15.380282   \n",
       "6  2010-01-12        19.508453  21.924177  21.616594  21.859800  14.870197   \n",
       "7  2010-01-13        19.662220  22.017166  21.494993  21.795422  15.044844   \n",
       "8  2010-01-14        19.956928  22.346209  21.816881  21.881260  15.634201   \n",
       "9  2010-01-15        19.495636  22.432047  21.695278  22.331903  15.347351   \n",
       "\n",
       "   AdjustedClose_AAL  High_AAL  Low_AAL  Open_AAL  ...  MTM6_ZTS  MTM12_ZTS  \\\n",
       "0           4.496876      4.94     4.66      4.84  ...       0.0        0.0   \n",
       "1           5.005958      5.37     4.71      4.79  ...       0.0        0.0   \n",
       "2           4.798555      5.38     5.00      5.19  ...       0.0        0.0   \n",
       "3           4.939965      5.43     5.05      5.06  ...       0.0        0.0   \n",
       "4           4.845691      5.43     5.06      5.27  ...       0.0        0.0   \n",
       "5           4.751417      5.23     4.94      5.13  ...       0.0        0.0   \n",
       "6           4.789126      5.15     4.96      5.06  ...       0.0        0.0   \n",
       "7           5.166223      5.50     5.02      5.12  ...       0.0        0.0   \n",
       "8           5.269925      5.71     5.41      5.46  ...       0.0        0.0   \n",
       "9           5.185079      5.84     5.43      5.64  ...       0.0        0.0   \n",
       "\n",
       "   ROC_ZTS    SMI_ZTS  WVAD_ZTS  RSI_ZTS   day  month    year  day_of_week  \n",
       "0      0.0  42.519742 -6.951455    100.0   4.0    1.0  2010.0          0.0  \n",
       "1      0.0  42.519742 -6.951455    100.0   5.0    1.0  2010.0          1.0  \n",
       "2      0.0  42.519742 -6.951455    100.0   6.0    1.0  2010.0          2.0  \n",
       "3      0.0  42.519742 -6.951455    100.0   7.0    1.0  2010.0          3.0  \n",
       "4      0.0  42.519742 -6.951455    100.0   8.0    1.0  2010.0          4.0  \n",
       "5      0.0  42.519742 -6.951455    100.0  11.0    1.0  2010.0          0.0  \n",
       "6      0.0  42.519742 -6.951455    100.0  12.0    1.0  2010.0          1.0  \n",
       "7      0.0  42.519742 -6.951455    100.0  13.0    1.0  2010.0          2.0  \n",
       "8      0.0  42.519742 -6.951455    100.0  14.0    1.0  2010.0          3.0  \n",
       "9      0.0  42.519742 -6.951455    100.0  15.0    1.0  2010.0          4.0  \n",
       "\n",
       "[10 rows x 9535 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AdjustedClose_A</th>\n",
       "      <th>High_A</th>\n",
       "      <th>Low_A</th>\n",
       "      <th>Open_A</th>\n",
       "      <th>Volume_A</th>\n",
       "      <th>AdjustedClose_AAL</th>\n",
       "      <th>High_AAL</th>\n",
       "      <th>Low_AAL</th>\n",
       "      <th>Open_AAL</th>\n",
       "      <th>...</th>\n",
       "      <th>MTM6_ZTS</th>\n",
       "      <th>MTM12_ZTS</th>\n",
       "      <th>ROC_ZTS</th>\n",
       "      <th>SMI_ZTS</th>\n",
       "      <th>WVAD_ZTS</th>\n",
       "      <th>RSI_ZTS</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>138.048584</td>\n",
       "      <td>140.589996</td>\n",
       "      <td>137.910004</td>\n",
       "      <td>138.190002</td>\n",
       "      <td>14.181265</td>\n",
       "      <td>13.44</td>\n",
       "      <td>13.72</td>\n",
       "      <td>13.39</td>\n",
       "      <td>13.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009812</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.425508</td>\n",
       "      <td>69.725948</td>\n",
       "      <td>3.849407</td>\n",
       "      <td>65.302317</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>130.496964</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>131.070007</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>14.545231</td>\n",
       "      <td>12.95</td>\n",
       "      <td>13.17</td>\n",
       "      <td>12.89</td>\n",
       "      <td>13.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010514</td>\n",
       "      <td>-0.035784</td>\n",
       "      <td>-3.578392</td>\n",
       "      <td>50.192628</td>\n",
       "      <td>-13.494252</td>\n",
       "      <td>58.800210</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>130.337784</td>\n",
       "      <td>131.500000</td>\n",
       "      <td>130.190002</td>\n",
       "      <td>130.550003</td>\n",
       "      <td>14.710210</td>\n",
       "      <td>13.09</td>\n",
       "      <td>13.25</td>\n",
       "      <td>12.95</td>\n",
       "      <td>13.01</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007468</td>\n",
       "      <td>-0.011463</td>\n",
       "      <td>-1.146263</td>\n",
       "      <td>32.484781</td>\n",
       "      <td>5.881362</td>\n",
       "      <td>60.076218</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>129.899979</td>\n",
       "      <td>131.960007</td>\n",
       "      <td>128.619995</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>14.147689</td>\n",
       "      <td>13.60</td>\n",
       "      <td>13.67</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>-0.009506</td>\n",
       "      <td>-0.950587</td>\n",
       "      <td>25.662202</td>\n",
       "      <td>8.620937</td>\n",
       "      <td>61.003954</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>132.705734</td>\n",
       "      <td>133.570007</td>\n",
       "      <td>129.809998</td>\n",
       "      <td>130.139999</td>\n",
       "      <td>14.086607</td>\n",
       "      <td>14.58</td>\n",
       "      <td>14.67</td>\n",
       "      <td>14.02</td>\n",
       "      <td>14.03</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005123</td>\n",
       "      <td>-0.009744</td>\n",
       "      <td>-0.974358</td>\n",
       "      <td>38.447674</td>\n",
       "      <td>6.413617</td>\n",
       "      <td>62.476993</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>130.019363</td>\n",
       "      <td>135.649994</td>\n",
       "      <td>130.009995</td>\n",
       "      <td>132.270004</td>\n",
       "      <td>14.176676</td>\n",
       "      <td>14.38</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.36</td>\n",
       "      <td>14.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>0.673071</td>\n",
       "      <td>48.371497</td>\n",
       "      <td>0.978387</td>\n",
       "      <td>62.078269</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>130.427307</td>\n",
       "      <td>131.160004</td>\n",
       "      <td>128.360001</td>\n",
       "      <td>130.580002</td>\n",
       "      <td>14.097904</td>\n",
       "      <td>14.35</td>\n",
       "      <td>14.41</td>\n",
       "      <td>14.20</td>\n",
       "      <td>14.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011192</td>\n",
       "      <td>0.021114</td>\n",
       "      <td>2.111374</td>\n",
       "      <td>64.393687</td>\n",
       "      <td>10.429972</td>\n",
       "      <td>65.226467</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-01-11</td>\n",
       "      <td>129.024429</td>\n",
       "      <td>130.679993</td>\n",
       "      <td>127.900002</td>\n",
       "      <td>130.580002</td>\n",
       "      <td>14.538460</td>\n",
       "      <td>14.59</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.24</td>\n",
       "      <td>14.48</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019593</td>\n",
       "      <td>0.008873</td>\n",
       "      <td>0.887276</td>\n",
       "      <td>65.601089</td>\n",
       "      <td>-8.128776</td>\n",
       "      <td>61.325508</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>129.880066</td>\n",
       "      <td>131.610001</td>\n",
       "      <td>129.639999</td>\n",
       "      <td>130.309998</td>\n",
       "      <td>14.066426</td>\n",
       "      <td>13.21</td>\n",
       "      <td>13.98</td>\n",
       "      <td>13.19</td>\n",
       "      <td>13.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.017596</td>\n",
       "      <td>1.759592</td>\n",
       "      <td>78.388801</td>\n",
       "      <td>6.727044</td>\n",
       "      <td>63.792990</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-01-15</td>\n",
       "      <td>129.870132</td>\n",
       "      <td>130.809998</td>\n",
       "      <td>128.600006</td>\n",
       "      <td>129.139999</td>\n",
       "      <td>14.139115</td>\n",
       "      <td>13.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>12.78</td>\n",
       "      <td>13.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009546</td>\n",
       "      <td>-0.019858</td>\n",
       "      <td>-1.985774</td>\n",
       "      <td>53.751148</td>\n",
       "      <td>-12.005724</td>\n",
       "      <td>54.098903</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>129.870132</td>\n",
       "      <td>130.809998</td>\n",
       "      <td>128.600006</td>\n",
       "      <td>129.139999</td>\n",
       "      <td>14.139115</td>\n",
       "      <td>13.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>12.78</td>\n",
       "      <td>13.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016110</td>\n",
       "      <td>-0.021150</td>\n",
       "      <td>-2.115033</td>\n",
       "      <td>37.894380</td>\n",
       "      <td>-12.005724</td>\n",
       "      <td>54.098903</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>127.472313</td>\n",
       "      <td>130.610001</td>\n",
       "      <td>126.739998</td>\n",
       "      <td>129.979996</td>\n",
       "      <td>14.044557</td>\n",
       "      <td>12.93</td>\n",
       "      <td>13.21</td>\n",
       "      <td>12.87</td>\n",
       "      <td>12.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025722</td>\n",
       "      <td>-0.032781</td>\n",
       "      <td>-3.278108</td>\n",
       "      <td>12.274323</td>\n",
       "      <td>-4.720022</td>\n",
       "      <td>51.077896</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-01-18</td>\n",
       "      <td>129.800507</td>\n",
       "      <td>130.729996</td>\n",
       "      <td>127.529999</td>\n",
       "      <td>128.020004</td>\n",
       "      <td>14.155406</td>\n",
       "      <td>13.82</td>\n",
       "      <td>13.87</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041606</td>\n",
       "      <td>-0.030880</td>\n",
       "      <td>-3.087962</td>\n",
       "      <td>12.255810</td>\n",
       "      <td>-3.862417</td>\n",
       "      <td>50.509673</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-01-19</td>\n",
       "      <td>130.556656</td>\n",
       "      <td>131.990005</td>\n",
       "      <td>129.210007</td>\n",
       "      <td>130.169998</td>\n",
       "      <td>14.236390</td>\n",
       "      <td>13.66</td>\n",
       "      <td>13.83</td>\n",
       "      <td>13.41</td>\n",
       "      <td>13.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042347</td>\n",
       "      <td>-0.023584</td>\n",
       "      <td>-2.358362</td>\n",
       "      <td>11.288130</td>\n",
       "      <td>-8.528162</td>\n",
       "      <td>47.558006</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-01-22</td>\n",
       "      <td>131.879929</td>\n",
       "      <td>132.729996</td>\n",
       "      <td>131.250000</td>\n",
       "      <td>131.500000</td>\n",
       "      <td>14.229143</td>\n",
       "      <td>13.62</td>\n",
       "      <td>14.08</td>\n",
       "      <td>13.54</td>\n",
       "      <td>13.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038906</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>-1.463614</td>\n",
       "      <td>17.997305</td>\n",
       "      <td>3.805104</td>\n",
       "      <td>51.524481</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>130.974533</td>\n",
       "      <td>133.940002</td>\n",
       "      <td>130.330002</td>\n",
       "      <td>133.639999</td>\n",
       "      <td>13.658741</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.47</td>\n",
       "      <td>13.93</td>\n",
       "      <td>14.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025338</td>\n",
       "      <td>-0.034642</td>\n",
       "      <td>-3.464203</td>\n",
       "      <td>16.311491</td>\n",
       "      <td>-11.815544</td>\n",
       "      <td>47.377899</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-01-24</td>\n",
       "      <td>129.123932</td>\n",
       "      <td>132.800003</td>\n",
       "      <td>129.429993</td>\n",
       "      <td>132.410004</td>\n",
       "      <td>13.948205</td>\n",
       "      <td>13.93</td>\n",
       "      <td>14.18</td>\n",
       "      <td>13.85</td>\n",
       "      <td>14.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035857</td>\n",
       "      <td>-0.051389</td>\n",
       "      <td>-5.138918</td>\n",
       "      <td>13.437572</td>\n",
       "      <td>-10.483195</td>\n",
       "      <td>44.887800</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-01-25</td>\n",
       "      <td>129.223419</td>\n",
       "      <td>131.429993</td>\n",
       "      <td>129.429993</td>\n",
       "      <td>131.110001</td>\n",
       "      <td>13.929643</td>\n",
       "      <td>15.36</td>\n",
       "      <td>15.46</td>\n",
       "      <td>14.52</td>\n",
       "      <td>14.59</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018753</td>\n",
       "      <td>-0.043993</td>\n",
       "      <td>-4.399303</td>\n",
       "      <td>7.423153</td>\n",
       "      <td>11.888204</td>\n",
       "      <td>46.703443</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-01-26</td>\n",
       "      <td>130.327835</td>\n",
       "      <td>134.229996</td>\n",
       "      <td>130.639999</td>\n",
       "      <td>133.710007</td>\n",
       "      <td>14.092992</td>\n",
       "      <td>15.13</td>\n",
       "      <td>15.84</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005354</td>\n",
       "      <td>-0.036474</td>\n",
       "      <td>-3.647432</td>\n",
       "      <td>19.206322</td>\n",
       "      <td>8.522397</td>\n",
       "      <td>52.265446</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2024-01-29</td>\n",
       "      <td>132.158524</td>\n",
       "      <td>132.899994</td>\n",
       "      <td>131.279999</td>\n",
       "      <td>131.750000</td>\n",
       "      <td>14.157113</td>\n",
       "      <td>14.94</td>\n",
       "      <td>15.39</td>\n",
       "      <td>14.86</td>\n",
       "      <td>15.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023251</td>\n",
       "      <td>-0.020080</td>\n",
       "      <td>-2.008038</td>\n",
       "      <td>36.297449</td>\n",
       "      <td>6.598533</td>\n",
       "      <td>53.764954</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>133.601181</td>\n",
       "      <td>134.520004</td>\n",
       "      <td>132.690002</td>\n",
       "      <td>132.830002</td>\n",
       "      <td>14.172276</td>\n",
       "      <td>14.53</td>\n",
       "      <td>14.88</td>\n",
       "      <td>14.53</td>\n",
       "      <td>14.79</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>-0.037750</td>\n",
       "      <td>-3.775012</td>\n",
       "      <td>45.998881</td>\n",
       "      <td>-6.702920</td>\n",
       "      <td>51.923379</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 9535 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  AdjustedClose_A      High_A       Low_A      Open_A  \\\n",
       "0   2024-01-02       138.048584  140.589996  137.910004  138.190002   \n",
       "1   2024-01-03       130.496964  138.000000  131.070007  138.000000   \n",
       "2   2024-01-04       130.337784  131.500000  130.190002  130.550003   \n",
       "3   2024-01-05       129.899979  131.960007  128.619995  130.000000   \n",
       "4   2024-01-08       132.705734  133.570007  129.809998  130.139999   \n",
       "5   2024-01-09       130.019363  135.649994  130.009995  132.270004   \n",
       "6   2024-01-10       130.427307  131.160004  128.360001  130.580002   \n",
       "7   2024-01-11       129.024429  130.679993  127.900002  130.580002   \n",
       "8   2024-01-12       129.880066  131.610001  129.639999  130.309998   \n",
       "9   2024-01-15       129.870132  130.809998  128.600006  129.139999   \n",
       "10  2024-01-16       129.870132  130.809998  128.600006  129.139999   \n",
       "11  2024-01-17       127.472313  130.610001  126.739998  129.979996   \n",
       "12  2024-01-18       129.800507  130.729996  127.529999  128.020004   \n",
       "13  2024-01-19       130.556656  131.990005  129.210007  130.169998   \n",
       "14  2024-01-22       131.879929  132.729996  131.250000  131.500000   \n",
       "15  2024-01-23       130.974533  133.940002  130.330002  133.639999   \n",
       "16  2024-01-24       129.123932  132.800003  129.429993  132.410004   \n",
       "17  2024-01-25       129.223419  131.429993  129.429993  131.110001   \n",
       "18  2024-01-26       130.327835  134.229996  130.639999  133.710007   \n",
       "19  2024-01-29       132.158524  132.899994  131.279999  131.750000   \n",
       "20  2024-01-30       133.601181  134.520004  132.690002  132.830002   \n",
       "\n",
       "     Volume_A  AdjustedClose_AAL  High_AAL  Low_AAL  Open_AAL  ...  MTM6_ZTS  \\\n",
       "0   14.181265              13.44     13.72    13.39     13.65  ...  0.009812   \n",
       "1   14.545231              12.95     13.17    12.89     13.15  ... -0.010514   \n",
       "2   14.710210              13.09     13.25    12.95     13.01  ... -0.007468   \n",
       "3   14.147689              13.60     13.67    13.01     13.05  ... -0.010411   \n",
       "4   14.086607              14.58     14.67    14.02     14.03  ... -0.005123   \n",
       "5   14.176676              14.38     14.63    14.36     14.51  ... -0.007245   \n",
       "6   14.097904              14.35     14.41    14.20     14.34  ...  0.011192   \n",
       "7   14.538460              14.59     14.63    14.24     14.48  ...  0.019593   \n",
       "8   14.066426              13.21     13.98    13.19     13.93  ...  0.025253   \n",
       "9   14.139115              13.19     13.20    12.78     13.08  ... -0.009546   \n",
       "10  14.139115              13.19     13.20    12.78     13.08  ... -0.016110   \n",
       "11  14.044557              12.93     13.21    12.87     12.96  ... -0.025722   \n",
       "12  14.155406              13.82     13.87    13.01     13.06  ... -0.041606   \n",
       "13  14.236390              13.66     13.83    13.41     13.83  ... -0.042347   \n",
       "14  14.229143              13.62     14.08    13.54     13.96  ... -0.038906   \n",
       "15  13.658741              14.00     14.47    13.93     14.14  ... -0.025338   \n",
       "16  13.948205              13.93     14.18    13.85     14.16  ... -0.035857   \n",
       "17  13.929643              15.36     15.46    14.52     14.59  ... -0.018753   \n",
       "18  14.092992              15.13     15.84    15.00     15.55  ...  0.005354   \n",
       "19  14.157113              14.94     15.39    14.86     15.37  ...  0.023251   \n",
       "20  14.172276              14.53     14.88    14.53     14.79  ...  0.001203   \n",
       "\n",
       "    MTM12_ZTS   ROC_ZTS    SMI_ZTS   WVAD_ZTS    RSI_ZTS   day  month    year  \\\n",
       "0   -0.004255 -0.425508  69.725948   3.849407  65.302317   2.0    1.0  2024.0   \n",
       "1   -0.035784 -3.578392  50.192628 -13.494252  58.800210   3.0    1.0  2024.0   \n",
       "2   -0.011463 -1.146263  32.484781   5.881362  60.076218   4.0    1.0  2024.0   \n",
       "3   -0.009506 -0.950587  25.662202   8.620937  61.003954   5.0    1.0  2024.0   \n",
       "4   -0.009744 -0.974358  38.447674   6.413617  62.476993   8.0    1.0  2024.0   \n",
       "5    0.006731  0.673071  48.371497   0.978387  62.078269   9.0    1.0  2024.0   \n",
       "6    0.021114  2.111374  64.393687  10.429972  65.226467  10.0    1.0  2024.0   \n",
       "7    0.008873  0.887276  65.601089  -8.128776  61.325508  11.0    1.0  2024.0   \n",
       "8    0.017596  1.759592  78.388801   6.727044  63.792990  12.0    1.0  2024.0   \n",
       "9   -0.019858 -1.985774  53.751148 -12.005724  54.098903  15.0    1.0  2024.0   \n",
       "10  -0.021150 -2.115033  37.894380 -12.005724  54.098903  16.0    1.0  2024.0   \n",
       "11  -0.032781 -3.278108  12.274323  -4.720022  51.077896  17.0    1.0  2024.0   \n",
       "12  -0.030880 -3.087962  12.255810  -3.862417  50.509673  18.0    1.0  2024.0   \n",
       "13  -0.023584 -2.358362  11.288130  -8.528162  47.558006  19.0    1.0  2024.0   \n",
       "14  -0.014636 -1.463614  17.997305   3.805104  51.524481  22.0    1.0  2024.0   \n",
       "15  -0.034642 -3.464203  16.311491 -11.815544  47.377899  23.0    1.0  2024.0   \n",
       "16  -0.051389 -5.138918  13.437572 -10.483195  44.887800  24.0    1.0  2024.0   \n",
       "17  -0.043993 -4.399303   7.423153  11.888204  46.703443  25.0    1.0  2024.0   \n",
       "18  -0.036474 -3.647432  19.206322   8.522397  52.265446  26.0    1.0  2024.0   \n",
       "19  -0.020080 -2.008038  36.297449   6.598533  53.764954  29.0    1.0  2024.0   \n",
       "20  -0.037750 -3.775012  45.998881  -6.702920  51.923379  30.0    1.0  2024.0   \n",
       "\n",
       "    day_of_week  \n",
       "0           1.0  \n",
       "1           2.0  \n",
       "2           3.0  \n",
       "3           4.0  \n",
       "4           0.0  \n",
       "5           1.0  \n",
       "6           2.0  \n",
       "7           3.0  \n",
       "8           4.0  \n",
       "9           0.0  \n",
       "10          1.0  \n",
       "11          2.0  \n",
       "12          3.0  \n",
       "13          4.0  \n",
       "14          0.0  \n",
       "15          1.0  \n",
       "16          2.0  \n",
       "17          3.0  \n",
       "18          4.0  \n",
       "19          0.0  \n",
       "20          1.0  \n",
       "\n",
       "[21 rows x 9535 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "\n",
    "train = train.loc[:, ~train.columns.str.contains('^Unnamed')]\n",
    "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]\n",
    "\n",
    "\n",
    "display(train.head(10))\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2D Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Principal Components Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Inicio del preprocesamiento y PCA...\n",
      "Hora antes del PCA: 10:15:32.647067\n",
      "Número de componentes según criterio de Kaiser (covarianza): 67\n",
      "Número de componentes según criterio de Kaiser (correlación): 473\n",
      "Hora después del PCA: 10:18:08.429042\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(utils)\n",
    "from utils import preprocess_and_pca\n",
    "\n",
    "cols_close = [col for col in train.columns if \"Close\" in col ]+ ['Date']\n",
    "cols_adjusted = [col for col in cols_close if \"Adjusted\" in col]\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X_train = train.drop(columns=cols_close)  # Eliminar todas las columnas que contienen \"Close\"\n",
    "X_test = test.drop(columns=cols_close)\n",
    "\n",
    "\n",
    "# Para y_train y y_test, nos  con \"AdjustedClose\" y \"Date\"\n",
    "y_train = train[cols_adjusted]\n",
    "y_test = test[cols_adjusted]\n",
    "\n",
    "\n",
    "# Aplicar la función con tus datos\n",
    "X_train_pca, X_test, varianza_acumulada = preprocess_and_pca(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdjustedClose_A</th>\n",
       "      <th>AdjustedClose_AAL</th>\n",
       "      <th>AdjustedClose_AAP</th>\n",
       "      <th>AdjustedClose_AAPL</th>\n",
       "      <th>AdjustedClose_ABBV</th>\n",
       "      <th>AdjustedClose_ABT</th>\n",
       "      <th>AdjustedClose_ACGL</th>\n",
       "      <th>AdjustedClose_ACN</th>\n",
       "      <th>AdjustedClose_ADBE</th>\n",
       "      <th>AdjustedClose_ADI</th>\n",
       "      <th>...</th>\n",
       "      <th>AdjustedClose_WYNN</th>\n",
       "      <th>AdjustedClose_XEL</th>\n",
       "      <th>AdjustedClose_XOM</th>\n",
       "      <th>AdjustedClose_XRAY</th>\n",
       "      <th>AdjustedClose_XYL</th>\n",
       "      <th>AdjustedClose_YUM</th>\n",
       "      <th>AdjustedClose_ZBH</th>\n",
       "      <th>AdjustedClose_ZBRA</th>\n",
       "      <th>AdjustedClose_ZION</th>\n",
       "      <th>AdjustedClose_ZTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.053032</td>\n",
       "      <td>4.496876</td>\n",
       "      <td>35.743465</td>\n",
       "      <td>6.447411</td>\n",
       "      <td>21.629181</td>\n",
       "      <td>18.763710</td>\n",
       "      <td>7.601905</td>\n",
       "      <td>31.941786</td>\n",
       "      <td>37.090000</td>\n",
       "      <td>22.258951</td>\n",
       "      <td>...</td>\n",
       "      <td>41.184948</td>\n",
       "      <td>12.678841</td>\n",
       "      <td>39.272129</td>\n",
       "      <td>31.206663</td>\n",
       "      <td>20.299332</td>\n",
       "      <td>18.878279</td>\n",
       "      <td>52.354065</td>\n",
       "      <td>28.670000</td>\n",
       "      <td>10.223137</td>\n",
       "      <td>28.409882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.835201</td>\n",
       "      <td>5.005958</td>\n",
       "      <td>35.531013</td>\n",
       "      <td>6.458559</td>\n",
       "      <td>21.629181</td>\n",
       "      <td>18.612122</td>\n",
       "      <td>7.576549</td>\n",
       "      <td>32.139191</td>\n",
       "      <td>37.700001</td>\n",
       "      <td>22.223808</td>\n",
       "      <td>...</td>\n",
       "      <td>43.689789</td>\n",
       "      <td>12.528476</td>\n",
       "      <td>39.425449</td>\n",
       "      <td>30.835669</td>\n",
       "      <td>20.299332</td>\n",
       "      <td>18.813721</td>\n",
       "      <td>54.011391</td>\n",
       "      <td>28.620001</td>\n",
       "      <td>10.583591</td>\n",
       "      <td>28.409882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.764729</td>\n",
       "      <td>4.798555</td>\n",
       "      <td>35.840828</td>\n",
       "      <td>6.355825</td>\n",
       "      <td>21.629181</td>\n",
       "      <td>18.715475</td>\n",
       "      <td>7.543795</td>\n",
       "      <td>32.480869</td>\n",
       "      <td>37.619999</td>\n",
       "      <td>22.181643</td>\n",
       "      <td>...</td>\n",
       "      <td>43.116714</td>\n",
       "      <td>12.552535</td>\n",
       "      <td>39.766201</td>\n",
       "      <td>31.038830</td>\n",
       "      <td>20.299332</td>\n",
       "      <td>18.679226</td>\n",
       "      <td>53.993958</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>11.503902</td>\n",
       "      <td>28.409882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.739101</td>\n",
       "      <td>4.939965</td>\n",
       "      <td>35.831978</td>\n",
       "      <td>6.344078</td>\n",
       "      <td>21.629181</td>\n",
       "      <td>18.870523</td>\n",
       "      <td>7.499420</td>\n",
       "      <td>32.450485</td>\n",
       "      <td>36.889999</td>\n",
       "      <td>22.005934</td>\n",
       "      <td>...</td>\n",
       "      <td>44.037506</td>\n",
       "      <td>12.498397</td>\n",
       "      <td>39.641258</td>\n",
       "      <td>31.445143</td>\n",
       "      <td>20.299332</td>\n",
       "      <td>18.673838</td>\n",
       "      <td>55.232574</td>\n",
       "      <td>27.690001</td>\n",
       "      <td>12.792341</td>\n",
       "      <td>28.409882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.732687</td>\n",
       "      <td>4.845691</td>\n",
       "      <td>35.973618</td>\n",
       "      <td>6.386254</td>\n",
       "      <td>21.629181</td>\n",
       "      <td>18.966999</td>\n",
       "      <td>7.484628</td>\n",
       "      <td>32.321415</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>22.132442</td>\n",
       "      <td>...</td>\n",
       "      <td>43.721996</td>\n",
       "      <td>12.504411</td>\n",
       "      <td>39.482250</td>\n",
       "      <td>31.445143</td>\n",
       "      <td>20.299332</td>\n",
       "      <td>18.679226</td>\n",
       "      <td>54.072460</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>12.585275</td>\n",
       "      <td>28.409882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>138.629974</td>\n",
       "      <td>14.310000</td>\n",
       "      <td>60.152214</td>\n",
       "      <td>192.656174</td>\n",
       "      <td>149.475891</td>\n",
       "      <td>106.896759</td>\n",
       "      <td>70.214340</td>\n",
       "      <td>348.842224</td>\n",
       "      <td>598.750000</td>\n",
       "      <td>193.812241</td>\n",
       "      <td>...</td>\n",
       "      <td>89.438622</td>\n",
       "      <td>59.755745</td>\n",
       "      <td>98.548370</td>\n",
       "      <td>34.478691</td>\n",
       "      <td>111.410484</td>\n",
       "      <td>127.463387</td>\n",
       "      <td>119.562088</td>\n",
       "      <td>269.410004</td>\n",
       "      <td>42.445381</td>\n",
       "      <td>193.067841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>138.868347</td>\n",
       "      <td>14.110000</td>\n",
       "      <td>59.828129</td>\n",
       "      <td>192.108871</td>\n",
       "      <td>149.167191</td>\n",
       "      <td>107.092850</td>\n",
       "      <td>69.900551</td>\n",
       "      <td>347.838379</td>\n",
       "      <td>598.260010</td>\n",
       "      <td>195.552704</td>\n",
       "      <td>...</td>\n",
       "      <td>90.012337</td>\n",
       "      <td>59.938690</td>\n",
       "      <td>98.770782</td>\n",
       "      <td>34.791958</td>\n",
       "      <td>112.379784</td>\n",
       "      <td>127.718323</td>\n",
       "      <td>120.655167</td>\n",
       "      <td>275.500000</td>\n",
       "      <td>43.294289</td>\n",
       "      <td>193.582718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>138.878296</td>\n",
       "      <td>13.990000</td>\n",
       "      <td>60.456661</td>\n",
       "      <td>192.208359</td>\n",
       "      <td>149.417999</td>\n",
       "      <td>107.641884</td>\n",
       "      <td>69.929077</td>\n",
       "      <td>348.222168</td>\n",
       "      <td>596.080017</td>\n",
       "      <td>196.024704</td>\n",
       "      <td>...</td>\n",
       "      <td>89.646355</td>\n",
       "      <td>59.754200</td>\n",
       "      <td>98.306625</td>\n",
       "      <td>34.880062</td>\n",
       "      <td>113.131493</td>\n",
       "      <td>128.286987</td>\n",
       "      <td>120.366989</td>\n",
       "      <td>275.790009</td>\n",
       "      <td>42.937355</td>\n",
       "      <td>194.968994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3614</th>\n",
       "      <td>138.828629</td>\n",
       "      <td>13.980000</td>\n",
       "      <td>60.387917</td>\n",
       "      <td>192.636276</td>\n",
       "      <td>149.292603</td>\n",
       "      <td>108.239960</td>\n",
       "      <td>70.423546</td>\n",
       "      <td>346.027466</td>\n",
       "      <td>595.520020</td>\n",
       "      <td>196.899872</td>\n",
       "      <td>...</td>\n",
       "      <td>90.764099</td>\n",
       "      <td>60.094044</td>\n",
       "      <td>96.885117</td>\n",
       "      <td>34.978340</td>\n",
       "      <td>113.072144</td>\n",
       "      <td>127.973251</td>\n",
       "      <td>120.863838</td>\n",
       "      <td>275.350006</td>\n",
       "      <td>43.082058</td>\n",
       "      <td>195.226456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3615</th>\n",
       "      <td>138.327164</td>\n",
       "      <td>13.740000</td>\n",
       "      <td>59.936157</td>\n",
       "      <td>191.591385</td>\n",
       "      <td>149.504837</td>\n",
       "      <td>107.916412</td>\n",
       "      <td>70.623230</td>\n",
       "      <td>345.358246</td>\n",
       "      <td>596.599976</td>\n",
       "      <td>195.247879</td>\n",
       "      <td>...</td>\n",
       "      <td>90.121140</td>\n",
       "      <td>60.113464</td>\n",
       "      <td>96.682037</td>\n",
       "      <td>34.978340</td>\n",
       "      <td>113.111710</td>\n",
       "      <td>128.110519</td>\n",
       "      <td>120.933395</td>\n",
       "      <td>273.329987</td>\n",
       "      <td>42.319973</td>\n",
       "      <td>195.434402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3616 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AdjustedClose_A  AdjustedClose_AAL  AdjustedClose_AAP  \\\n",
       "0           20.053032           4.496876          35.743465   \n",
       "1           19.835201           5.005958          35.531013   \n",
       "2           19.764729           4.798555          35.840828   \n",
       "3           19.739101           4.939965          35.831978   \n",
       "4           19.732687           4.845691          35.973618   \n",
       "...               ...                ...                ...   \n",
       "3611       138.629974          14.310000          60.152214   \n",
       "3612       138.868347          14.110000          59.828129   \n",
       "3613       138.878296          13.990000          60.456661   \n",
       "3614       138.828629          13.980000          60.387917   \n",
       "3615       138.327164          13.740000          59.936157   \n",
       "\n",
       "      AdjustedClose_AAPL  AdjustedClose_ABBV  AdjustedClose_ABT  \\\n",
       "0               6.447411           21.629181          18.763710   \n",
       "1               6.458559           21.629181          18.612122   \n",
       "2               6.355825           21.629181          18.715475   \n",
       "3               6.344078           21.629181          18.870523   \n",
       "4               6.386254           21.629181          18.966999   \n",
       "...                  ...                 ...                ...   \n",
       "3611          192.656174          149.475891         106.896759   \n",
       "3612          192.108871          149.167191         107.092850   \n",
       "3613          192.208359          149.417999         107.641884   \n",
       "3614          192.636276          149.292603         108.239960   \n",
       "3615          191.591385          149.504837         107.916412   \n",
       "\n",
       "      AdjustedClose_ACGL  AdjustedClose_ACN  AdjustedClose_ADBE  \\\n",
       "0               7.601905          31.941786           37.090000   \n",
       "1               7.576549          32.139191           37.700001   \n",
       "2               7.543795          32.480869           37.619999   \n",
       "3               7.499420          32.450485           36.889999   \n",
       "4               7.484628          32.321415           36.689999   \n",
       "...                  ...                ...                 ...   \n",
       "3611           70.214340         348.842224          598.750000   \n",
       "3612           69.900551         347.838379          598.260010   \n",
       "3613           69.929077         348.222168          596.080017   \n",
       "3614           70.423546         346.027466          595.520020   \n",
       "3615           70.623230         345.358246          596.599976   \n",
       "\n",
       "      AdjustedClose_ADI  ...  AdjustedClose_WYNN  AdjustedClose_XEL  \\\n",
       "0             22.258951  ...           41.184948          12.678841   \n",
       "1             22.223808  ...           43.689789          12.528476   \n",
       "2             22.181643  ...           43.116714          12.552535   \n",
       "3             22.005934  ...           44.037506          12.498397   \n",
       "4             22.132442  ...           43.721996          12.504411   \n",
       "...                 ...  ...                 ...                ...   \n",
       "3611         193.812241  ...           89.438622          59.755745   \n",
       "3612         195.552704  ...           90.012337          59.938690   \n",
       "3613         196.024704  ...           89.646355          59.754200   \n",
       "3614         196.899872  ...           90.764099          60.094044   \n",
       "3615         195.247879  ...           90.121140          60.113464   \n",
       "\n",
       "      AdjustedClose_XOM  AdjustedClose_XRAY  AdjustedClose_XYL  \\\n",
       "0             39.272129           31.206663          20.299332   \n",
       "1             39.425449           30.835669          20.299332   \n",
       "2             39.766201           31.038830          20.299332   \n",
       "3             39.641258           31.445143          20.299332   \n",
       "4             39.482250           31.445143          20.299332   \n",
       "...                 ...                 ...                ...   \n",
       "3611          98.548370           34.478691         111.410484   \n",
       "3612          98.770782           34.791958         112.379784   \n",
       "3613          98.306625           34.880062         113.131493   \n",
       "3614          96.885117           34.978340         113.072144   \n",
       "3615          96.682037           34.978340         113.111710   \n",
       "\n",
       "      AdjustedClose_YUM  AdjustedClose_ZBH  AdjustedClose_ZBRA  \\\n",
       "0             18.878279          52.354065           28.670000   \n",
       "1             18.813721          54.011391           28.620001   \n",
       "2             18.679226          53.993958           28.400000   \n",
       "3             18.673838          55.232574           27.690001   \n",
       "4             18.679226          54.072460           27.600000   \n",
       "...                 ...                ...                 ...   \n",
       "3611         127.463387         119.562088          269.410004   \n",
       "3612         127.718323         120.655167          275.500000   \n",
       "3613         128.286987         120.366989          275.790009   \n",
       "3614         127.973251         120.863838          275.350006   \n",
       "3615         128.110519         120.933395          273.329987   \n",
       "\n",
       "      AdjustedClose_ZION  AdjustedClose_ZTS  \n",
       "0              10.223137          28.409882  \n",
       "1              10.583591          28.409882  \n",
       "2              11.503902          28.409882  \n",
       "3              12.792341          28.409882  \n",
       "4              12.585275          28.409882  \n",
       "...                  ...                ...  \n",
       "3611           42.445381         193.067841  \n",
       "3612           43.294289         193.582718  \n",
       "3613           42.937355         194.968994  \n",
       "3614           43.082058         195.226456  \n",
       "3615           42.319973         195.434402  \n",
       "\n",
       "[3616 rows x 500 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdjustedClose_A</th>\n",
       "      <th>AdjustedClose_AAL</th>\n",
       "      <th>AdjustedClose_AAP</th>\n",
       "      <th>AdjustedClose_AAPL</th>\n",
       "      <th>AdjustedClose_ABBV</th>\n",
       "      <th>AdjustedClose_ABT</th>\n",
       "      <th>AdjustedClose_ACGL</th>\n",
       "      <th>AdjustedClose_ACN</th>\n",
       "      <th>AdjustedClose_ADBE</th>\n",
       "      <th>AdjustedClose_ADI</th>\n",
       "      <th>...</th>\n",
       "      <th>AdjustedClose_WYNN</th>\n",
       "      <th>AdjustedClose_XEL</th>\n",
       "      <th>AdjustedClose_XOM</th>\n",
       "      <th>AdjustedClose_XRAY</th>\n",
       "      <th>AdjustedClose_XYL</th>\n",
       "      <th>AdjustedClose_YUM</th>\n",
       "      <th>AdjustedClose_ZBH</th>\n",
       "      <th>AdjustedClose_ZBRA</th>\n",
       "      <th>AdjustedClose_ZION</th>\n",
       "      <th>AdjustedClose_ZTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.048584</td>\n",
       "      <td>13.44</td>\n",
       "      <td>60.839672</td>\n",
       "      <td>184.734985</td>\n",
       "      <td>154.183792</td>\n",
       "      <td>107.700714</td>\n",
       "      <td>71.840385</td>\n",
       "      <td>341.431366</td>\n",
       "      <td>580.070007</td>\n",
       "      <td>190.350952</td>\n",
       "      <td>...</td>\n",
       "      <td>93.573265</td>\n",
       "      <td>61.628197</td>\n",
       "      <td>98.983528</td>\n",
       "      <td>35.017651</td>\n",
       "      <td>112.429237</td>\n",
       "      <td>126.512314</td>\n",
       "      <td>120.625351</td>\n",
       "      <td>267.980011</td>\n",
       "      <td>42.580433</td>\n",
       "      <td>194.642227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130.496964</td>\n",
       "      <td>12.95</td>\n",
       "      <td>59.906696</td>\n",
       "      <td>183.351761</td>\n",
       "      <td>154.801224</td>\n",
       "      <td>107.377167</td>\n",
       "      <td>73.162132</td>\n",
       "      <td>332.573730</td>\n",
       "      <td>571.789978</td>\n",
       "      <td>185.808044</td>\n",
       "      <td>...</td>\n",
       "      <td>93.009445</td>\n",
       "      <td>61.735008</td>\n",
       "      <td>99.815163</td>\n",
       "      <td>34.221577</td>\n",
       "      <td>110.342270</td>\n",
       "      <td>126.580956</td>\n",
       "      <td>119.711151</td>\n",
       "      <td>252.520004</td>\n",
       "      <td>40.516045</td>\n",
       "      <td>191.037918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130.337784</td>\n",
       "      <td>13.09</td>\n",
       "      <td>59.405842</td>\n",
       "      <td>181.023178</td>\n",
       "      <td>155.765961</td>\n",
       "      <td>108.808601</td>\n",
       "      <td>73.666107</td>\n",
       "      <td>331.756866</td>\n",
       "      <td>567.049988</td>\n",
       "      <td>182.966232</td>\n",
       "      <td>...</td>\n",
       "      <td>93.187492</td>\n",
       "      <td>61.880653</td>\n",
       "      <td>98.944847</td>\n",
       "      <td>35.066795</td>\n",
       "      <td>111.103867</td>\n",
       "      <td>126.178955</td>\n",
       "      <td>119.442841</td>\n",
       "      <td>252.970001</td>\n",
       "      <td>41.133430</td>\n",
       "      <td>192.137054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129.899979</td>\n",
       "      <td>13.60</td>\n",
       "      <td>60.417381</td>\n",
       "      <td>180.296707</td>\n",
       "      <td>156.421967</td>\n",
       "      <td>108.632126</td>\n",
       "      <td>73.038513</td>\n",
       "      <td>331.294312</td>\n",
       "      <td>564.599976</td>\n",
       "      <td>183.438217</td>\n",
       "      <td>...</td>\n",
       "      <td>94.611870</td>\n",
       "      <td>61.880653</td>\n",
       "      <td>99.244621</td>\n",
       "      <td>34.771950</td>\n",
       "      <td>110.925835</td>\n",
       "      <td>125.835777</td>\n",
       "      <td>119.224236</td>\n",
       "      <td>252.690002</td>\n",
       "      <td>42.493610</td>\n",
       "      <td>192.939117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132.705734</td>\n",
       "      <td>14.58</td>\n",
       "      <td>60.800385</td>\n",
       "      <td>184.655365</td>\n",
       "      <td>155.737015</td>\n",
       "      <td>110.200813</td>\n",
       "      <td>73.333298</td>\n",
       "      <td>334.965332</td>\n",
       "      <td>580.549988</td>\n",
       "      <td>185.827698</td>\n",
       "      <td>...</td>\n",
       "      <td>95.996674</td>\n",
       "      <td>61.735008</td>\n",
       "      <td>97.591034</td>\n",
       "      <td>35.283012</td>\n",
       "      <td>111.618187</td>\n",
       "      <td>126.129936</td>\n",
       "      <td>121.201706</td>\n",
       "      <td>261.089996</td>\n",
       "      <td>42.966301</td>\n",
       "      <td>194.226349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>130.019363</td>\n",
       "      <td>14.38</td>\n",
       "      <td>60.289703</td>\n",
       "      <td>184.237411</td>\n",
       "      <td>156.585983</td>\n",
       "      <td>110.524361</td>\n",
       "      <td>72.410927</td>\n",
       "      <td>337.327332</td>\n",
       "      <td>586.200012</td>\n",
       "      <td>186.594666</td>\n",
       "      <td>...</td>\n",
       "      <td>94.226097</td>\n",
       "      <td>61.434002</td>\n",
       "      <td>96.382263</td>\n",
       "      <td>35.902184</td>\n",
       "      <td>110.886269</td>\n",
       "      <td>125.718124</td>\n",
       "      <td>121.102341</td>\n",
       "      <td>256.440002</td>\n",
       "      <td>42.483967</td>\n",
       "      <td>194.018402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>130.427307</td>\n",
       "      <td>14.35</td>\n",
       "      <td>59.887058</td>\n",
       "      <td>185.282303</td>\n",
       "      <td>159.074982</td>\n",
       "      <td>112.161690</td>\n",
       "      <td>73.181152</td>\n",
       "      <td>339.866486</td>\n",
       "      <td>591.030029</td>\n",
       "      <td>185.227859</td>\n",
       "      <td>...</td>\n",
       "      <td>92.613777</td>\n",
       "      <td>60.899960</td>\n",
       "      <td>95.434593</td>\n",
       "      <td>36.747402</td>\n",
       "      <td>111.014839</td>\n",
       "      <td>126.679001</td>\n",
       "      <td>122.453773</td>\n",
       "      <td>255.270004</td>\n",
       "      <td>42.377853</td>\n",
       "      <td>196.820679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>129.024429</td>\n",
       "      <td>14.59</td>\n",
       "      <td>61.563461</td>\n",
       "      <td>184.685196</td>\n",
       "      <td>158.004135</td>\n",
       "      <td>111.816887</td>\n",
       "      <td>73.808746</td>\n",
       "      <td>342.464722</td>\n",
       "      <td>597.489990</td>\n",
       "      <td>185.808044</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880859</td>\n",
       "      <td>59.229870</td>\n",
       "      <td>95.415245</td>\n",
       "      <td>35.823559</td>\n",
       "      <td>110.451073</td>\n",
       "      <td>126.482903</td>\n",
       "      <td>121.609116</td>\n",
       "      <td>255.029999</td>\n",
       "      <td>41.808697</td>\n",
       "      <td>194.780869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>129.880066</td>\n",
       "      <td>13.21</td>\n",
       "      <td>60.449146</td>\n",
       "      <td>185.013596</td>\n",
       "      <td>158.169708</td>\n",
       "      <td>112.230652</td>\n",
       "      <td>73.875305</td>\n",
       "      <td>350.692444</td>\n",
       "      <td>596.539978</td>\n",
       "      <td>185.218033</td>\n",
       "      <td>...</td>\n",
       "      <td>93.088570</td>\n",
       "      <td>59.530880</td>\n",
       "      <td>96.653023</td>\n",
       "      <td>35.931667</td>\n",
       "      <td>110.510414</td>\n",
       "      <td>126.610374</td>\n",
       "      <td>122.294769</td>\n",
       "      <td>248.869995</td>\n",
       "      <td>40.622158</td>\n",
       "      <td>196.988998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>129.870132</td>\n",
       "      <td>13.19</td>\n",
       "      <td>60.547760</td>\n",
       "      <td>182.734787</td>\n",
       "      <td>157.273666</td>\n",
       "      <td>112.073021</td>\n",
       "      <td>74.331734</td>\n",
       "      <td>348.950470</td>\n",
       "      <td>597.679993</td>\n",
       "      <td>184.657532</td>\n",
       "      <td>...</td>\n",
       "      <td>93.533691</td>\n",
       "      <td>59.142483</td>\n",
       "      <td>94.467583</td>\n",
       "      <td>35.332153</td>\n",
       "      <td>109.748817</td>\n",
       "      <td>127.277100</td>\n",
       "      <td>121.867485</td>\n",
       "      <td>243.520004</td>\n",
       "      <td>39.705719</td>\n",
       "      <td>191.097366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>129.870132</td>\n",
       "      <td>13.19</td>\n",
       "      <td>60.547760</td>\n",
       "      <td>182.734787</td>\n",
       "      <td>157.273666</td>\n",
       "      <td>112.073021</td>\n",
       "      <td>74.331734</td>\n",
       "      <td>348.950470</td>\n",
       "      <td>597.679993</td>\n",
       "      <td>184.657532</td>\n",
       "      <td>...</td>\n",
       "      <td>93.533691</td>\n",
       "      <td>59.142483</td>\n",
       "      <td>94.467583</td>\n",
       "      <td>35.332153</td>\n",
       "      <td>109.748817</td>\n",
       "      <td>127.277100</td>\n",
       "      <td>121.867485</td>\n",
       "      <td>243.520004</td>\n",
       "      <td>39.705719</td>\n",
       "      <td>191.097366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>127.472313</td>\n",
       "      <td>12.93</td>\n",
       "      <td>60.192757</td>\n",
       "      <td>181.789398</td>\n",
       "      <td>157.819077</td>\n",
       "      <td>112.240501</td>\n",
       "      <td>74.807182</td>\n",
       "      <td>349.325836</td>\n",
       "      <td>596.099976</td>\n",
       "      <td>182.336884</td>\n",
       "      <td>...</td>\n",
       "      <td>90.170601</td>\n",
       "      <td>58.346279</td>\n",
       "      <td>93.781006</td>\n",
       "      <td>34.909538</td>\n",
       "      <td>108.492683</td>\n",
       "      <td>126.561356</td>\n",
       "      <td>121.460068</td>\n",
       "      <td>239.169998</td>\n",
       "      <td>39.493492</td>\n",
       "      <td>189.027832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>129.800507</td>\n",
       "      <td>13.82</td>\n",
       "      <td>61.307068</td>\n",
       "      <td>187.710419</td>\n",
       "      <td>159.046280</td>\n",
       "      <td>112.329163</td>\n",
       "      <td>75.082947</td>\n",
       "      <td>355.153656</td>\n",
       "      <td>593.700012</td>\n",
       "      <td>186.801178</td>\n",
       "      <td>...</td>\n",
       "      <td>93.078697</td>\n",
       "      <td>57.919048</td>\n",
       "      <td>93.606941</td>\n",
       "      <td>35.292839</td>\n",
       "      <td>110.184021</td>\n",
       "      <td>127.914421</td>\n",
       "      <td>123.049980</td>\n",
       "      <td>249.589996</td>\n",
       "      <td>39.088337</td>\n",
       "      <td>189.059601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>130.556656</td>\n",
       "      <td>13.66</td>\n",
       "      <td>62.865135</td>\n",
       "      <td>190.626114</td>\n",
       "      <td>160.477982</td>\n",
       "      <td>112.989227</td>\n",
       "      <td>76.908669</td>\n",
       "      <td>359.450500</td>\n",
       "      <td>611.549988</td>\n",
       "      <td>191.521103</td>\n",
       "      <td>...</td>\n",
       "      <td>93.207283</td>\n",
       "      <td>57.753979</td>\n",
       "      <td>93.751984</td>\n",
       "      <td>35.047138</td>\n",
       "      <td>110.648888</td>\n",
       "      <td>129.718506</td>\n",
       "      <td>122.582947</td>\n",
       "      <td>249.089996</td>\n",
       "      <td>40.149467</td>\n",
       "      <td>186.955627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>131.879929</td>\n",
       "      <td>13.62</td>\n",
       "      <td>64.531685</td>\n",
       "      <td>192.944748</td>\n",
       "      <td>161.081818</td>\n",
       "      <td>112.496651</td>\n",
       "      <td>76.518806</td>\n",
       "      <td>362.720032</td>\n",
       "      <td>603.590027</td>\n",
       "      <td>194.018723</td>\n",
       "      <td>...</td>\n",
       "      <td>91.070717</td>\n",
       "      <td>57.219940</td>\n",
       "      <td>93.626282</td>\n",
       "      <td>35.646652</td>\n",
       "      <td>112.142403</td>\n",
       "      <td>128.502716</td>\n",
       "      <td>121.817795</td>\n",
       "      <td>254.570007</td>\n",
       "      <td>41.779762</td>\n",
       "      <td>189.754303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>130.974533</td>\n",
       "      <td>14.00</td>\n",
       "      <td>65.162796</td>\n",
       "      <td>194.228455</td>\n",
       "      <td>163.136871</td>\n",
       "      <td>112.309471</td>\n",
       "      <td>76.395187</td>\n",
       "      <td>365.337616</td>\n",
       "      <td>597.179993</td>\n",
       "      <td>198.011002</td>\n",
       "      <td>...</td>\n",
       "      <td>92.099442</td>\n",
       "      <td>57.472393</td>\n",
       "      <td>94.680321</td>\n",
       "      <td>34.880062</td>\n",
       "      <td>112.518257</td>\n",
       "      <td>129.179260</td>\n",
       "      <td>120.317307</td>\n",
       "      <td>256.089996</td>\n",
       "      <td>41.258835</td>\n",
       "      <td>186.677765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>129.123932</td>\n",
       "      <td>13.93</td>\n",
       "      <td>64.531685</td>\n",
       "      <td>193.551773</td>\n",
       "      <td>159.942307</td>\n",
       "      <td>109.127365</td>\n",
       "      <td>76.195496</td>\n",
       "      <td>364.389343</td>\n",
       "      <td>606.479980</td>\n",
       "      <td>194.992233</td>\n",
       "      <td>...</td>\n",
       "      <td>93.266624</td>\n",
       "      <td>56.530544</td>\n",
       "      <td>96.314575</td>\n",
       "      <td>34.074150</td>\n",
       "      <td>110.480743</td>\n",
       "      <td>126.914330</td>\n",
       "      <td>119.760834</td>\n",
       "      <td>254.169998</td>\n",
       "      <td>42.580433</td>\n",
       "      <td>184.663101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>129.223419</td>\n",
       "      <td>15.36</td>\n",
       "      <td>65.212097</td>\n",
       "      <td>193.223389</td>\n",
       "      <td>160.828613</td>\n",
       "      <td>110.782448</td>\n",
       "      <td>75.691521</td>\n",
       "      <td>367.392212</td>\n",
       "      <td>622.580017</td>\n",
       "      <td>194.008896</td>\n",
       "      <td>...</td>\n",
       "      <td>95.363617</td>\n",
       "      <td>57.181103</td>\n",
       "      <td>98.761116</td>\n",
       "      <td>34.447617</td>\n",
       "      <td>111.746765</td>\n",
       "      <td>127.679085</td>\n",
       "      <td>121.191765</td>\n",
       "      <td>256.989990</td>\n",
       "      <td>42.725136</td>\n",
       "      <td>185.903656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>130.327835</td>\n",
       "      <td>15.13</td>\n",
       "      <td>65.330437</td>\n",
       "      <td>191.481918</td>\n",
       "      <td>160.117599</td>\n",
       "      <td>110.348976</td>\n",
       "      <td>75.976791</td>\n",
       "      <td>366.532837</td>\n",
       "      <td>613.929993</td>\n",
       "      <td>190.695114</td>\n",
       "      <td>...</td>\n",
       "      <td>95.680145</td>\n",
       "      <td>57.666592</td>\n",
       "      <td>99.602417</td>\n",
       "      <td>34.319855</td>\n",
       "      <td>111.163208</td>\n",
       "      <td>126.571152</td>\n",
       "      <td>120.923470</td>\n",
       "      <td>252.169998</td>\n",
       "      <td>42.464668</td>\n",
       "      <td>190.071899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>132.158524</td>\n",
       "      <td>14.94</td>\n",
       "      <td>66.030586</td>\n",
       "      <td>190.795288</td>\n",
       "      <td>159.640381</td>\n",
       "      <td>111.767616</td>\n",
       "      <td>75.815140</td>\n",
       "      <td>368.784973</td>\n",
       "      <td>630.229980</td>\n",
       "      <td>193.910568</td>\n",
       "      <td>...</td>\n",
       "      <td>96.006561</td>\n",
       "      <td>57.928757</td>\n",
       "      <td>99.728127</td>\n",
       "      <td>34.811256</td>\n",
       "      <td>112.122612</td>\n",
       "      <td>128.002670</td>\n",
       "      <td>122.145721</td>\n",
       "      <td>255.929993</td>\n",
       "      <td>43.342518</td>\n",
       "      <td>191.302521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>133.601181</td>\n",
       "      <td>14.53</td>\n",
       "      <td>66.316559</td>\n",
       "      <td>187.123276</td>\n",
       "      <td>160.624069</td>\n",
       "      <td>111.797180</td>\n",
       "      <td>76.623405</td>\n",
       "      <td>366.760010</td>\n",
       "      <td>627.960022</td>\n",
       "      <td>191.491608</td>\n",
       "      <td>...</td>\n",
       "      <td>93.959023</td>\n",
       "      <td>58.025852</td>\n",
       "      <td>101.391388</td>\n",
       "      <td>34.427963</td>\n",
       "      <td>112.508362</td>\n",
       "      <td>128.071289</td>\n",
       "      <td>121.599190</td>\n",
       "      <td>250.910004</td>\n",
       "      <td>42.869827</td>\n",
       "      <td>189.982559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    AdjustedClose_A  AdjustedClose_AAL  AdjustedClose_AAP  AdjustedClose_AAPL  \\\n",
       "0        138.048584              13.44          60.839672          184.734985   \n",
       "1        130.496964              12.95          59.906696          183.351761   \n",
       "2        130.337784              13.09          59.405842          181.023178   \n",
       "3        129.899979              13.60          60.417381          180.296707   \n",
       "4        132.705734              14.58          60.800385          184.655365   \n",
       "5        130.019363              14.38          60.289703          184.237411   \n",
       "6        130.427307              14.35          59.887058          185.282303   \n",
       "7        129.024429              14.59          61.563461          184.685196   \n",
       "8        129.880066              13.21          60.449146          185.013596   \n",
       "9        129.870132              13.19          60.547760          182.734787   \n",
       "10       129.870132              13.19          60.547760          182.734787   \n",
       "11       127.472313              12.93          60.192757          181.789398   \n",
       "12       129.800507              13.82          61.307068          187.710419   \n",
       "13       130.556656              13.66          62.865135          190.626114   \n",
       "14       131.879929              13.62          64.531685          192.944748   \n",
       "15       130.974533              14.00          65.162796          194.228455   \n",
       "16       129.123932              13.93          64.531685          193.551773   \n",
       "17       129.223419              15.36          65.212097          193.223389   \n",
       "18       130.327835              15.13          65.330437          191.481918   \n",
       "19       132.158524              14.94          66.030586          190.795288   \n",
       "20       133.601181              14.53          66.316559          187.123276   \n",
       "\n",
       "    AdjustedClose_ABBV  AdjustedClose_ABT  AdjustedClose_ACGL  \\\n",
       "0           154.183792         107.700714           71.840385   \n",
       "1           154.801224         107.377167           73.162132   \n",
       "2           155.765961         108.808601           73.666107   \n",
       "3           156.421967         108.632126           73.038513   \n",
       "4           155.737015         110.200813           73.333298   \n",
       "5           156.585983         110.524361           72.410927   \n",
       "6           159.074982         112.161690           73.181152   \n",
       "7           158.004135         111.816887           73.808746   \n",
       "8           158.169708         112.230652           73.875305   \n",
       "9           157.273666         112.073021           74.331734   \n",
       "10          157.273666         112.073021           74.331734   \n",
       "11          157.819077         112.240501           74.807182   \n",
       "12          159.046280         112.329163           75.082947   \n",
       "13          160.477982         112.989227           76.908669   \n",
       "14          161.081818         112.496651           76.518806   \n",
       "15          163.136871         112.309471           76.395187   \n",
       "16          159.942307         109.127365           76.195496   \n",
       "17          160.828613         110.782448           75.691521   \n",
       "18          160.117599         110.348976           75.976791   \n",
       "19          159.640381         111.767616           75.815140   \n",
       "20          160.624069         111.797180           76.623405   \n",
       "\n",
       "    AdjustedClose_ACN  AdjustedClose_ADBE  AdjustedClose_ADI  ...  \\\n",
       "0          341.431366          580.070007         190.350952  ...   \n",
       "1          332.573730          571.789978         185.808044  ...   \n",
       "2          331.756866          567.049988         182.966232  ...   \n",
       "3          331.294312          564.599976         183.438217  ...   \n",
       "4          334.965332          580.549988         185.827698  ...   \n",
       "5          337.327332          586.200012         186.594666  ...   \n",
       "6          339.866486          591.030029         185.227859  ...   \n",
       "7          342.464722          597.489990         185.808044  ...   \n",
       "8          350.692444          596.539978         185.218033  ...   \n",
       "9          348.950470          597.679993         184.657532  ...   \n",
       "10         348.950470          597.679993         184.657532  ...   \n",
       "11         349.325836          596.099976         182.336884  ...   \n",
       "12         355.153656          593.700012         186.801178  ...   \n",
       "13         359.450500          611.549988         191.521103  ...   \n",
       "14         362.720032          603.590027         194.018723  ...   \n",
       "15         365.337616          597.179993         198.011002  ...   \n",
       "16         364.389343          606.479980         194.992233  ...   \n",
       "17         367.392212          622.580017         194.008896  ...   \n",
       "18         366.532837          613.929993         190.695114  ...   \n",
       "19         368.784973          630.229980         193.910568  ...   \n",
       "20         366.760010          627.960022         191.491608  ...   \n",
       "\n",
       "    AdjustedClose_WYNN  AdjustedClose_XEL  AdjustedClose_XOM  \\\n",
       "0            93.573265          61.628197          98.983528   \n",
       "1            93.009445          61.735008          99.815163   \n",
       "2            93.187492          61.880653          98.944847   \n",
       "3            94.611870          61.880653          99.244621   \n",
       "4            95.996674          61.735008          97.591034   \n",
       "5            94.226097          61.434002          96.382263   \n",
       "6            92.613777          60.899960          95.434593   \n",
       "7            92.880859          59.229870          95.415245   \n",
       "8            93.088570          59.530880          96.653023   \n",
       "9            93.533691          59.142483          94.467583   \n",
       "10           93.533691          59.142483          94.467583   \n",
       "11           90.170601          58.346279          93.781006   \n",
       "12           93.078697          57.919048          93.606941   \n",
       "13           93.207283          57.753979          93.751984   \n",
       "14           91.070717          57.219940          93.626282   \n",
       "15           92.099442          57.472393          94.680321   \n",
       "16           93.266624          56.530544          96.314575   \n",
       "17           95.363617          57.181103          98.761116   \n",
       "18           95.680145          57.666592          99.602417   \n",
       "19           96.006561          57.928757          99.728127   \n",
       "20           93.959023          58.025852         101.391388   \n",
       "\n",
       "    AdjustedClose_XRAY  AdjustedClose_XYL  AdjustedClose_YUM  \\\n",
       "0            35.017651         112.429237         126.512314   \n",
       "1            34.221577         110.342270         126.580956   \n",
       "2            35.066795         111.103867         126.178955   \n",
       "3            34.771950         110.925835         125.835777   \n",
       "4            35.283012         111.618187         126.129936   \n",
       "5            35.902184         110.886269         125.718124   \n",
       "6            36.747402         111.014839         126.679001   \n",
       "7            35.823559         110.451073         126.482903   \n",
       "8            35.931667         110.510414         126.610374   \n",
       "9            35.332153         109.748817         127.277100   \n",
       "10           35.332153         109.748817         127.277100   \n",
       "11           34.909538         108.492683         126.561356   \n",
       "12           35.292839         110.184021         127.914421   \n",
       "13           35.047138         110.648888         129.718506   \n",
       "14           35.646652         112.142403         128.502716   \n",
       "15           34.880062         112.518257         129.179260   \n",
       "16           34.074150         110.480743         126.914330   \n",
       "17           34.447617         111.746765         127.679085   \n",
       "18           34.319855         111.163208         126.571152   \n",
       "19           34.811256         112.122612         128.002670   \n",
       "20           34.427963         112.508362         128.071289   \n",
       "\n",
       "    AdjustedClose_ZBH  AdjustedClose_ZBRA  AdjustedClose_ZION  \\\n",
       "0          120.625351          267.980011           42.580433   \n",
       "1          119.711151          252.520004           40.516045   \n",
       "2          119.442841          252.970001           41.133430   \n",
       "3          119.224236          252.690002           42.493610   \n",
       "4          121.201706          261.089996           42.966301   \n",
       "5          121.102341          256.440002           42.483967   \n",
       "6          122.453773          255.270004           42.377853   \n",
       "7          121.609116          255.029999           41.808697   \n",
       "8          122.294769          248.869995           40.622158   \n",
       "9          121.867485          243.520004           39.705719   \n",
       "10         121.867485          243.520004           39.705719   \n",
       "11         121.460068          239.169998           39.493492   \n",
       "12         123.049980          249.589996           39.088337   \n",
       "13         122.582947          249.089996           40.149467   \n",
       "14         121.817795          254.570007           41.779762   \n",
       "15         120.317307          256.089996           41.258835   \n",
       "16         119.760834          254.169998           42.580433   \n",
       "17         121.191765          256.989990           42.725136   \n",
       "18         120.923470          252.169998           42.464668   \n",
       "19         122.145721          255.929993           43.342518   \n",
       "20         121.599190          250.910004           42.869827   \n",
       "\n",
       "    AdjustedClose_ZTS  \n",
       "0          194.642227  \n",
       "1          191.037918  \n",
       "2          192.137054  \n",
       "3          192.939117  \n",
       "4          194.226349  \n",
       "5          194.018402  \n",
       "6          196.820679  \n",
       "7          194.780869  \n",
       "8          196.988998  \n",
       "9          191.097366  \n",
       "10         191.097366  \n",
       "11         189.027832  \n",
       "12         189.059601  \n",
       "13         186.955627  \n",
       "14         189.754303  \n",
       "15         186.677765  \n",
       "16         184.663101  \n",
       "17         185.903656  \n",
       "18         190.071899  \n",
       "19         191.302521  \n",
       "20         189.982559  \n",
       "\n",
       "[21 rows x 500 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3637, 500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.75005242,  1.75740624,  1.72655083, ..., -1.61078119,\n",
       "         1.86025205, -0.70525103],\n",
       "       [ 1.6878497 ,  1.58950324,  1.72194064, ..., -1.61078119,\n",
       "         1.86025205,  0.00352822],\n",
       "       [ 1.53174227,  1.56790155,  1.54117521, ..., -1.61078119,\n",
       "         1.86025205,  0.71230746],\n",
       "       ...,\n",
       "       [ 1.59730729,  1.57894773,  1.61784899, ..., -1.61078119,\n",
       "         1.86025205,  1.42108671],\n",
       "       [ 1.56536527,  1.59465795,  1.57029173, ..., -1.61078119,\n",
       "         1.86025205, -1.41403028],\n",
       "       [ 1.60427229,  1.62926964,  1.5964967 , ..., -1.61078119,\n",
       "         1.86025205, -0.70525103]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(21, 500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>64.273168</td>\n",
       "      <td>24.464557</td>\n",
       "      <td>115.136796</td>\n",
       "      <td>57.670481</td>\n",
       "      <td>60.939588</td>\n",
       "      <td>55.750927</td>\n",
       "      <td>27.705658</td>\n",
       "      <td>138.736347</td>\n",
       "      <td>205.964436</td>\n",
       "      <td>78.381387</td>\n",
       "      <td>...</td>\n",
       "      <td>102.141455</td>\n",
       "      <td>36.933363</td>\n",
       "      <td>58.258436</td>\n",
       "      <td>43.123386</td>\n",
       "      <td>54.487089</td>\n",
       "      <td>67.480185</td>\n",
       "      <td>97.213288</td>\n",
       "      <td>159.536612</td>\n",
       "      <td>29.890582</td>\n",
       "      <td>81.599657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1           2          3          4          5    \\\n",
       "0   64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "1   64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "2   64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "3   64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "4   64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "5   64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "6   64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "7   64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "8   64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "9   64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "10  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "11  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "12  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "13  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "14  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "15  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "16  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "17  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "18  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "19  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "20  64.273168  24.464557  115.136796  57.670481  60.939588  55.750927   \n",
       "\n",
       "          6           7           8          9    ...         490        491  \\\n",
       "0   27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "1   27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "2   27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "3   27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "4   27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "5   27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "6   27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "7   27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "8   27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "9   27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "10  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "11  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "12  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "13  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "14  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "15  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "16  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "17  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "18  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "19  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "20  27.705658  138.736347  205.964436  78.381387  ...  102.141455  36.933363   \n",
       "\n",
       "          492        493        494        495        496         497  \\\n",
       "0   58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "1   58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "2   58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "3   58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "4   58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "5   58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "6   58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "7   58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "8   58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "9   58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "10  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "11  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "12  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "13  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "14  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "15  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "16  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "17  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "18  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "19  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "20  58.258436  43.123386  54.487089  67.480185  97.213288  159.536612   \n",
       "\n",
       "          498        499  \n",
       "0   29.890582  81.599657  \n",
       "1   29.890582  81.599657  \n",
       "2   29.890582  81.599657  \n",
       "3   29.890582  81.599657  \n",
       "4   29.890582  81.599657  \n",
       "5   29.890582  81.599657  \n",
       "6   29.890582  81.599657  \n",
       "7   29.890582  81.599657  \n",
       "8   29.890582  81.599657  \n",
       "9   29.890582  81.599657  \n",
       "10  29.890582  81.599657  \n",
       "11  29.890582  81.599657  \n",
       "12  29.890582  81.599657  \n",
       "13  29.890582  81.599657  \n",
       "14  29.890582  81.599657  \n",
       "15  29.890582  81.599657  \n",
       "16  29.890582  81.599657  \n",
       "17  29.890582  81.599657  \n",
       "18  29.890582  81.599657  \n",
       "19  29.890582  81.599657  \n",
       "20  29.890582  81.599657  \n",
       "\n",
       "[21 rows x 500 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train)\n",
    "\n",
    "display(y_test)\n",
    "historic_prices=pd.concat([y_train,y_test],axis=0,ignore_index=True)\n",
    "display(historic_prices.shape)\n",
    "historic_prices.to_csv('historic_prices.csv')\n",
    "\n",
    "\n",
    "sys=DummyRegressor(strategy='mean')\n",
    "sys.fit(X_train_pca,y_train)\n",
    "preds=sys.predict(X_test)\n",
    "display(X_test)\n",
    "display(preds.shape)\n",
    "display(type(preds))\n",
    "\n",
    "preds_df=pd.DataFrame(preds)\n",
    "display(preds_df)\n",
    "preds_df.to_csv('predicciones_dummy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Model \n",
    "\n",
    "### **Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_30316\\2615889880.py:310: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  execution_times = pd.concat([execution_times, new_row], ignore_index=True)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.82126e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.82126e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.02883e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=8.02883e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.68324e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.68324e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.50587e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.50587e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.15263e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.15263e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.90449e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.90449e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.1653e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.1653e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.68073e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.68073e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33235e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.33235e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.09517e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.09517e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.09953e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.09953e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.946e+00, tolerance: 3.464e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.031e+01, tolerance: 2.944e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.659e+01, tolerance: 9.059e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.454e+01, tolerance: 1.115e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.318e+01, tolerance: 1.200e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.038e+01, tolerance: 1.235e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.597e+01, tolerance: 2.065e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.635e+01, tolerance: 1.319e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.768e+01, tolerance: 3.379e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.066e+01, tolerance: 1.414e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.154e+01, tolerance: 6.629e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.054e+02, tolerance: 1.842e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.575e+01, tolerance: 1.506e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.211e+02, tolerance: 3.463e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.187e+02, tolerance: 6.776e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.357e+02, tolerance: 4.896e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.417e+02, tolerance: 1.530e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.505e+02, tolerance: 6.225e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+02, tolerance: 2.896e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.657e+02, tolerance: 6.781e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.931e+02, tolerance: 3.537e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.819e+02, tolerance: 7.738e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+02, tolerance: 3.897e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.984e+02, tolerance: 9.833e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.307e+02, tolerance: 4.287e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.140e+02, tolerance: 1.541e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.503e+02, tolerance: 4.946e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.352e+02, tolerance: 2.770e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.736e+02, tolerance: 5.696e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.573e+02, tolerance: 3.807e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.967e+02, tolerance: 6.147e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.828e+02, tolerance: 4.998e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.182e+02, tolerance: 6.235e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.109e+02, tolerance: 6.381e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.333e+02, tolerance: 6.252e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.504e+02, tolerance: 8.125e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.518e+02, tolerance: 6.299e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.853e+02, tolerance: 1.084e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.689e+02, tolerance: 6.646e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.483e+02, tolerance: 1.770e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.853e+02, tolerance: 6.809e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.145e+02, tolerance: 3.131e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.999e+02, tolerance: 6.863e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.621e+02, tolerance: 4.075e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.157e+02, tolerance: 6.980e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.628e+02, tolerance: 4.748e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.316e+02, tolerance: 7.187e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.336e+02, tolerance: 5.566e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.475e+02, tolerance: 7.359e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.141e-01, tolerance: 4.720e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.252e+00, tolerance: 3.464e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.355e+01, tolerance: 2.944e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.148e+01, tolerance: 9.059e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+01, tolerance: 1.115e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.006e+01, tolerance: 1.200e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.810e+01, tolerance: 1.235e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.747e+01, tolerance: 2.065e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.786e+01, tolerance: 1.319e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.938e+01, tolerance: 3.379e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.658e+01, tolerance: 1.414e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.720e+01, tolerance: 6.629e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.207e+01, tolerance: 1.842e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.577e+01, tolerance: 1.506e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.482e+01, tolerance: 3.463e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.183e+01, tolerance: 6.776e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.359e+01, tolerance: 4.896e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+01, tolerance: 1.530e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.275e+01, tolerance: 6.225e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 276\u001b[0m\n\u001b[0;32m    273\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;241m~\u001b[39mX_train\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^Unnamed\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo optimizado en el fold actual\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Predecir en el conjunto de validación\u001b[39;00m\n\u001b[0;32m    279\u001b[0m predictions \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_val_fold)\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\multioutput.py:278\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\multioutput.py:67\u001b[0m, in \u001b[0;36m_fit_estimator\u001b[1;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m     65\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1077\u001b[0m, in \u001b[0;36mElasticNet.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1076\u001b[0m     this_Xy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1077\u001b[0m _, this_coef, this_dual_gap, this_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43malphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthis_Xy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# from here on **params\u001b[39;49;00m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m coef_[k] \u001b[38;5;241m=\u001b[39m this_coef[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1102\u001b[0m dual_gaps_[k] \u001b[38;5;241m=\u001b[39m this_dual_gap[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697\u001b[0m, in \u001b[0;36menet_path\u001b[1;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[0;32m    683\u001b[0m     model \u001b[38;5;241m=\u001b[39m cd_fast\u001b[38;5;241m.\u001b[39menet_coordinate_descent_gram(\n\u001b[0;32m    684\u001b[0m         coef_,\n\u001b[0;32m    685\u001b[0m         l1_reg,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    694\u001b[0m         positive,\n\u001b[0;32m    695\u001b[0m     )\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m precompute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m--> 697\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcd_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menet_coordinate_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecompute should be one of True, False, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or array-like. Got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;241m%\u001b[39m precompute\n\u001b[0;32m    704\u001b[0m     )\n",
      "File \u001b[1;32m_cd_fast.pyx:262\u001b[0m, in \u001b[0;36msklearn.linear_model._cd_fast.enet_coordinate_descent\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\numpy\\core\\getlimits.py:484\u001b[0m, in \u001b[0;36mfinfo.__new__\u001b[1;34m(cls, dtype)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03mfinfo(dtype)\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    482\u001b[0m _finfo_cache \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dtype):\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    486\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_finfo_cache\u001b[38;5;241m.\u001b[39mget(dtype)  \u001b[38;5;66;03m# most common path\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics={\n",
    "    'MSE':[]\n",
    "    ,'MAE':[] \n",
    "    ,'RMSE':[]\n",
    "    ,'R2':[]\n",
    "    ,'MAPE':[]\n",
    "}\n",
    "scoring = {\n",
    "    'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "    'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False),\n",
    "    'R2': make_scorer(r2_score),\n",
    "    'MAPE': make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "}\n",
    "models = {\n",
    "    'DummyRegressor': MultiOutputRegressor(DummyRegressor()),  # Baseline sin ningún ajuste\n",
    "    'Poisson': MultiOutputRegressor(PoissonRegressor(alpha=1.0, max_iter=100)),  # Aumentamos alpha y limitamos el número de iteraciones\n",
    "    'Tweedie': MultiOutputRegressor(TweedieRegressor(alpha=1.0, max_iter=100)),  # Aumentamos alpha y limitamos el número de iteraciones\n",
    "    'Ridge': MultiOutputRegressor(Ridge(alpha=1.0)),  # Establecemos alpha para controlar la regularización\n",
    "    'Lasso': MultiOutputRegressor(Lasso(alpha=0.1)),  # Establecemos alpha para limitar la complejidad\n",
    "    'ElasticNet': MultiOutputRegressor(ElasticNet(alpha=0.1, l1_ratio=0.5)),  # Reducimos alpha y ajustamos l1_ratio\n",
    "    'BayesianRidge': MultiOutputRegressor(BayesianRidge()),  # Sin cambios (modelo simple por defecto)\n",
    "    'ARDRegression': MultiOutputRegressor(ARDRegression(alpha_1=1e-6, alpha_2=1e-6)),  # Aumentamos los valores de alpha para mayor regularización\n",
    "    'Huber': MultiOutputRegressor(HuberRegressor(alpha=0.1)),  # Establecemos alpha para controlar la regularización\n",
    "    'PassiveAggressive': MultiOutputRegressor(PassiveAggressiveRegressor(C=0.5, max_iter=100)),  # Reducimos C y limitamos el número de iteraciones\n",
    "    'KNN': MultiOutputRegressor(KNeighborsRegressor(n_neighbors=45, n_jobs=1)),  # Reducimos el número de vecinos\n",
    "    'LinearRegression': MultiOutputRegressor(LinearRegression(n_jobs=1)),  # Sin cambios (es un modelo simple por defecto)\n",
    "    'DecisionTree': MultiOutputRegressor(DecisionTreeRegressor(max_depth=4)),  # Reducimos la profundidad máxima\n",
    "    'SVR': MultiOutputRegressor(SVR(C=0.5)),  # Reducimos el valor de C para limitar la complejidad\n",
    "    'GaussianProcess': MultiOutputRegressor(GaussianProcessRegressor(alpha=1e-1)),  # Aumentamos alpha para agregar regularización\n",
    "    'ExtraTrees': MultiOutputRegressor(ExtraTreesRegressor(n_jobs=1, max_depth=4, n_estimators=50)),  # Reducimos la profundidad máxima y el número de estimadores\n",
    "    'Bagging': MultiOutputRegressor(BaggingRegressor(n_jobs=1, n_estimators=20)),  # Reducimos el número de estimadores\n",
    "    'TheilSen': MultiOutputRegressor(TheilSenRegressor(n_jobs=1, max_subpopulation=1000)),  # Limitamos el tamaño de la subpoblación\n",
    "    'CatBoost': CatBoostRegressor(thread_count=1, loss_function='MultiRMSE', depth=4, iterations=50),  # Reducimos la profundidad y el número de iteraciones\n",
    "    'XGBoost': XGBRegressor(n_jobs=1, objective='reg:squarederror', max_depth=4, n_estimators=50),  # Reducimos la profundidad y el número de estimadores\n",
    "    'LightGBM': MultiOutputRegressor(LGBMRegressor(n_jobs=1, objective='regression_l2')),  # Adaptar para multisalida\n",
    "    'HistGradientBoosting': MultiOutputRegressor(HistGradientBoostingRegressor(max_iter=50, max_depth=4)),  # Reducimos el número de iteraciones y la profundidad\n",
    "    'RandomForest': MultiOutputRegressor(RandomForestRegressor(n_jobs=1, max_depth=5, n_estimators=50)),  # Reducimos la profundidad máxima y el número de estimadores\n",
    "    'GradientBoosting': MultiOutputRegressor(GradientBoostingRegressor(max_depth=5, n_estimators=50)),  # Reducimos la profundidad máxima y el número de estimadores\n",
    "    'AdaBoost': MultiOutputRegressor(AdaBoostRegressor(n_estimators=50)),  # Reducimos el número de estimadores\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=metrics, index=models.keys())\n",
    "\n",
    "abbreviations = {\n",
    "    'DummyRegressor': 'Dummy',\n",
    "    'RandomForest': 'RF',\n",
    "    'GradientBoosting': 'GB',\n",
    "    'AdaBoost': 'AB',\n",
    "    'KNN': 'KNN',\n",
    "    'LinearRegression': 'LR',\n",
    "    'DecisionTree': 'DT',\n",
    "    'SVR': 'SVR',\n",
    "    'GaussianProcess': 'GP',\n",
    "    'ExtraTrees': 'ET',\n",
    "    'Bagging': 'Bag',\n",
    "    'Ridge': 'Ridge',\n",
    "    'Lasso': 'Lasso',\n",
    "    'ElasticNet': 'EN',\n",
    "    'BayesianRidge': 'BR',\n",
    "    'ARDRegression': 'ARD',\n",
    "    'Huber': 'Huber',\n",
    "    'PassiveAggressive': 'PA',\n",
    "    'TheilSen': 'TS',\n",
    "    'CatBoost': 'Cat',\n",
    "    'XGBoost': 'XGB',\n",
    "    'LightGBM': 'LGBM',\n",
    "    'HistGradientBoosting': 'HGB',\n",
    "    'Poisson': 'Poi',\n",
    "    'Tweedie': 'Twee'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "execution_times = pd.DataFrame(columns=[\"Model\", \"Execution_Time\"])\n",
    "\n",
    "# Crear un TimeSeriesSplit con 5 splits\n",
    "tscv = TimeSeriesSplit(n_splits=25)\n",
    "\n",
    "# Para cada modelo en la lista\n",
    "for name, model in models.items():\n",
    "    # print(f\"Training {name} ...\")\n",
    "    \n",
    "    # Inicializar listas para promediar métricas en cada fold\n",
    "    mse_list, rmse_list, mae_list, r2_list, mape_list = [], [], [], [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "   # Obtener param_grid para el modelo actual\n",
    "    # param_grid = param_grids.get(name, {})\n",
    "    param_grid=False\n",
    "    \n",
    "    # Si hay hiperparámetros para ajustar, usar RandomizedSearchCV\n",
    "    if param_grid:\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=20,\n",
    "            cv=5,\n",
    "            scoring=scoring,\n",
    "            refit='RMSE',\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        best_model = search.best_estimator_\n",
    "        print(f\"Best Parameters for {name}:\", search.best_params_)\n",
    "    else:\n",
    "        # Si no hay parámetros, usar el modelo original\n",
    "        best_model = model\n",
    "    \n",
    "    # Inicializar listas para métricas por fold\n",
    "    mse_list, rmse_list, mae_list, r2_list, mape_list = [], [], [], [], []\n",
    "    \n",
    "    # Entrenar y evaluar el modelo con los mejores parámetros en cada fold\n",
    "    split_counter = 1\n",
    "    total_splits = tscv.get_n_splits(X_train)\n",
    "    for train_index, val_index in tscv.split(X_train):\n",
    "        progress_percentage = (split_counter / total_splits) * 100\n",
    "        \n",
    "        # Dividir los datos según los índices\n",
    "        X_train_fold = X_train.iloc[train_index]\n",
    "        X_val_fold = X_train.iloc[val_index]\n",
    "        y_train_fold = y_train.iloc[train_index]\n",
    "        y_val_fold = y_train.iloc[val_index]\n",
    "        \n",
    "        X_train = X_train.loc[:, ~X_train.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        # Entrenar el modelo optimizado en el fold actual\n",
    "        best_model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predecir en el conjunto de validación\n",
    "        predictions = best_model.predict(X_val_fold)\n",
    "        \n",
    "        # Calcular métricas para este fold\n",
    "        mse = mean_squared_error(y_val_fold, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_val_fold, predictions)\n",
    "        r2 = r2_score(y_val_fold, predictions)\n",
    "        mape = mean_absolute_percentage_error(y_val_fold, predictions)\n",
    "        \n",
    "        # Guardar métricas del fold\n",
    "        mse_list.append(mse)\n",
    "        rmse_list.append(rmse)\n",
    "        mae_list.append(mae)\n",
    "        r2_list.append(r2)\n",
    "        mape_list.append(mape)\n",
    "        \n",
    "        split_counter += 1\n",
    "    \n",
    "    # Promediar las métricas sobre todos los folds y guardarlas\n",
    "    metrics_df.loc[name] = [\n",
    "        np.mean(mse_list),\n",
    "        np.mean(rmse_list),\n",
    "        np.mean(mae_list),\n",
    "        np.mean(r2_list),\n",
    "        np.mean(mape_list)\n",
    "    ]\n",
    "    \n",
    "    # Registrar tiempo de fin\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    new_row = pd.DataFrame({\"Model\": [name], \"Execution_Time\": [execution_time]})\n",
    "    execution_times = pd.concat([execution_times, new_row], ignore_index=True)\n",
    "\n",
    "execution_times.set_index(\"Model\", inplace=True)\n",
    "display(execution_times)\n",
    "# Cambiar nombres en el índice del DataFrame\n",
    "display(metrics_df.sort_values(by=['RMSE','MAPE'],ascending=True))\n",
    "metrics_df_sorted = metrics_df.sort_values(by='RMSE', ascending=True)\n",
    "\n",
    "metrics_df.rename(index=abbreviations, inplace=True)\n",
    "\n",
    "# Crear una figura con subplots organizados en una cuadrícula\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 15))  # Ajustar el tamaño según las necesidades\n",
    "axes = axes.flatten()  # Aplanar los ejes para facilitar la iteración\n",
    "fig.suptitle('Forecasting Metrics for All Models (Logarithmic Scale)', fontsize=16)\n",
    "\n",
    "# Iterar sobre las métricas para graficar en cada subplot\n",
    "for i, metric in enumerate(metrics_df.columns):\n",
    "    ax = axes[i]  # Seleccionar el subplot correspondiente\n",
    "    \n",
    "    # Ordenar métricas de forma ascendente\n",
    "    sorted_metric = metrics_df[metric].sort_values(ascending=True)\n",
    "    \n",
    "    # Graficar métricas como barras con escala logarítmica\n",
    "    ax.bar(sorted_metric.index, sorted_metric, color='skyblue')\n",
    "    ax.set_xlabel('Models', fontsize=12)\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_title(f'Comparison of {metric} (Log Scale)', fontsize=14)\n",
    "    ax.tick_params(axis='x', rotation=45)  # Rotar etiquetas del eje X\n",
    "    ax.set_yscale('log')  # Aplicar escala logarítmica\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)  # Agregar líneas de cuadrícula\n",
    "\n",
    "# Eliminar subplots vacíos si hay más subplots que métricas\n",
    "for j in range(len(metrics_df.columns), len(axes)):\n",
    "    fig.delaxes(axes[j])  # Eliminar ejes innecesarios\n",
    "\n",
    "# Ajustar diseño y guardar la figura\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Ajustar espacio para el título general\n",
    "plt.savefig('plots/metrics_comparison_log_scale.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evolution of PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdjustedClose_A</th>\n",
       "      <th>AdjustedClose_AAL</th>\n",
       "      <th>AdjustedClose_AAP</th>\n",
       "      <th>AdjustedClose_AAPL</th>\n",
       "      <th>AdjustedClose_ABBV</th>\n",
       "      <th>AdjustedClose_ABT</th>\n",
       "      <th>AdjustedClose_ACGL</th>\n",
       "      <th>AdjustedClose_ACN</th>\n",
       "      <th>AdjustedClose_ADBE</th>\n",
       "      <th>AdjustedClose_ADI</th>\n",
       "      <th>...</th>\n",
       "      <th>AdjustedClose_WYNN</th>\n",
       "      <th>AdjustedClose_XEL</th>\n",
       "      <th>AdjustedClose_XOM</th>\n",
       "      <th>AdjustedClose_XRAY</th>\n",
       "      <th>AdjustedClose_XYL</th>\n",
       "      <th>AdjustedClose_YUM</th>\n",
       "      <th>AdjustedClose_ZBH</th>\n",
       "      <th>AdjustedClose_ZBRA</th>\n",
       "      <th>AdjustedClose_ZION</th>\n",
       "      <th>AdjustedClose_ZTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3616</th>\n",
       "      <td>138.048569</td>\n",
       "      <td>13.44</td>\n",
       "      <td>60.839672</td>\n",
       "      <td>184.734985</td>\n",
       "      <td>154.183792</td>\n",
       "      <td>107.700714</td>\n",
       "      <td>71.840385</td>\n",
       "      <td>341.431335</td>\n",
       "      <td>580.070007</td>\n",
       "      <td>191.156937</td>\n",
       "      <td>...</td>\n",
       "      <td>93.573257</td>\n",
       "      <td>61.628197</td>\n",
       "      <td>98.983528</td>\n",
       "      <td>35.017651</td>\n",
       "      <td>112.744011</td>\n",
       "      <td>127.125343</td>\n",
       "      <td>120.625359</td>\n",
       "      <td>267.980011</td>\n",
       "      <td>42.580433</td>\n",
       "      <td>194.642258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3617</th>\n",
       "      <td>130.496964</td>\n",
       "      <td>12.95</td>\n",
       "      <td>59.906696</td>\n",
       "      <td>183.351761</td>\n",
       "      <td>154.801224</td>\n",
       "      <td>107.377167</td>\n",
       "      <td>73.162132</td>\n",
       "      <td>332.573761</td>\n",
       "      <td>571.789978</td>\n",
       "      <td>186.594772</td>\n",
       "      <td>...</td>\n",
       "      <td>93.009453</td>\n",
       "      <td>61.735004</td>\n",
       "      <td>99.815155</td>\n",
       "      <td>34.221573</td>\n",
       "      <td>110.651207</td>\n",
       "      <td>127.194328</td>\n",
       "      <td>119.711151</td>\n",
       "      <td>252.520004</td>\n",
       "      <td>40.516045</td>\n",
       "      <td>191.037918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>130.337769</td>\n",
       "      <td>13.09</td>\n",
       "      <td>59.405838</td>\n",
       "      <td>181.023163</td>\n",
       "      <td>155.765945</td>\n",
       "      <td>108.808601</td>\n",
       "      <td>73.666107</td>\n",
       "      <td>331.756897</td>\n",
       "      <td>567.049988</td>\n",
       "      <td>183.740936</td>\n",
       "      <td>...</td>\n",
       "      <td>93.187492</td>\n",
       "      <td>61.880650</td>\n",
       "      <td>98.944847</td>\n",
       "      <td>35.066792</td>\n",
       "      <td>111.414932</td>\n",
       "      <td>126.790359</td>\n",
       "      <td>119.442848</td>\n",
       "      <td>252.970001</td>\n",
       "      <td>41.133430</td>\n",
       "      <td>192.137054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>129.899979</td>\n",
       "      <td>13.60</td>\n",
       "      <td>60.417377</td>\n",
       "      <td>180.296707</td>\n",
       "      <td>156.421982</td>\n",
       "      <td>108.632126</td>\n",
       "      <td>73.038513</td>\n",
       "      <td>331.294281</td>\n",
       "      <td>564.599976</td>\n",
       "      <td>184.214935</td>\n",
       "      <td>...</td>\n",
       "      <td>94.611870</td>\n",
       "      <td>61.880650</td>\n",
       "      <td>99.244621</td>\n",
       "      <td>34.771950</td>\n",
       "      <td>111.236412</td>\n",
       "      <td>126.445534</td>\n",
       "      <td>119.224243</td>\n",
       "      <td>252.690002</td>\n",
       "      <td>42.493614</td>\n",
       "      <td>192.939102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AdjustedClose_A  AdjustedClose_AAL  AdjustedClose_AAP  \\\n",
       "3616       138.048569              13.44          60.839672   \n",
       "3617       130.496964              12.95          59.906696   \n",
       "3618       130.337769              13.09          59.405838   \n",
       "3619       129.899979              13.60          60.417377   \n",
       "\n",
       "      AdjustedClose_AAPL  AdjustedClose_ABBV  AdjustedClose_ABT  \\\n",
       "3616          184.734985          154.183792         107.700714   \n",
       "3617          183.351761          154.801224         107.377167   \n",
       "3618          181.023163          155.765945         108.808601   \n",
       "3619          180.296707          156.421982         108.632126   \n",
       "\n",
       "      AdjustedClose_ACGL  AdjustedClose_ACN  AdjustedClose_ADBE  \\\n",
       "3616           71.840385         341.431335          580.070007   \n",
       "3617           73.162132         332.573761          571.789978   \n",
       "3618           73.666107         331.756897          567.049988   \n",
       "3619           73.038513         331.294281          564.599976   \n",
       "\n",
       "      AdjustedClose_ADI  ...  AdjustedClose_WYNN  AdjustedClose_XEL  \\\n",
       "3616         191.156937  ...           93.573257          61.628197   \n",
       "3617         186.594772  ...           93.009453          61.735004   \n",
       "3618         183.740936  ...           93.187492          61.880650   \n",
       "3619         184.214935  ...           94.611870          61.880650   \n",
       "\n",
       "      AdjustedClose_XOM  AdjustedClose_XRAY  AdjustedClose_XYL  \\\n",
       "3616          98.983528           35.017651         112.744011   \n",
       "3617          99.815155           34.221573         110.651207   \n",
       "3618          98.944847           35.066792         111.414932   \n",
       "3619          99.244621           34.771950         111.236412   \n",
       "\n",
       "      AdjustedClose_YUM  AdjustedClose_ZBH  AdjustedClose_ZBRA  \\\n",
       "3616         127.125343         120.625359          267.980011   \n",
       "3617         127.194328         119.711151          252.520004   \n",
       "3618         126.790359         119.442848          252.970001   \n",
       "3619         126.445534         119.224243          252.690002   \n",
       "\n",
       "      AdjustedClose_ZION  AdjustedClose_ZTS  \n",
       "3616           42.580433         194.642258  \n",
       "3617           40.516045         191.037918  \n",
       "3618           41.133430         192.137054  \n",
       "3619           42.493614         192.939102  \n",
       "\n",
       "[4 rows x 500 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Evaluando PCA con 10 componentes...\n",
      "Entrenando modelo: Bagging con 10 componentes.\n",
      "Entrenando modelo: XGBoost con 10 componentes.\n",
      "Entrenando modelo: LightGBM con 10 componentes.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 64,273169\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 24,464557\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 115,136796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 57,670481\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000589 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 60,939586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 55,750927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 27,705658\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 138,736347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 205,964436\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 78,713267\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 39,304132\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 104,428593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 116,701513\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 46,013905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 51,291335\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 12,338559\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 34,430038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 40,612303\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 33,040440\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 80,098369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 73,229852\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 67,461280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 94,569666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 181,592016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 45,739347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 67,752858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 72,622068\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 46,844218\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 7,452104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,527779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 69,061097\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 134,296905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 129,018475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 121,239396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 63,349146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 50,784006\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 158,447469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 137,444402\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 36,477111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 47,606857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 142,573312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 18,626951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 67,982222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 91,064480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 62,556705\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 130,662405\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 21,540635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 85,602709\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 75,244354\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 87,510330\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 930,674085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 164,938291\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 19,588425\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 41,170736\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 45,932353\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 31,058915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 45,044980\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 155,822127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 25,901865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 40,469115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 59,415365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 242,420462\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 263,523244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,919522\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 1458,650267\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 26,460360\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 367,760696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 41,956363\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 75,765668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 290174,628003\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 193,495928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 27,921393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 25,069369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,907625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 78,124926\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 42,362030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 23,226066\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 48,646566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 19,263583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 110,747086\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 111,314815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 71,181615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43,326206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 80,083369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 33,183335\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 62,306457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 75,013890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 73,550714\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 46,507342\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 38,610216\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 23,522323\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 48,977445\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 65,152282\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 282,961297\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 139,099921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 58,785390\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 55,175053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 101,724580\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 41,786935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 27,312301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 101,921850\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 14,925946\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 128,893837\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 34,852748\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 39,984801\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 18,732087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 72,790192\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 52,965868\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 49,959307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 80,163840\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 207,467813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 158,198037\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 35,474510\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 13,939297\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 68,939666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 121,933706\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 110,057904\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 28,648089\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 36,507881\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 16,012349\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 40,213574\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000830 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43,694452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 18,088054\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 52,245684\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,212850\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 57,268764\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 84,003560\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 28,336523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 48,889753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,750248\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 45,960254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 56,050893\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 151,604479\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 55,734691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 100,428471\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 79,141622\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 41,139913\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 95,675632\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 90,697100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 75,590093\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000232 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 80,720565\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 21,270384\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 73,265947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 40,048154\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 183,839347\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 69,193960\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 65,382147\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 59,263473\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 69,533012\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 35,633645\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 30,762858\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 39,667672\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 75,536890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 29,868069\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 120,350598\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 55,044004\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 111,489626\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 174,183111\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 44,165408\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 122,259935\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 198,823498\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 61,169389\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 54,694968\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 50,298670\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 66,222614\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 146,163869\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 356,007680\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 47,619585\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 28,918124\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 47,225479\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 172,374794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 76,208466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 63,762829\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 57,385299\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 36,487009\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 44,735082\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 22,881483\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 61,269523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 94,116715\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 69,558545\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 8,556699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 61,985189\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 26,054299\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 23,913402\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 200,385033\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 152,904945\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 27,240810\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 130,920254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 67,459401\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 113,352123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 18,769866\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 59,832620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 33,997398\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 33,947352\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 85,578100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 75,346264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 18,882120\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 51,074986\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 127,397294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 78,623395\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 60,790864\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 12,516755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 50,682244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 42,838683\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 64,422616\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 20,848652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 30,752112\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 87,281029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 53,889560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 53,989382\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 78,691136\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 83,026452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 56,526675\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 187,203133\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 273,563971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 31,931177\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 55,548615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 8,168713\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 101,802797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 143,143538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 64,592333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 39,152018\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 129,682666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 73,969593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 41,490426\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 108,941628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 10,201222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 15,940215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 27,575635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 54,714308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 13,175280\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 102,550337\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 229,710742\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 23,390218\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 105,982900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 60,185947\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 202,870693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 107,834554\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 86,946373\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 187,141232\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 64,607472\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 30,392330\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 192,987652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 21,555397\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 29,571560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 16,710715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 108,515338\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 30,460976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 24,080988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 132,007583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 131,486020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 112,791938\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 17,637146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 57,219693\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 95,687315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 33,794505\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 93,480410\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 97,876709\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 21,912322\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 73,199190\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 46,093115\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 16,438729\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 10,753201\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 68,939723\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43,920423\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 14,096365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 133,256453\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 87,612134\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000578 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 15,721990\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 63,865183\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 36,450012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 24,769727\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 44,516217\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 51,212983\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 50,536895\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 125,719618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 108,412829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 157,500368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 29,009594\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 120,689878\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 217,611502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 35,530647\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 30,944535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 87,537730\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 16,382429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 12,785285\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,435266\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43,560569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 47,300699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 54,256971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 41,946870\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 166,063227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 82,288557\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 88,802275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 29,948731\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 132,331013\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 35,349907\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 165,742556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 148,102350\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 37,578255\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 63,843866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 36,584154\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 136,941334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 24,394658\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 136,321247\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 48,533729\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 179,086145\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 190,675668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 75,010080\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 1666,576030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 25,386906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 28,246333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 116,207027\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 34,985245\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 44,537847\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 151,143900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 51,035445\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 55,438796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 18,639104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 38,739180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 182,149048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 109,450642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 107,823127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 104,351206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 41,763368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 597,522842\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000474 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 34,369095\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 34,177085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 25,141780\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 114,598988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 35,447551\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 35,497197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 202,637161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 15,914222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 63,753815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 213,579852\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 203,597168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 23,858207\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 122,415246\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43,165149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 63,336897\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 57,179049\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 7,276545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 2497,717971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 18,823695\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 14,823864\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 14,798063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 88,881701\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 37,823600\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 55,030046\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 27,174944\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,304650\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 53,606860\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 23,705542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 45,913786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 320,540965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 50,307359\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 52,566711\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 34,226085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 132,628757\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 54,429820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 35,620081\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,620224\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 36,680865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 92,638450\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 24,312064\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 39,673829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 81,468552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000859 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 50,755618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 152,651364\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 26,687165\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 71,000772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 57,065596\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 59,719361\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 88,459196\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 37,664288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 51,104075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 153,333674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 88,449857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 21,185020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 61,188913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 159,792275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 58,134763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 63,514118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 51,943408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 77,259296\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 67,724795\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 84,979891\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 66,216777\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43,402760\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 380,445351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 10,367124\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 46,745199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 46,974022\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 102,399973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 97,602807\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 140,500087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 17,722666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 238,345140\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000288 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 62,005247\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 61,282311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 59,145432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 150,244696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 51,834321\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 35,500053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 88,414274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 34,050781\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 122,890956\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 88,749976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 49,867103\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 124,911185\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 126,134195\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 38,263768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 96,792307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 169,895419\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43,104972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 97,263327\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,745910\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 55,185356\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 36,065972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 129,361250\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 93,541580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 70,031920\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 1666,576024\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 25,477708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 133,370384\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 44,402406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 13,697914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 50,331662\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 269,285168\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 195,216485\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 41,362433\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 69,253534\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 45,467382\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 30,175569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 185,541951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 82,442215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 38,451615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 233,795660\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 63,057352\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,563928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 38,951092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 37,695184\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 75,719514\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 97,931061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 90,183408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 68,175113\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 44,042423\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 73,999726\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 74,245661\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 77,064951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43,849427\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 194,908310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 47,789524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 27,435380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 98,354204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000474 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 214,523610\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 195,123577\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 111,350071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 93,111735\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 141,296571\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,589302\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 108,384699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 42,639394\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 17,388354\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 51,511754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 101,023656\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 103,684010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 114,434399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 146,118858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 15,704959\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 42,771016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 24,855924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 32,287471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 67,768270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 174,784256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 38,927970\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 26,612904\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 51,228786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 50,937023\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 50,523945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 35,057047\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 107,978511\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 73,625574\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 19,637486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 27,776182\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 19,354545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 130,106295\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 135,634553\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 22,029626\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 102,141455\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 36,933363\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 58,258436\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43,123386\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 54,639642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 67,807169\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 97,213288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 159,536612\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 29,890582\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 81,599657\n",
      "Evaluando PCA con 20 componentes...\n",
      "Entrenando modelo: Bagging con 20 componentes.\n",
      "Entrenando modelo: XGBoost con 20 componentes.\n",
      "Entrenando modelo: LightGBM con 20 componentes.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 64,273169\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 24,464557\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 115,136796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 57,670481\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 60,939586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 55,750927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 27,705658\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 138,736347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 205,964436\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 78,713267\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 39,304132\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 104,428593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 116,701513\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 46,013905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 51,291335\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 12,338559\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 34,430038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 40,612303\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 33,040440\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 80,098369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 73,229852\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 67,461280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 94,569666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 181,592016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 45,739347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 67,752858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 72,622068\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 46,844218\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 7,452104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,527779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 69,061097\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 134,296905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 129,018475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 121,239396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 63,349146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 50,784006\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 158,447469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 137,444402\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 36,477111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 47,606857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 142,573312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 18,626951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 67,982222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 91,064480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 62,556705\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 130,662405\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 21,540635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 85,602709\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000861 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 75,244354\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 87,510330\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 930,674085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000848 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 164,938291\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 19,588425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 41,170736\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 45,932353\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 31,058915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 45,044980\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000861 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 155,822127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 25,901865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 40,469115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 59,415365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 242,420462\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 263,523244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,919522\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 1458,650267\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 26,460360\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 367,760696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 41,956363\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 75,765668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 290174,628003\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 193,495928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 27,921393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 25,069369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,907625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 78,124926\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 42,362030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 23,226066\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 48,646566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 19,263583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 110,747086\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 111,314815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 71,181615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 43,326206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 80,083369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 33,183335\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 62,306457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 75,013890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 73,550714\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 46,507342\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 38,610216\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 23,522323\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 48,977445\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 65,152282\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 282,961297\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 139,099921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 58,785390\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 55,175053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 101,724580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000949 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 41,786935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 27,312301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 101,921850\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 14,925946\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 128,893837\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 34,852748\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 39,984801\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 18,732087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 72,790192\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 52,965868\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 49,959307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 80,163840\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 207,467813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 158,198037\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 35,474510\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 13,939297\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 68,939666\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 121,933706\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 110,057904\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 28,648089\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 36,507881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 16,012349\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 40,213574\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 43,694452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 18,088054\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 52,245684\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,212850\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 57,268764\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 84,003560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 28,336523\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000952 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 48,889753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,750248\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 45,960254\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 56,050893\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 151,604479\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 55,734691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 100,428471\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 79,141622\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 41,139913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 95,675632\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 90,697100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 75,590093\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 80,720565\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 21,270384\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 73,265947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 40,048154\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 183,839347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 69,193960\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 65,382147\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 59,263473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 69,533012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 35,633645\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 30,762858\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 39,667672\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 75,536890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 29,868069\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 120,350598\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 55,044004\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 111,489626\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 174,183111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 44,165408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 122,259935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 198,823498\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 61,169389\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 54,694968\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000510 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 50,298670\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 66,222614\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 146,163869\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 356,007680\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 47,619585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 28,918124\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 47,225479\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 172,374794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 76,208466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 63,762829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 57,385299\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 36,487009\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 44,735082\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 22,881483\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 61,269523\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 94,116715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 69,558545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 8,556699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 61,985189\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 26,054299\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 23,913402\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 200,385033\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 152,904945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 27,240810\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 130,920254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 67,459401\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 113,352123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 18,769866\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 59,832620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 33,997398\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 33,947352\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 85,578100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 75,346264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 18,882120\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 51,074986\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 127,397294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 78,623395\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 60,790864\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 12,516755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 50,682244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 42,838683\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 64,422616\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 20,848652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 30,752112\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 87,281029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 53,889560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 53,989382\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 78,691136\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 83,026452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 56,526675\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 187,203133\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 273,563971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 31,931177\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 55,548615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 8,168713\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 101,802797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 143,143538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 64,592333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 39,152018\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 129,682666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 73,969593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 41,490426\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 108,941628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 10,201222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 15,940215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 27,575635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 54,714308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 13,175280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 102,550337\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 229,710742\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 23,390218\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 105,982900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 60,185947\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 202,870693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 107,834554\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 86,946373\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 187,141232\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 64,607472\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 30,392330\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 192,987652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 21,555397\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 29,571560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 16,710715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 108,515338\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 30,460976\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 24,080988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 132,007583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 131,486020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 112,791938\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 17,637146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 57,219693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 95,687315\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 33,794505\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 93,480410\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 97,876709\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 21,912322\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 73,199190\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 46,093115\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 16,438729\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 10,753201\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 68,939723\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 43,920423\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 14,096365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 133,256453\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 87,612134\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 15,721990\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 63,865183\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 36,450012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 24,769727\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 44,516217\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 51,212983\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 50,536895\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 125,719618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 108,412829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 157,500368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 29,009594\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 120,689878\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 217,611502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 35,530647\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 30,944535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 87,537730\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 16,382429\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 12,785285\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,435266\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 43,560569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 47,300699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 54,256971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 41,946870\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 166,063227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 82,288557\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 88,802275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 29,948731\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 132,331013\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 35,349907\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 165,742556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 148,102350\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 37,578255\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 63,843866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 36,584154\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 136,941334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 24,394658\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 136,321247\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 48,533729\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 179,086145\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 190,675668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 75,010080\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 1666,576030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 25,386906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 28,246333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 116,207027\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 34,985245\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 44,537847\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 151,143900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 51,035445\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 55,438796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 18,639104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 38,739180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 182,149048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 109,450642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 107,823127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 104,351206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 41,763368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 597,522842\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 34,369095\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 34,177085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 25,141780\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 114,598988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 35,447551\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 35,497197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 202,637161\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 15,914222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 63,753815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 213,579852\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 203,597168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 23,858207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 122,415246\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 43,165149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 63,336897\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 57,179049\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 7,276545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 2497,717971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 18,823695\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 14,823864\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 14,798063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 88,881701\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 37,823600\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 55,030046\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 27,174944\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,304650\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 53,606860\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 23,705542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001752 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 45,913786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 320,540965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 50,307359\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 52,566711\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 34,226085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 132,628757\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 54,429820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 35,620081\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,620224\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 36,680865\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 92,638450\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 24,312064\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 39,673829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 81,468552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 50,755618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 152,651364\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 26,687165\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 71,000772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 57,065596\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 59,719361\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 88,459196\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 37,664288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 51,104075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 153,333674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 88,449857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 21,185020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 61,188913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 159,792275\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 58,134763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 63,514118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 51,943408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 77,259296\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 67,724795\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 84,979891\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 66,216777\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 43,402760\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 380,445351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 10,367124\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 46,745199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 46,974022\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 102,399973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 97,602807\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 140,500087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 17,722666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 238,345140\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 62,005247\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 61,282311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 59,145432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 150,244696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 51,834321\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 35,500053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 88,414274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 34,050781\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 122,890956\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 88,749976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 49,867103\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 124,911185\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 126,134195\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 38,263768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 96,792307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 169,895419\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 43,104972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 97,263327\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,745910\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 55,185356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 36,065972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 129,361250\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 93,541580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 70,031920\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 1666,576024\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 25,477708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 133,370384\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 44,402406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 13,697914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 50,331662\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 269,285168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 195,216485\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 41,362433\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 69,253534\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 45,467382\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 30,175569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 185,541951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 82,442215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 38,451615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 233,795660\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 63,057352\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,563928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 38,951092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 37,695184\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 75,719514\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 97,931061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 90,183408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 68,175113\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 44,042423\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 73,999726\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 74,245661\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 77,064951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 43,849427\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 194,908310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 47,789524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 27,435380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 98,354204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 214,523610\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 195,123577\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 111,350071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 93,111735\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 141,296571\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,589302\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 108,384699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 42,639394\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 17,388354\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 51,511754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 101,023656\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 103,684010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 114,434399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 146,118858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 15,704959\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 42,771016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 24,855924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 32,287471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 67,768270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 174,784256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 38,927970\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 26,612904\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 51,228786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 50,937023\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 50,523945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 35,057047\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 107,978511\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 73,625574\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 19,637486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 27,776182\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 19,354545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 130,106295\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 135,634553\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 22,029626\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 102,141455\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 36,933363\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 58,258436\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 43,123386\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 54,639642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 67,807169\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 97,213288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 159,536612\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 29,890582\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 81,599657\n",
      "Evaluando PCA con 30 componentes...\n",
      "Entrenando modelo: Bagging con 30 componentes.\n",
      "Entrenando modelo: XGBoost con 30 componentes.\n",
      "Entrenando modelo: LightGBM con 30 componentes.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 64,273169\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 24,464557\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 115,136796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 57,670481\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 60,939586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 55,750927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 27,705658\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 138,736347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 205,964436\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 78,713267\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 39,304132\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 104,428593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 116,701513\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 46,013905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 51,291335\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 12,338559\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 34,430038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 40,612303\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 33,040440\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 80,098369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 73,229852\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 67,461280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 94,569666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 181,592016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 45,739347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 67,752858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 72,622068\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 46,844218\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 7,452104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,527779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 69,061097\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 134,296905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 129,018475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 121,239396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 63,349146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 50,784006\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 158,447469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 137,444402\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 36,477111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 47,606857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 142,573312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 18,626951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 67,982222\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 91,064480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 62,556705\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 130,662405\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 21,540635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 85,602709\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 75,244354\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 87,510330\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 930,674085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 164,938291\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 19,588425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 41,170736\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 45,932353\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 31,058915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 45,044980\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 155,822127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 25,901865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 40,469115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 59,415365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 242,420462\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 263,523244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,919522\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 1458,650267\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 26,460360\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 367,760696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 41,956363\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 75,765668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 290174,628003\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 193,495928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 27,921393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 25,069369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,907625\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 78,124926\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 42,362030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 23,226066\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 48,646566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 19,263583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 110,747086\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 111,314815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 71,181615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 43,326206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 80,083369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 33,183335\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 62,306457\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 75,013890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 73,550714\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 46,507342\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 38,610216\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 23,522323\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 48,977445\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 65,152282\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 282,961297\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 139,099921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 58,785390\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 55,175053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 101,724580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 41,786935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 27,312301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 101,921850\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 14,925946\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 128,893837\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 34,852748\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 39,984801\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 18,732087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 72,790192\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001577 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 52,965868\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 49,959307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 80,163840\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 207,467813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 158,198037\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 35,474510\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 13,939297\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 68,939666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 121,933706\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 110,057904\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 28,648089\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 36,507881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 16,012349\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 40,213574\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 43,694452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 18,088054\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 52,245684\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,212850\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 57,268764\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 84,003560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 28,336523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 48,889753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,750248\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 45,960254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 56,050893\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 151,604479\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 55,734691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 100,428471\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 79,141622\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 41,139913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 95,675632\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 90,697100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 75,590093\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001949 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 80,720565\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 21,270384\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 73,265947\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 40,048154\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 183,839347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 69,193960\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 65,382147\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 59,263473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 69,533012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 35,633645\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 30,762858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 39,667672\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 75,536890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 29,868069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 120,350598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 55,044004\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 111,489626\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 174,183111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 44,165408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 122,259935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 198,823498\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 61,169389\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 54,694968\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 50,298670\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 66,222614\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 146,163869\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 356,007680\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 47,619585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 28,918124\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001783 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 47,225479\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001520 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 172,374794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 76,208466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 63,762829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 57,385299\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 36,487009\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 44,735082\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 22,881483\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 61,269523\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 94,116715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 69,558545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 8,556699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 61,985189\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 26,054299\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 23,913402\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 200,385033\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 152,904945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 27,240810\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 130,920254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001848 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 67,459401\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 113,352123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 18,769866\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 59,832620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 33,997398\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 33,947352\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 85,578100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 75,346264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 18,882120\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 51,074986\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 127,397294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 78,623395\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 60,790864\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 12,516755\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 50,682244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 42,838683\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 64,422616\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001609 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 20,848652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 30,752112\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 87,281029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 53,889560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 53,989382\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 78,691136\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 83,026452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 56,526675\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 187,203133\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 273,563971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 31,931177\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 55,548615\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 8,168713\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 101,802797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 143,143538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 64,592333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 39,152018\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 129,682666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 73,969593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 41,490426\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 108,941628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 10,201222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 15,940215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 27,575635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 54,714308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 13,175280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 102,550337\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 229,710742\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 23,390218\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 105,982900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 60,185947\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 202,870693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 107,834554\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 86,946373\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 187,141232\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 64,607472\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 30,392330\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 192,987652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 21,555397\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 29,571560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 16,710715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 108,515338\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 30,460976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002104 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 24,080988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 132,007583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 131,486020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 112,791938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 17,637146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 57,219693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 95,687315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 33,794505\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001601 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 93,480410\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 97,876709\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 21,912322\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 73,199190\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 46,093115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 16,438729\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 10,753201\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 68,939723\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 43,920423\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 14,096365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 133,256453\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 87,612134\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 15,721990\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 63,865183\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 36,450012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 24,769727\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 44,516217\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 51,212983\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 50,536895\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 125,719618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001474 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 108,412829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 157,500368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 29,009594\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 120,689878\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 217,611502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 35,530647\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 30,944535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 87,537730\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 16,382429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 12,785285\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,435266\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 43,560569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 47,300699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 54,256971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 41,946870\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 166,063227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 82,288557\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 88,802275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 29,948731\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 132,331013\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 35,349907\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 165,742556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 148,102350\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 37,578255\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 63,843866\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 36,584154\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 136,941334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001504 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 24,394658\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001530 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 136,321247\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 48,533729\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 179,086145\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 190,675668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 75,010080\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 1666,576030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 25,386906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 28,246333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 116,207027\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 34,985245\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001497 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 44,537847\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 151,143900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 51,035445\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 55,438796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 18,639104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 38,739180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 182,149048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 109,450642\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 107,823127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 104,351206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 41,763368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 597,522842\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 34,369095\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 34,177085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 25,141780\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,001087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 114,598988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 35,447551\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 35,497197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 202,637161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 15,914222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 63,753815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 213,579852\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 203,597168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 23,858207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 122,415246\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 43,165149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 63,336897\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 57,179049\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 7,276545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 2497,717971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 18,823695\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 14,823864\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 14,798063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 88,881701\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 37,823600\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 55,030046\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 27,174944\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,304650\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 53,606860\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 23,705542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 45,913786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 320,540965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 50,307359\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 52,566711\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 34,226085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 132,628757\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 54,429820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 35,620081\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,620224\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 36,680865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 92,638450\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 24,312064\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 39,673829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 81,468552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 50,755618\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 152,651364\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 26,687165\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 71,000772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 57,065596\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 59,719361\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 88,459196\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 37,664288\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 51,104075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 153,333674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 88,449857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 21,185020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 61,188913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 159,792275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 58,134763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 63,514118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 51,943408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 77,259296\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 67,724795\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 84,979891\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 66,216777\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 43,402760\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 380,445351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 10,367124\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 46,745199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 46,974022\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 102,399973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 97,602807\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 140,500087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 17,722666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 238,345140\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 62,005247\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 61,282311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 59,145432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 150,244696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 51,834321\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 35,500053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 88,414274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 34,050781\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 122,890956\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 88,749976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 49,867103\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 124,911185\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 126,134195\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 38,263768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 96,792307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 169,895419\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 43,104972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 97,263327\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,745910\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 55,185356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 36,065972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 129,361250\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 93,541580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 70,031920\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 1666,576024\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 25,477708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 133,370384\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 44,402406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 13,697914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 50,331662\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 269,285168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 195,216485\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001577 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 41,362433\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 69,253534\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 45,467382\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 30,175569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 185,541951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001497 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 82,442215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 38,451615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 233,795660\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001667 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 63,057352\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,563928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 38,951092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 37,695184\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 75,719514\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 97,931061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 90,183408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 68,175113\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 44,042423\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,001057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 73,999726\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 74,245661\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 77,064951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 43,849427\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 194,908310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 47,789524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 27,435380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 98,354204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 214,523610\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 195,123577\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 111,350071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 93,111735\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001943 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 141,296571\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,589302\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 108,384699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 42,639394\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 17,388354\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 51,511754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 101,023656\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 103,684010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 114,434399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 146,118858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 15,704959\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 42,771016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 24,855924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 32,287471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 67,768270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 174,784256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 38,927970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 26,612904\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 51,228786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 50,937023\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 50,523945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 35,057047\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 107,978511\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 73,625574\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 19,637486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 27,776182\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 19,354545\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 130,106295\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 135,634553\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 22,029626\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 102,141455\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 36,933363\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 58,258436\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 43,123386\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 54,639642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 67,807169\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 97,213288\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 159,536612\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 29,890582\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 81,599657\n",
      "Evaluando PCA con 40 componentes...\n",
      "Entrenando modelo: Bagging con 40 componentes.\n",
      "Entrenando modelo: XGBoost con 40 componentes.\n",
      "Entrenando modelo: LightGBM con 40 componentes.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 64,273169\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 24,464557\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 115,136796\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 57,670481\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 60,939586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 55,750927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 27,705658\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 138,736347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 205,964436\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 78,713267\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 39,304132\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 104,428593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 116,701513\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 46,013905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 51,291335\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 12,338559\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 34,430038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 40,612303\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 33,040440\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 80,098369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 73,229852\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 67,461280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 94,569666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 181,592016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 45,739347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 67,752858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 72,622068\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 46,844218\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001859 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 7,452104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,527779\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 69,061097\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 134,296905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 129,018475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 121,239396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 63,349146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 50,784006\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 158,447469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 137,444402\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 36,477111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 47,606857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 142,573312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 18,626951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 67,982222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 91,064480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 62,556705\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 130,662405\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 21,540635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 85,602709\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 75,244354\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 87,510330\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 930,674085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 164,938291\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 19,588425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 41,170736\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 45,932353\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 31,058915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 45,044980\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 155,822127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 25,901865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 40,469115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001577 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 59,415365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 242,420462\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 263,523244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,919522\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 1458,650267\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001474 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 26,460360\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 367,760696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 41,956363\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 75,765668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 290174,628003\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 193,495928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 27,921393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 25,069369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,907625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 78,124926\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,001047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 42,362030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 23,226066\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 48,646566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 19,263583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 110,747086\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 111,314815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 71,181615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001943 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 43,326206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 80,083369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 33,183335\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 62,306457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 75,013890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 73,550714\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 46,507342\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 38,610216\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 23,522323\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 48,977445\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 65,152282\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 282,961297\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 139,099921\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 58,785390\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 55,175053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 101,724580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 41,786935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 27,312301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 101,921850\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 14,925946\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 128,893837\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,001046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 34,852748\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 39,984801\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 18,732087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 72,790192\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 52,965868\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 49,959307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001952 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 80,163840\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 207,467813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001949 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 158,198037\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 35,474510\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 13,939297\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 68,939666\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 121,933706\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 110,057904\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 28,648089\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 36,507881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 16,012349\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 40,213574\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 43,694452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 18,088054\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 52,245684\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,212850\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 57,268764\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 84,003560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 28,336523\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 48,889753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,750248\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 45,960254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 56,050893\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 151,604479\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 55,734691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 100,428471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 79,141622\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 41,139913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 95,675632\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 90,697100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 75,590093\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 80,720565\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 21,270384\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 73,265947\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 40,048154\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 183,839347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 69,193960\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 65,382147\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 59,263473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 69,533012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 35,633645\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 30,762858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 39,667672\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 75,536890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 29,868069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 120,350598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 55,044004\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 111,489626\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 174,183111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 44,165408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 122,259935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 198,823498\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 61,169389\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 54,694968\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 50,298670\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 66,222614\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 146,163869\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 356,007680\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 47,619585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 28,918124\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 47,225479\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 172,374794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 76,208466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 63,762829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 57,385299\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 36,487009\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 44,735082\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 22,881483\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 61,269523\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 94,116715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 69,558545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 8,556699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 61,985189\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 26,054299\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 23,913402\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 200,385033\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 152,904945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 27,240810\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 130,920254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 67,459401\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 113,352123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 18,769866\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 59,832620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 33,997398\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 33,947352\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 85,578100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 75,346264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 18,882120\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 51,074986\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 127,397294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 78,623395\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 60,790864\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 12,516755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 50,682244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 42,838683\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 64,422616\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 20,848652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 30,752112\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 87,281029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 53,889560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 53,989382\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 78,691136\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 83,026452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 56,526675\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 187,203133\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 273,563971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 31,931177\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 55,548615\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 8,168713\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 101,802797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 143,143538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 64,592333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 39,152018\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 129,682666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 73,969593\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,001174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 41,490426\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 108,941628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 10,201222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001520 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 15,940215\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 27,575635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 54,714308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 13,175280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 102,550337\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 229,710742\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 23,390218\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 105,982900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 60,185947\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 202,870693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 107,834554\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 86,946373\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 187,141232\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 64,607472\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 30,392330\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 192,987652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 21,555397\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 29,571560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 16,710715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 108,515338\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 30,460976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 24,080988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 132,007583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 131,486020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 112,791938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 17,637146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 57,219693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 95,687315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 33,794505\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 93,480410\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 97,876709\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 21,912322\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 73,199190\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 46,093115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 16,438729\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 10,753201\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 68,939723\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 43,920423\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 14,096365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 133,256453\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 87,612134\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 15,721990\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 63,865183\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 36,450012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 24,769727\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 44,516217\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 51,212983\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 50,536895\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 125,719618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 108,412829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 157,500368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 29,009594\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 120,689878\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 217,611502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001846 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 35,530647\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 30,944535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 87,537730\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 16,382429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 12,785285\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,435266\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 43,560569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 47,300699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 54,256971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 41,946870\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 166,063227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 82,288557\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 88,802275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 29,948731\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 132,331013\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 35,349907\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 165,742556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 148,102350\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 37,578255\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 63,843866\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 36,584154\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 136,941334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 24,394658\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 136,321247\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 48,533729\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 179,086145\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 190,675668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 75,010080\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 1666,576030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 25,386906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 28,246333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 116,207027\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 34,985245\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 44,537847\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 151,143900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 51,035445\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 55,438796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 18,639104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 38,739180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 182,149048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 109,450642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 107,823127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 104,351206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 41,763368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 597,522842\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 34,369095\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 34,177085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 25,141780\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 114,598988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 35,447551\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 35,497197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 202,637161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 15,914222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 63,753815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 213,579852\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 203,597168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 23,858207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 122,415246\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 43,165149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 63,336897\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 57,179049\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 7,276545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 2497,717971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 18,823695\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 14,823864\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 14,798063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 88,881701\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 37,823600\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 55,030046\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 27,174944\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,304650\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 53,606860\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 23,705542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 45,913786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 320,540965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 50,307359\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 52,566711\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 34,226085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 132,628757\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 54,429820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 35,620081\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,620224\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 36,680865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 92,638450\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 24,312064\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 39,673829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001846 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 81,468552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 50,755618\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 152,651364\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 26,687165\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 71,000772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 57,065596\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 59,719361\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 88,459196\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 37,664288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 51,104075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 153,333674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 88,449857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 21,185020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 61,188913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 159,792275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 58,134763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 63,514118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 51,943408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 77,259296\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 67,724795\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 84,979891\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 66,216777\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 43,402760\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 380,445351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 10,367124\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 46,745199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 46,974022\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 102,399973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 97,602807\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 140,500087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 17,722666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 238,345140\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 62,005247\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 61,282311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 59,145432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 150,244696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 51,834321\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 35,500053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 88,414274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 34,050781\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 122,890956\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 88,749976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 49,867103\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001104 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 124,911185\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 126,134195\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 38,263768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 96,792307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 169,895419\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 43,104972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 97,263327\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,745910\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 55,185356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 36,065972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 129,361250\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 93,541580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 70,031920\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 1666,576024\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 25,477708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 133,370384\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 44,402406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 13,697914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 50,331662\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 269,285168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 195,216485\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 41,362433\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 69,253534\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 45,467382\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 30,175569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 185,541951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 82,442215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 38,451615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 233,795660\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 63,057352\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,563928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 38,951092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 37,695184\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 75,719514\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 97,931061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 90,183408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 68,175113\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 44,042423\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 73,999726\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 74,245661\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,003031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 77,064951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 43,849427\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 194,908310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 47,789524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 27,435380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 98,354204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 214,523610\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 195,123577\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 111,350071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 93,111735\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 141,296571\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,589302\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 108,384699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 42,639394\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 17,388354\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 51,511754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 101,023656\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 103,684010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 114,434399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 146,118858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001787 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 15,704959\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 42,771016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 24,855924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 32,287471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 67,768270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 174,784256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 38,927970\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 26,612904\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 51,228786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 50,937023\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 50,523945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 35,057047\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 107,978511\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 73,625574\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 19,637486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 27,776182\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 19,354545\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 130,106295\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 135,634553\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 22,029626\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 102,141455\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 36,933363\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 58,258436\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 43,123386\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 54,639642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 67,807169\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 97,213288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 159,536612\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 29,890582\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 81,599657\n",
      "Evaluando PCA con 50 componentes...\n",
      "Entrenando modelo: Bagging con 50 componentes.\n",
      "Entrenando modelo: XGBoost con 50 componentes.\n",
      "Entrenando modelo: LightGBM con 50 componentes.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 64,273169\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 24,464557\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 115,136796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 57,670481\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 60,939586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 55,750927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 27,705658\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 138,736347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 205,964436\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 78,713267\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 39,304132\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 104,428593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 116,701513\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 46,013905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 51,291335\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 12,338559\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 34,430038\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 40,612303\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 33,040440\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 80,098369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 73,229852\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 67,461280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 94,569666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 181,592016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 45,739347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 67,752858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 72,622068\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 46,844218\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 7,452104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,527779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 69,061097\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 134,296905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 129,018475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 121,239396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 63,349146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 50,784006\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 158,447469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 137,444402\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 36,477111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 47,606857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 142,573312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 18,626951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 67,982222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 91,064480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 62,556705\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 130,662405\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 21,540635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 85,602709\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 75,244354\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 87,510330\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 930,674085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 164,938291\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 19,588425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 41,170736\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 45,932353\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 31,058915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 45,044980\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 155,822127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 25,901865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 40,469115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 59,415365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 242,420462\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 263,523244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,919522\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 1458,650267\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 26,460360\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 367,760696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 41,956363\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 75,765668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 290174,628003\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 193,495928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 27,921393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 25,069369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,907625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 78,124926\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 42,362030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 23,226066\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 48,646566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 19,263583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 110,747086\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 111,314815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 71,181615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 43,326206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 80,083369\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 33,183335\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 62,306457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 75,013890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 73,550714\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 46,507342\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 38,610216\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 23,522323\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 48,977445\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 65,152282\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 282,961297\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 139,099921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 58,785390\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 55,175053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 101,724580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 41,786935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 27,312301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 101,921850\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 14,925946\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 128,893837\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 34,852748\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 39,984801\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 18,732087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 72,790192\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 52,965868\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 49,959307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 80,163840\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 207,467813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 158,198037\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 35,474510\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 13,939297\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 68,939666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 121,933706\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 110,057904\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 28,648089\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 36,507881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 16,012349\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 40,213574\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 43,694452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 18,088054\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 52,245684\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,212850\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 57,268764\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 84,003560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 28,336523\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 48,889753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,750248\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 45,960254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 56,050893\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 151,604479\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 55,734691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 100,428471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 79,141622\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 41,139913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 95,675632\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 90,697100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 75,590093\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 80,720565\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 21,270384\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 73,265947\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 40,048154\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 183,839347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 69,193960\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 65,382147\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 59,263473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 69,533012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 35,633645\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 30,762858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 39,667672\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 75,536890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 29,868069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 120,350598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 55,044004\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 111,489626\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 174,183111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 44,165408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 122,259935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 198,823498\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 61,169389\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 54,694968\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 50,298670\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 66,222614\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 146,163869\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 356,007680\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 47,619585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 28,918124\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 47,225479\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 172,374794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 76,208466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 63,762829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 57,385299\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 36,487009\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 44,735082\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 22,881483\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 61,269523\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 94,116715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 69,558545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 8,556699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 61,985189\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 26,054299\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 23,913402\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 200,385033\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 152,904945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 27,240810\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 130,920254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 67,459401\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 113,352123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 18,769866\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000975 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 59,832620\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 33,997398\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 33,947352\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 85,578100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 75,346264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 18,882120\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 51,074986\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 127,397294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 78,623395\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 60,790864\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 12,516755\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 50,682244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 42,838683\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 64,422616\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 20,848652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 30,752112\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 87,281029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 53,889560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 53,989382\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 78,691136\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 83,026452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 56,526675\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 187,203133\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 273,563971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 31,931177\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 55,548615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 8,168713\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 101,802797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 143,143538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 64,592333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 39,152018\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 129,682666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 73,969593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 41,490426\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 108,941628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 10,201222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 15,940215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 27,575635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 54,714308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 13,175280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 102,550337\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 229,710742\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 23,390218\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 105,982900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 60,185947\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 202,870693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 107,834554\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 86,946373\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 187,141232\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 64,607472\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 30,392330\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 192,987652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 21,555397\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 29,571560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 16,710715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 108,515338\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 30,460976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 24,080988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 132,007583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 131,486020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 112,791938\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 17,637146\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 57,219693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 95,687315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 33,794505\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 93,480410\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 97,876709\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 21,912322\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 73,199190\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 46,093115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 16,438729\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 10,753201\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 68,939723\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 43,920423\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 14,096365\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 133,256453\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 87,612134\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 15,721990\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 63,865183\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 36,450012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 24,769727\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 44,516217\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 51,212983\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 50,536895\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 125,719618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 108,412829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 157,500368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 29,009594\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 120,689878\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 217,611502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 35,530647\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 30,944535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 87,537730\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 16,382429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 12,785285\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,435266\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 43,560569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 47,300699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 54,256971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 41,946870\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 166,063227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 82,288557\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 88,802275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 29,948731\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 132,331013\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 35,349907\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 165,742556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 148,102350\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 37,578255\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 63,843866\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 36,584154\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 136,941334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 24,394658\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 136,321247\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 48,533729\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 179,086145\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 190,675668\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000952 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 75,010080\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 1666,576030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 25,386906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 28,246333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 116,207027\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 34,985245\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 44,537847\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 151,143900\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 51,035445\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 55,438796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 18,639104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 38,739180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000952 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 182,149048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 109,450642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 107,823127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 104,351206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 41,763368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 597,522842\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 34,369095\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 34,177085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 25,141780\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 114,598988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 35,447551\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 35,497197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 202,637161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 15,914222\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 63,753815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,000422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 213,579852\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 203,597168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 23,858207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 122,415246\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 43,165149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 63,336897\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 57,179049\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 7,276545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 2497,717971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 18,823695\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 14,823864\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 14,798063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 88,881701\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 37,823600\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 55,030046\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 27,174944\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,304650\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 53,606860\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 23,705542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 45,913786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 320,540965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 50,307359\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 52,566711\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 34,226085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 132,628757\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 54,429820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 35,620081\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,620224\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 36,680865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 92,638450\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 24,312064\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 39,673829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 81,468552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 50,755618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 152,651364\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 26,687165\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 71,000772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 57,065596\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 59,719361\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 88,459196\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 37,664288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 51,104075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001086 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 153,333674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 88,449857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 21,185020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 61,188913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 159,792275\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 58,134763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 63,514118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 51,943408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 77,259296\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 67,724795\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 84,979891\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 66,216777\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 43,402760\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 380,445351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 10,367124\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 46,745199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 46,974022\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 102,399973\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 97,602807\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 140,500087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 17,722666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 238,345140\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 62,005247\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 61,282311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 59,145432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 150,244696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 51,834321\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 35,500053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 88,414274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 34,050781\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 122,890956\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 88,749976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 49,867103\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 124,911185\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 126,134195\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 38,263768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 96,792307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 169,895419\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 43,104972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 97,263327\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,745910\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 55,185356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 36,065972\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 129,361250\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 93,541580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 70,031920\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 1666,576024\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 25,477708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 133,370384\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 44,402406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 13,697914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 50,331662\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 269,285168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 195,216485\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 41,362433\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 69,253534\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 45,467382\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 30,175569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 185,541951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 82,442215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 38,451615\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 233,795660\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 63,057352\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,563928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 38,951092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 37,695184\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 75,719514\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 97,931061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 90,183408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 68,175113\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 44,042423\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 73,999726\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 74,245661\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 77,064951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 43,849427\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 194,908310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 47,789524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 27,435380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 98,354204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 214,523610\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 195,123577\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 111,350071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 93,111735\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 141,296571\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,589302\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 108,384699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 42,639394\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 17,388354\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 51,511754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 101,023656\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 103,684010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 114,434399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 146,118858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 15,704959\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 42,771016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 24,855924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 32,287471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 67,768270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 174,784256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 38,927970\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 26,612904\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 51,228786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 50,937023\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 50,523945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 35,057047\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 107,978511\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 73,625574\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 19,637486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 27,776182\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 19,354545\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 130,106295\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 135,634553\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 22,029626\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 102,141455\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 36,933363\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 58,258436\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 43,123386\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 54,639642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 67,807169\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,000978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 97,213288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 159,536612\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 29,890582\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 3616, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 81,599657\n",
      "Evaluando PCA con 100 componentes...\n",
      "Entrenando modelo: Bagging con 100 componentes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenando modelo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m con \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m componentes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_pca)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Calcular métricas\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\multioutput.py:278\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\multioutput.py:67\u001b[0m, in \u001b[0;36m_fit_estimator\u001b[1;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m     65\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:66\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     69\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     72\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:402\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    399\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    400\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:545\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, max_depth, check_input, **fit_params)\u001b[0m\n\u001b[0;32m    542\u001b[0m seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39mn_more_estimators)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m seeds\n\u001b[1;32m--> 545\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    564\u001b[0m     itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[0;32m    565\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:187\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, seeds, total_n_estimators, verbose, check_input, fit_params)\u001b[0m\n\u001b[0;32m    185\u001b[0m     fit_params_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m curr_sample_weight\n\u001b[0;32m    186\u001b[0m     X_ \u001b[38;5;241m=\u001b[39m X[:, features] \u001b[38;5;28;01mif\u001b[39;00m requires_feature_indexing \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 187\u001b[0m     \u001b[43mestimator_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# cannot use sample_weight, so use indexing\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     y_ \u001b[38;5;241m=\u001b[39m _safe_indexing(y, indices)\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1377\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m \n\u001b[0;32m   1351\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "# train = pd.read_csv('train.csv')\n",
    "# test = pd.read_csv('test.csv')\n",
    "\n",
    "# Identificar columnas objetivo\n",
    "# Identificar las columnas que contienen \"Close\" (todas)\n",
    "cols_close = [col for col in train.columns if \"Close\" in col ]+ ['Date']\n",
    "\n",
    "# Identificar las columnas que contienen \"AdjustedClose\"\n",
    "cols_adjusted = [col for col in cols_close if \"Adjusted\" in col]\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X_train = train.drop(columns=cols_close)  # Eliminar todas las columnas que contienen \"Close\"\n",
    "X_test = test.drop(columns=cols_close)\n",
    "# Limitar X_test para coincidir con y_test\n",
    "X_test =X_test.iloc[0:4, :]\n",
    "\n",
    "\n",
    "# Para y_train y y_test, quedarnos con \"AdjustedClose\" y \"Date\"\n",
    "y_train = train[cols_adjusted]\n",
    "y_test = test[cols_adjusted]\n",
    "y_test =y_test.iloc[0:4, :]\n",
    "# y_test=y_test.to_frame()\n",
    "# y_test=y_test.T\n",
    "display(y_test)\n",
    "print(type(X_train))\n",
    "\n",
    "\n",
    "# Listas de componentes y modelos\n",
    "list_componentes = [10,20,30,40,50,100,150,200, 473]\n",
    "# list_componentes = [comp for comp in range(1,11)]\n",
    "models = {\n",
    "    'Bagging': MultiOutputRegressor(BaggingRegressor(n_jobs=1, n_estimators=20)),\n",
    "    'XGBoost': XGBRegressor(n_jobs=1, objective='reg:squarederror', max_depth=4, n_estimators=50),\n",
    "    'LightGBM': MultiOutputRegressor(LGBMRegressor(n_jobs=1, objective='regression_l2')),\n",
    "}\n",
    "\n",
    "metrics = ['RMSE', 'R2', 'MAPE','MPE']\n",
    "all_metrics_df = {metric: pd.DataFrame(index=models.keys(), columns=[str(c) for c in list_componentes]) for metric in metrics}\n",
    "\n",
    "# Crear carpeta para guardar gráficos\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Evaluar modelos con diferentes componentes de PCA\n",
    "for n_components in list_componentes:\n",
    "    print(f\"Evaluando PCA con {n_components} componentes...\")\n",
    "\n",
    "    # Aplicar Incremental PCA\n",
    "    ipca = IncrementalPCA(n_components=n_components)\n",
    "    X_train_pca = ipca.fit_transform(X_train)\n",
    "    X_test_pca = ipca.transform(X_test)\n",
    "\n",
    "    # Evaluar modelos\n",
    "    metrics_df = pd.DataFrame(index=models.keys(), columns=metrics)\n",
    "    for name, model in models.items():\n",
    "        print(f\"Entrenando modelo: {name} con {n_components} componentes.\")\n",
    "        model.fit(X_train_pca, y_train)\n",
    "        predictions = model.predict(X_test_pca)\n",
    "\n",
    "        # Calcular métricas\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        # mae = mean_absolute_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "        mpe= mean_positive_error(y_test,predictions)\n",
    "        \n",
    "        metrics_df.loc[name] = [rmse,r2, mape, mpe]\n",
    "\n",
    "    # Guardar métricas en el DataFrame general\n",
    "    for metric in metrics:\n",
    "        all_metrics_df[metric][str(n_components)] = metrics_df[metric]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "axes = axes.flatten() \n",
    "fig.suptitle('Forecasting Metrics While Increasing the Number of Components 4-day Forecast', fontsize=16)\n",
    "# Graficar resultados\n",
    "# Graficar resultados en cada eje\n",
    "for i, metric in enumerate(metrics):  # Enumerar para acceder a los ejes correspondientes\n",
    "    ax = axes[i]  # Seleccionar el eje correspondiente\n",
    "    for model in models.keys():\n",
    "        ax.plot(list_componentes, \n",
    "                all_metrics_df[metric].loc[model].astype(float), \n",
    "                marker='o', \n",
    "                label=model)\n",
    "    \n",
    "    ax.set_xlabel('Number of Components (PCA)', fontsize=12)\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_title(f'Comparison of {metric}', fontsize=14)\n",
    "    ax.legend(title='Models', loc='upper right')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Ajustar el diseño y ocultar cualquier subplot vacío\n",
    "for j in range(len(metrics), len(axes)):  # Si hay más ejes que métricas\n",
    "    fig.delaxes(axes[j])  # Eliminar los ejes innecesarios\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Ajustar espacio para el título general\n",
    "plt.savefig('plots/metrics_comparison_pca_comp.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evolution of number of forecasting days**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelos para 0 días...\n",
      "Ventana vacía para 0 días. Saltando...\n",
      "Evaluando modelos para 1 días...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 2 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 3 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 4 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 5 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 6 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 7 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 8 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 9 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 10 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 11 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 12 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 13 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 14 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 15 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 16 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 17 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 18 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n",
      "Evaluando modelos para 19 días...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 64.241936\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 3522, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 24.478717\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.116934</td>\n",
       "      <td>3.645657</td>\n",
       "      <td>4.026132</td>\n",
       "      <td>3.246022</td>\n",
       "      <td>3.427823</td>\n",
       "      <td>3.482308</td>\n",
       "      <td>3.142443</td>\n",
       "      <td>3.346563</td>\n",
       "      <td>3.96214</td>\n",
       "      <td>2.796507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.668702</td>\n",
       "      <td>4.727322</td>\n",
       "      <td>4.645324</td>\n",
       "      <td>4.502353</td>\n",
       "      <td>4.356869</td>\n",
       "      <td>4.315982</td>\n",
       "      <td>4.358435</td>\n",
       "      <td>4.330405</td>\n",
       "      <td>4.343979</td>\n",
       "      <td>4.272724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.615008</td>\n",
       "      <td>3.740312</td>\n",
       "      <td>3.583931</td>\n",
       "      <td>3.443985</td>\n",
       "      <td>3.467888</td>\n",
       "      <td>3.366624</td>\n",
       "      <td>3.264774</td>\n",
       "      <td>3.193164</td>\n",
       "      <td>3.123047</td>\n",
       "      <td>3.047873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1    2    3    4    5    6    7    8    9  ...        10  \\\n",
       "Bagging   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  3.116934   \n",
       "XGBoost   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  4.668702   \n",
       "LightGBM  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  3.615008   \n",
       "\n",
       "                11        12        13        14        15        16  \\\n",
       "Bagging   3.645657  4.026132  3.246022  3.427823  3.482308  3.142443   \n",
       "XGBoost   4.727322  4.645324  4.502353  4.356869  4.315982  4.358435   \n",
       "LightGBM  3.740312  3.583931  3.443985  3.467888  3.366624  3.264774   \n",
       "\n",
       "                17        18        19  \n",
       "Bagging   3.346563   3.96214  2.796507  \n",
       "XGBoost   4.330405  4.343979  4.272724  \n",
       "LightGBM  3.193164  3.123047  3.047873  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0093</td>\n",
       "      <td>-1.576072</td>\n",
       "      <td>-7.817894</td>\n",
       "      <td>-1.35972</td>\n",
       "      <td>-2.156137</td>\n",
       "      <td>-2.106252</td>\n",
       "      <td>-2.132199</td>\n",
       "      <td>-1.951324</td>\n",
       "      <td>-3.080388</td>\n",
       "      <td>-1.269726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.698388</td>\n",
       "      <td>-7.106119</td>\n",
       "      <td>-7.278038</td>\n",
       "      <td>-7.430531</td>\n",
       "      <td>-7.516815</td>\n",
       "      <td>-7.612248</td>\n",
       "      <td>-7.796693</td>\n",
       "      <td>-6.102967</td>\n",
       "      <td>-5.725851</td>\n",
       "      <td>-5.402166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.88315</td>\n",
       "      <td>-2.198756</td>\n",
       "      <td>-2.174506</td>\n",
       "      <td>-2.183829</td>\n",
       "      <td>-2.337988</td>\n",
       "      <td>-2.410833</td>\n",
       "      <td>-2.382468</td>\n",
       "      <td>-2.23055</td>\n",
       "      <td>-2.222768</td>\n",
       "      <td>-2.060818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1    2    3    4    5    6    7    8    9  ...        10  \\\n",
       "Bagging   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...   -1.0093   \n",
       "XGBoost   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ... -7.698388   \n",
       "LightGBM  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  -1.88315   \n",
       "\n",
       "                11        12        13        14        15        16  \\\n",
       "Bagging  -1.576072 -7.817894  -1.35972 -2.156137 -2.106252 -2.132199   \n",
       "XGBoost  -7.106119 -7.278038 -7.430531 -7.516815 -7.612248 -7.796693   \n",
       "LightGBM -2.198756 -2.174506 -2.183829 -2.337988 -2.410833 -2.382468   \n",
       "\n",
       "                17        18        19  \n",
       "Bagging  -1.951324 -3.080388 -1.269726  \n",
       "XGBoost  -6.102967 -5.725851 -5.402166  \n",
       "LightGBM  -2.23055 -2.222768 -2.060818  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.040616</td>\n",
       "      <td>0.081157</td>\n",
       "      <td>0.031889</td>\n",
       "      <td>0.040806</td>\n",
       "      <td>0.037314</td>\n",
       "      <td>0.037392</td>\n",
       "      <td>0.036151</td>\n",
       "      <td>0.043037</td>\n",
       "      <td>0.033002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085911</td>\n",
       "      <td>0.086253</td>\n",
       "      <td>0.082877</td>\n",
       "      <td>0.080013</td>\n",
       "      <td>0.077608</td>\n",
       "      <td>0.075123</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.072212</td>\n",
       "      <td>0.070313</td>\n",
       "      <td>0.067932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039992</td>\n",
       "      <td>0.045903</td>\n",
       "      <td>0.04242</td>\n",
       "      <td>0.040037</td>\n",
       "      <td>0.039201</td>\n",
       "      <td>0.039094</td>\n",
       "      <td>0.038384</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>0.042089</td>\n",
       "      <td>0.041037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1    2    3    4    5    6    7    8    9  ...        10  \\\n",
       "Bagging   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.033268   \n",
       "XGBoost   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.085911   \n",
       "LightGBM  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.039992   \n",
       "\n",
       "                11        12        13        14        15        16  \\\n",
       "Bagging   0.040616  0.081157  0.031889  0.040806  0.037314  0.037392   \n",
       "XGBoost   0.086253  0.082877  0.080013  0.077608  0.075123  0.073497   \n",
       "LightGBM  0.045903   0.04242  0.040037  0.039201  0.039094  0.038384   \n",
       "\n",
       "                17        18        19  \n",
       "Bagging   0.036151  0.043037  0.033002  \n",
       "XGBoost   0.072212  0.070313  0.067932  \n",
       "LightGBM  0.040613  0.042089  0.041037  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.267407</td>\n",
       "      <td>1.746747</td>\n",
       "      <td>2.234978</td>\n",
       "      <td>1.216108</td>\n",
       "      <td>0.847366</td>\n",
       "      <td>1.404251</td>\n",
       "      <td>1.215118</td>\n",
       "      <td>1.517059</td>\n",
       "      <td>2.258948</td>\n",
       "      <td>0.80279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.868418</td>\n",
       "      <td>3.01102</td>\n",
       "      <td>3.001611</td>\n",
       "      <td>2.917638</td>\n",
       "      <td>2.812487</td>\n",
       "      <td>2.817315</td>\n",
       "      <td>2.878469</td>\n",
       "      <td>2.867021</td>\n",
       "      <td>2.886864</td>\n",
       "      <td>2.834456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.750619</td>\n",
       "      <td>1.98018</td>\n",
       "      <td>1.815165</td>\n",
       "      <td>1.686853</td>\n",
       "      <td>1.574306</td>\n",
       "      <td>1.469352</td>\n",
       "      <td>1.401286</td>\n",
       "      <td>1.339293</td>\n",
       "      <td>1.288194</td>\n",
       "      <td>1.220394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1    2    3    4    5    6    7    8    9  ...        10  \\\n",
       "Bagging   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  1.267407   \n",
       "XGBoost   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  2.868418   \n",
       "LightGBM  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  1.750619   \n",
       "\n",
       "                11        12        13        14        15        16  \\\n",
       "Bagging   1.746747  2.234978  1.216108  0.847366  1.404251  1.215118   \n",
       "XGBoost    3.01102  3.001611  2.917638  2.812487  2.817315  2.878469   \n",
       "LightGBM   1.98018  1.815165  1.686853  1.574306  1.469352  1.401286   \n",
       "\n",
       "                17        18        19  \n",
       "Bagging   1.517059  2.258948   0.80279  \n",
       "XGBoost   2.867021  2.886864  2.834456  \n",
       "LightGBM  1.339293  1.288194  1.220394  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train=pd.read_csv('train_pequeño.csv')\n",
    "test=pd.read_csv('test_pequeño.csv')\n",
    "\n",
    "cols_close = [col for col in train.columns if \"Close\" in col ]+ ['Date']\n",
    "cols_adjusted = [col for col in cols_close if \"Adjusted\" in col]\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X_train = train.drop(columns=cols_close)  # Eliminar todas las columnas que contienen \"Close\"\n",
    "X_test = test.drop(columns=cols_close)\n",
    "\n",
    "\n",
    "number_days_original=X_test.shape[0]\n",
    "print(number_days_original)\n",
    "# Para y_train y y_test, quedarnos con \"AdjustedClose\" y \"Date\"\n",
    "y_train = train[cols_adjusted]\n",
    "y_test = test[cols_adjusted]\n",
    "\n",
    "\n",
    "\n",
    "list_componentes=[day for day in range(0,number_days_original)]\n",
    "display(list_componentes)\n",
    "models = {\n",
    "    'Bagging': MultiOutputRegressor(BaggingRegressor(n_jobs=1)),\n",
    "    'XGBoost': XGBRegressor(n_jobs=1, objective='reg:squarederror'),\n",
    "    'LightGBM': MultiOutputRegressor(LGBMRegressor(n_jobs=1, objective='regression_l2')),\n",
    "}\n",
    "\n",
    "metrics = ['RMSE', 'R2', 'MAPE','MPE']\n",
    "all_metrics_df = {metric: pd.DataFrame(index=models.keys(), columns=[str(c) for c in list_componentes]) for metric in metrics}\n",
    "X_train = X_train.loc[:, ~X_train.columns.str.contains('^Unnamed')]\n",
    "X_test = X_test.loc[:, ~X_test.columns.str.contains('^Unnamed')]\n",
    "# Evaluar modelos con diferentes componentes de PCA\n",
    "for n_components in list_componentes:\n",
    "    \n",
    "    \n",
    "    print(f\"Evaluando modelos para {n_components} días...\")\n",
    "\n",
    "    # # Validar índice\n",
    "    # if n_components >= X_test.shape[0] or n_components >= y_test.shape[0]:\n",
    "    #     print(f\"Saltando n_components={n_components}: fuera de rango.\")\n",
    "    #     continue\n",
    "\n",
    "    # Seleccionar ventana\n",
    "    X_test_window = X_test.iloc[0:n_components, :].copy()\n",
    "    y_test_window = y_test.iloc[0:n_components, :].copy()\n",
    "\n",
    "    # Verificar subconjuntos no vacíos\n",
    "    if X_test_window.empty or y_test_window.empty:\n",
    "        print(f\"Ventana vacía para {n_components} días. Saltando...\")\n",
    "        continue\n",
    "\n",
    "    # Aplicar PCA\n",
    "    n_pca_components = min(8, X_train.shape[1])\n",
    "    ipca = IncrementalPCA(n_components=n_pca_components)\n",
    "    X_train_pca = ipca.fit_transform(X_train)\n",
    "    X_test_pca = ipca.transform(X_test_window)\n",
    "\n",
    "\n",
    "    # Evaluar modelos\n",
    "    metrics_df = pd.DataFrame(index=models.keys(), columns=metrics)\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_pca, y_train)\n",
    "        predictions = model.predict(X_test_pca)\n",
    "\n",
    "        # Calcular métricas\n",
    "        mse = mean_squared_error(y_test_window, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        # mae = mean_absolute_error(y_test, predictions)\n",
    "        r2   = r2_score(y_test_window, predictions)\n",
    "        mape = mean_absolute_percentage_error(y_test_window, predictions)\n",
    "        mpe  = mean_positive_error(y_test_window,predictions)\n",
    "        \n",
    "        metrics_df.loc[name] = [rmse,r2, mape, mpe]\n",
    "\n",
    "    # Guardar métricas en el DataFrame general\n",
    "    for metric in metrics:\n",
    "        all_metrics_df[metric][n_components] = metrics_df[metric]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten() \n",
    "fig.suptitle('Forecasting Metrics While Increasing forecast days', fontsize=16)\n",
    "# Graficar resultados\n",
    "\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    display(all_metrics_df[metric])\n",
    "    for model in models.keys():\n",
    "        ax.plot(all_metrics_df[metric].columns.astype(int),\n",
    "                all_metrics_df[metric].loc[model].astype(float),\n",
    "                marker='o', label=model)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('Ventana de Predicción (Días)')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# Ajustar el diseño y ocultar cualquier subplot vacío\n",
    "for j in range(len(metrics), len(axes)):  # Si hay más ejes que métricas\n",
    "    fig.delaxes(axes[j])  # Eliminar los ejes innecesarios\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Ajustar espacio para el título general\n",
    "plt.savefig('plots/metrics_comparison_days_comp.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_close = [col for col in train.columns if \"Close\" in col ]+ ['Date']\n",
    "cols_adjusted = [col for col in cols_close if \"Adjusted\" in col]\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X_train = train.drop(columns=cols_close)  # Eliminar todas las columnas que contienen \"Close\"\n",
    "X_test = test.drop(columns=cols_close)\n",
    "\n",
    "\n",
    "number_days_original=X_test.shape[0]\n",
    "# Para y_train y y_test, quedarnos con \"AdjustedClose\" y \"Date\"\n",
    "y_train = train[cols_adjusted]\n",
    "y_test = test[cols_adjusted]\n",
    "\n",
    "ipca = IncrementalPCA(n_components=473)\n",
    "X_train = ipca.fit_transform(X_train)\n",
    "X_test = ipca.transform(X_test)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=25)\n",
    "\n",
    "param_grids = {\n",
    "    'DummyRegressor': {},\n",
    "    \n",
    "    'RandomForest': {\n",
    "        'estimator__n_estimators': [50, 100, 200],\n",
    "        'estimator__max_depth': [3, 5, 7, None],\n",
    "        'estimator__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    \n",
    "    'GradientBoosting': {\n",
    "        'estimator__n_estimators': [50, 100, 200],\n",
    "        'estimator__max_depth': [3, 5, 7],\n",
    "        'estimator__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'estimator__subsample': [1.0, 0.8, 0.6]\n",
    "    },\n",
    "    \n",
    "    'AdaBoost': {\n",
    "        'estimator__n_estimators': [50, 100, 200],\n",
    "        'estimator__learning_rate': [0.01, 0.1, 0.5]\n",
    "    },\n",
    "    \n",
    "    'KNN': {\n",
    "        'estimator__n_neighbors': [5, 15, 45],\n",
    "        'estimator__weights': ['uniform', 'distance'],\n",
    "        'estimator__p': [1, 2]  # distancia Manhattan o Euclídea\n",
    "    },\n",
    "    \n",
    "    'LinearRegression': {\n",
    "        # Normalmente no hay mucho que tunear, pero puedes probar:\n",
    "        'estimator__fit_intercept': [True, False]\n",
    "    },\n",
    "    \n",
    "    'DecisionTree': {\n",
    "        'estimator__max_depth': [3, 5, 7, None],\n",
    "        'estimator__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    \n",
    "    'SVR': {\n",
    "        'estimator__C': [0.1, 1, 10],\n",
    "        'estimator__epsilon': [0.01, 0.1, 0.5],\n",
    "        'estimator__kernel': ['rbf', 'linear']\n",
    "    },\n",
    "    \n",
    "    'GaussianProcess': {\n",
    "        # Ajustar la regularización:\n",
    "        'estimator__alpha': [1e-2, 1e-1, 1],\n",
    "        # Si fuese posible ajustar kernel, agregar aquí. Por defecto RBF.\n",
    "    },\n",
    "    \n",
    "    'ExtraTrees': {\n",
    "        'estimator__n_estimators': [50, 100, 200],\n",
    "        'estimator__max_depth': [3, 5, 7, None],\n",
    "        'estimator__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    \n",
    "    'Bagging': {\n",
    "        'estimator__n_estimators': [10, 20, 50],\n",
    "        'estimator__max_samples': [0.5, 0.8, 1.0],\n",
    "        'estimator__max_features': [0.5, 0.8, 1.0]\n",
    "    },\n",
    "    \n",
    "    'Ridge': {\n",
    "        'estimator__alpha': [0.1, 1.0, 10.0]\n",
    "    },\n",
    "    \n",
    "    'Lasso': {\n",
    "        'estimator__alpha': [0.01, 0.1, 1.0]\n",
    "    },\n",
    "    \n",
    "    'ElasticNet': {\n",
    "        'estimator__alpha': [0.01, 0.1, 1.0],\n",
    "        'estimator__l1_ratio': [0.2, 0.5, 0.8]\n",
    "    },\n",
    "    \n",
    "    'BayesianRidge': {\n",
    "        # Parámetros de regularización:\n",
    "        'estimator__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'estimator__alpha_2': [1e-7, 1e-6, 1e-5]\n",
    "    },\n",
    "    \n",
    "    'ARDRegression': {\n",
    "        'estimator__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'estimator__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'estimator__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'estimator__lambda_2': [1e-7, 1e-6, 1e-5]\n",
    "    },\n",
    "    \n",
    "    'Huber': {\n",
    "        'estimator__alpha': [0.0001, 0.01, 0.1],\n",
    "        'estimator__epsilon': [1.1, 1.35, 1.5]\n",
    "    },\n",
    "    \n",
    "    'PassiveAggressive': {\n",
    "        'estimator__C': [0.1, 0.5, 1.0],\n",
    "        'estimator__max_iter': [100, 500, 1000]\n",
    "    },\n",
    "    \n",
    "    'TheilSen': {\n",
    "        'estimator__max_subpopulation': [1000, 5000],\n",
    "        'estimator__n_subsamples': [None, 50, 100]\n",
    "    },\n",
    "    \n",
    "    'CatBoost': {\n",
    "        'depth': [3, 4, 6],\n",
    "        'iterations': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [1.0, 0.8]\n",
    "    },\n",
    "    \n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 4, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [1.0, 0.8],\n",
    "        'colsample_bytree': [1.0, 0.8]\n",
    "    },\n",
    "    \n",
    "    'LightGBM': {\n",
    "        'estimator__n_estimators': [50, 100, 200],\n",
    "        'estimator__num_leaves': [31, 63, 127],\n",
    "        'estimator__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'estimator__subsample': [1.0, 0.8]\n",
    "    },\n",
    "    \n",
    "    'HistGradientBoosting': {\n",
    "        'estimator__max_iter': [50, 100, 200],\n",
    "        'estimator__max_depth': [3, 5, 7],\n",
    "        'estimator__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'estimator__l2_regularization': [0.0, 0.1, 1.0]\n",
    "    },\n",
    "    \n",
    "    'Poisson': {\n",
    "        'estimator__alpha': [0.1, 1.0, 10.0],\n",
    "        'estimator__max_iter': [100, 300, 500]\n",
    "    },\n",
    "    \n",
    "    'Tweedie': {\n",
    "        'estimator__alpha': [0.1, 1.0, 10.0],\n",
    "        'estimator__power': [1.0, 1.5, 2.0],  # Ajustar dependiendo del rango de la variable objetivo\n",
    "        'estimator__max_iter': [100, 300, 500]\n",
    "    }\n",
    "}\n",
    "\n",
    "if param_grid:\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=20,\n",
    "            cv=5,\n",
    "            scoring=scoring,\n",
    "            refit='RMSE',\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        best_model = search.best_estimator_\n",
    "        print(f\"Best Parameters for {name}:\", search.best_params_)\n",
    "\n",
    "split_counter = 1\n",
    "total_splits = tscv.get_n_splits(X_train)\n",
    "for train_index, val_index in tscv.split(X_train):\n",
    "    progress_percentage = (split_counter / total_splits) * 100\n",
    "    \n",
    "    # Dividir los datos según los índices\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    X_val_fold = X_train.iloc[val_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    y_val_fold = y_train.iloc[val_index]\n",
    "    \n",
    "    X_train = X_train.loc[:, ~X_train.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Entrenar el modelo optimizado en el fold actual\n",
    "    best_model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    predictions = best_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calcular métricas para este fold\n",
    "    mse = mean_squared_error(y_val_fold, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_val_fold, predictions)\n",
    "    r2 = r2_score(y_val_fold, predictions)\n",
    "    mape = mean_absolute_percentage_error(y_val_fold, predictions)\n",
    "    \n",
    "    # Guardar métricas del fold\n",
    "    mse_list.append(mse)\n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "    mape_list.append(mape)\n",
    "    \n",
    "    split_counter += 1\n",
    "\n",
    "# Promediar las métricas sobre todos los folds y guardarlas\n",
    "metrics= [\n",
    "    np.mean(mse_list),\n",
    "    np.mean(rmse_list),\n",
    "    np.mean(mae_list),\n",
    "    np.mean(r2_list),\n",
    "    np.mean(mape_list)\n",
    "]\n",
    "\n",
    "metrics_df_sorted = metrics_df.sort_values(by='RMSE', ascending=True)\n",
    "\n",
    "\n",
    "best_model.fit(X_train_pca, y_train)\n",
    "predictions = model.predict(X_test_pca)\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions,columns=y_train.columns)\n",
    "predictions_df.to_csv('predicciones_multisalida.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3D Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3D Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared all files in destination folder: data/probar\n",
      "Copied: SBAC.csv\n",
      "Copied: MHK.csv\n",
      "Copied: GD.csv\n",
      "Copied: MOS.csv\n",
      "Copied: ADI.csv\n",
      "Copied: BXP.csv\n",
      "Copied: ETSY.csv\n",
      "Copied: MS.csv\n",
      "Copied: PNR.csv\n",
      "Copied: ENPH.csv\n",
      "Copied: MCHP.csv\n",
      "Copied: PFE.csv\n",
      "Copied: NDAQ.csv\n",
      "Copied: F.csv\n",
      "Copied: ADM.csv\n",
      "Copied: AVGO.csv\n",
      "Copied: DAY.csv\n",
      "Copied: FDS.csv\n",
      "Copied: UAL.csv\n",
      "Copied: CI.csv\n",
      "Copied: ESS.csv\n",
      "Copied: FITB.csv\n",
      "Copied: CPAY.csv\n",
      "Copied: ROP.csv\n",
      "Copied: KMI.csv\n",
      "Copied: DTE.csv\n",
      "Copied: APTV.csv\n",
      "Copied: BRO.csv\n",
      "Copied: PRU.csv\n",
      "Copied: CB.csv\n",
      "Copied: NKE.csv\n",
      "Copied: BAC.csv\n",
      "Copied: EFX.csv\n",
      "Copied: SYF.csv\n",
      "Copied: ROK.csv\n",
      "Copied: SO.csv\n",
      "Copied: JKHY.csv\n",
      "Copied: DOC.csv\n",
      "Copied: FTNT.csv\n",
      "Copied: WELL.csv\n",
      "Copied: BR.csv\n",
      "Copied: CAH.csv\n",
      "Copied: T.csv\n",
      "Copied: VRSK.csv\n",
      "Copied: AMAT.csv\n",
      "Copied: WYNN.csv\n",
      "Copied: EW.csv\n",
      "Copied: OTIS.csv\n",
      "Copied: LLY.csv\n",
      "Copied: IFF.csv\n",
      "Successfully copied 50 files to the folder: data/probar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\Proyect2_LABIACD\\utils.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_dataset.reset_index(inplace=True)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Horizontal Structure (3542, 331)\n",
      "Valores Nulos en Horizontal Series([], dtype: int64)\n",
      "Duplicados:  0\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################### CÓDIGO DATOS PEQUEÑOS\n",
    "# Importar y recargar utilidades\n",
    "reload(utils)\n",
    "from utils import copy_files\n",
    "\n",
    "# Configuración de carpetas y parámetros\n",
    "carpeta_origen = r'data/stocks'\n",
    "carpeta_destino = r'data/probar'\n",
    "num_archivos = 50\n",
    "aleatorio = True\n",
    "\n",
    "# Copiar archivos\n",
    "copy_files(carpeta_origen, carpeta_destino, num_archivos, aleatorio)\n",
    "\n",
    "# Cargar y procesar datos originales\n",
    "reload(utils)\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "original_data     = join_stock_data(r\"data/probar\",axis=True)\n",
    "original_data['Date'] = pd.to_datetime(original_data['Date']).dt.tz_localize(None)\n",
    "start = '2010-01-01'\n",
    "end = '2024-01-31'\n",
    "\n",
    "macro_data=join_macro(start,end)\n",
    "\n",
    "raw_data = pd.merge(original_data, macro_data, on='Date', how='inner')\n",
    "raw_data = raw_data.sort_values(by=[ 'Date']).reset_index(drop=True)\n",
    "raw_data=raw_data[raw_data['Date'].notna()]\n",
    "\n",
    "\n",
    "\n",
    "raw_data.bfill( inplace=True)\n",
    "raw_data.ffill(inplace=True)\n",
    "\n",
    "df = raw_data\n",
    "print('Size of Horizontal Structure',raw_data.shape)\n",
    "\n",
    "\n",
    "# Iterar sobre todas las columnas menos 'Date'\n",
    "for columna in df.columns:\n",
    "    if columna not in ['Date'] :\n",
    "        # Verificar valores no numéricos en la columna\n",
    "        non_numeric_values = df[columna][~df[columna].apply(lambda x: isinstance(x, (int, float)))]\n",
    "        # Convertir la columna a tipo numérico, forzando los errores a NaN\n",
    "        df[columna] = pd.to_numeric(df[columna], errors='coerce')\n",
    "        \n",
    "\n",
    "\n",
    "volume_columns = [col for col in df.columns if \"Volume\" in col]\n",
    "for col in volume_columns: \n",
    "    df[col]=np.log1p(df[col])\n",
    "\n",
    "\n",
    "reload(utils)\n",
    "\n",
    "df2=df.copy()\n",
    "\n",
    "df2['Date']=pd.to_datetime(df2['Date'],format=\"ISO8601\")\n",
    "\n",
    "datos_tecnicos=join_technical_indicators(database=df2,folder = r\"data/probar\",axis=True)\n",
    "\n",
    "df2 = pd.merge(df2, datos_tecnicos, on='Date', how='outer')\n",
    "df2 = df2.sort_values(by=[ 'Date']).reset_index(drop=True)\n",
    "\n",
    "df2['day']          = df2['Date'].dt.day.astype('float64')\n",
    "df2['month']        = df2['Date'].dt.month.astype('float64')\n",
    "df2['year']         = df2['Date'].dt.year.astype('float64')\n",
    "df2['day_of_week']  = df2['Date'].dt.dayofweek.astype('float64')\n",
    "\n",
    "\n",
    "# Filtrar y mostrar columnas con valores nulos en df2\n",
    "null_columns_df2 = df2.isnull().sum()\n",
    "print('Valores Nulos en Horizontal',null_columns_df2[null_columns_df2 > 0])\n",
    "\n",
    "print('Duplicados: ',df2.duplicated().sum())\n",
    "\n",
    "columnas_a_eliminar = [col for col in df2.columns if \"Close_\" in col and \"AdjustedClose_\" not in col]\n",
    "df2 = df2.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "\n",
    "\n",
    "train=df2[df2['Date']<'2024-01-01']\n",
    "test=df2[df2['Date']>='2024-01-01']\n",
    "\n",
    "# display(test['Date'])\n",
    "    \n",
    "\n",
    "train = train.loc[:, ~train.columns.str.contains('^Unnamed')]\n",
    "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Empresas 50\n",
      "Num Fechas  3522\n",
      "Num Características Empresa 19\n",
      "Num Características Macro 31\n",
      "Fecha actual 2010-01-04 00:00:00\n",
      "Fecha actual 2010-01-05 00:00:00\n",
      "Fecha actual 2010-01-06 00:00:00\n",
      "Fecha actual 2010-01-07 00:00:00\n",
      "Fecha actual 2010-01-08 00:00:00\n",
      "Fecha actual 2010-01-11 00:00:00\n",
      "Fecha actual 2010-01-12 00:00:00\n",
      "Fecha actual 2010-01-13 00:00:00\n",
      "Fecha actual 2010-01-14 00:00:00\n",
      "Fecha actual 2010-01-15 00:00:00\n",
      "Fecha actual 2010-01-19 00:00:00\n",
      "Fecha actual 2010-01-20 00:00:00\n",
      "Fecha actual 2010-01-21 00:00:00\n",
      "Fecha actual 2010-01-22 00:00:00\n",
      "Fecha actual 2010-01-25 00:00:00\n",
      "Fecha actual 2010-01-26 00:00:00\n",
      "Fecha actual 2010-01-27 00:00:00\n",
      "Fecha actual 2010-01-28 00:00:00\n",
      "Fecha actual 2010-01-29 00:00:00\n",
      "Fecha actual 2010-02-01 00:00:00\n",
      "Fecha actual 2010-02-02 00:00:00\n",
      "Fecha actual 2010-02-03 00:00:00\n",
      "Fecha actual 2010-02-04 00:00:00\n",
      "Fecha actual 2010-02-05 00:00:00\n",
      "Fecha actual 2010-02-08 00:00:00\n",
      "Fecha actual 2010-02-09 00:00:00\n",
      "Fecha actual 2010-02-10 00:00:00\n",
      "Fecha actual 2010-02-11 00:00:00\n",
      "Fecha actual 2010-02-12 00:00:00\n",
      "Fecha actual 2010-02-16 00:00:00\n",
      "Fecha actual 2010-02-17 00:00:00\n",
      "Fecha actual 2010-02-18 00:00:00\n",
      "Fecha actual 2010-02-19 00:00:00\n",
      "Fecha actual 2010-02-22 00:00:00\n",
      "Fecha actual 2010-02-23 00:00:00\n",
      "Fecha actual 2010-02-24 00:00:00\n",
      "Fecha actual 2010-02-25 00:00:00\n",
      "Fecha actual 2010-02-26 00:00:00\n",
      "Fecha actual 2010-03-01 00:00:00\n",
      "Fecha actual 2010-03-02 00:00:00\n",
      "Fecha actual 2010-03-03 00:00:00\n",
      "Fecha actual 2010-03-04 00:00:00\n",
      "Fecha actual 2010-03-05 00:00:00\n",
      "Fecha actual 2010-03-08 00:00:00\n",
      "Fecha actual 2010-03-09 00:00:00\n",
      "Fecha actual 2010-03-10 00:00:00\n",
      "Fecha actual 2010-03-11 00:00:00\n",
      "Fecha actual 2010-03-12 00:00:00\n",
      "Fecha actual 2010-03-15 00:00:00\n",
      "Fecha actual 2010-03-16 00:00:00\n",
      "Fecha actual 2010-03-17 00:00:00\n",
      "Fecha actual 2010-03-18 00:00:00\n",
      "Fecha actual 2010-03-19 00:00:00\n",
      "Fecha actual 2010-03-22 00:00:00\n",
      "Fecha actual 2010-03-23 00:00:00\n",
      "Fecha actual 2010-03-24 00:00:00\n",
      "Fecha actual 2010-03-25 00:00:00\n",
      "Fecha actual 2010-03-26 00:00:00\n",
      "Fecha actual 2010-03-29 00:00:00\n",
      "Fecha actual 2010-03-30 00:00:00\n",
      "Fecha actual 2010-03-31 00:00:00\n",
      "Fecha actual 2010-04-01 00:00:00\n",
      "Fecha actual 2010-04-05 00:00:00\n",
      "Fecha actual 2010-04-06 00:00:00\n",
      "Fecha actual 2010-04-07 00:00:00\n",
      "Fecha actual 2010-04-08 00:00:00\n",
      "Fecha actual 2010-04-09 00:00:00\n",
      "Fecha actual 2010-04-12 00:00:00\n",
      "Fecha actual 2010-04-13 00:00:00\n",
      "Fecha actual 2010-04-14 00:00:00\n",
      "Fecha actual 2010-04-15 00:00:00\n",
      "Fecha actual 2010-04-16 00:00:00\n",
      "Fecha actual 2010-04-19 00:00:00\n",
      "Fecha actual 2010-04-20 00:00:00\n",
      "Fecha actual 2010-04-21 00:00:00\n",
      "Fecha actual 2010-04-22 00:00:00\n",
      "Fecha actual 2010-04-23 00:00:00\n",
      "Fecha actual 2010-04-26 00:00:00\n",
      "Fecha actual 2010-04-27 00:00:00\n",
      "Fecha actual 2010-04-28 00:00:00\n",
      "Fecha actual 2010-04-29 00:00:00\n",
      "Fecha actual 2010-04-30 00:00:00\n",
      "Fecha actual 2010-05-03 00:00:00\n",
      "Fecha actual 2010-05-04 00:00:00\n",
      "Fecha actual 2010-05-05 00:00:00\n",
      "Fecha actual 2010-05-06 00:00:00\n",
      "Fecha actual 2010-05-07 00:00:00\n",
      "Fecha actual 2010-05-10 00:00:00\n",
      "Fecha actual 2010-05-11 00:00:00\n",
      "Fecha actual 2010-05-12 00:00:00\n",
      "Fecha actual 2010-05-13 00:00:00\n",
      "Fecha actual 2010-05-14 00:00:00\n",
      "Fecha actual 2010-05-17 00:00:00\n",
      "Fecha actual 2010-05-18 00:00:00\n",
      "Fecha actual 2010-05-19 00:00:00\n",
      "Fecha actual 2010-05-20 00:00:00\n",
      "Fecha actual 2010-05-21 00:00:00\n",
      "Fecha actual 2010-05-24 00:00:00\n",
      "Fecha actual 2010-05-25 00:00:00\n",
      "Fecha actual 2010-05-26 00:00:00\n",
      "Fecha actual 2010-05-27 00:00:00\n",
      "Fecha actual 2010-05-28 00:00:00\n",
      "Fecha actual 2010-06-01 00:00:00\n",
      "Fecha actual 2010-06-02 00:00:00\n",
      "Fecha actual 2010-06-03 00:00:00\n",
      "Fecha actual 2010-06-04 00:00:00\n",
      "Fecha actual 2010-06-07 00:00:00\n",
      "Fecha actual 2010-06-08 00:00:00\n",
      "Fecha actual 2010-06-09 00:00:00\n",
      "Fecha actual 2010-06-10 00:00:00\n",
      "Fecha actual 2010-06-11 00:00:00\n",
      "Fecha actual 2010-06-14 00:00:00\n",
      "Fecha actual 2010-06-15 00:00:00\n",
      "Fecha actual 2010-06-16 00:00:00\n",
      "Fecha actual 2010-06-17 00:00:00\n",
      "Fecha actual 2010-06-18 00:00:00\n",
      "Fecha actual 2010-06-21 00:00:00\n",
      "Fecha actual 2010-06-22 00:00:00\n",
      "Fecha actual 2010-06-23 00:00:00\n",
      "Fecha actual 2010-06-24 00:00:00\n",
      "Fecha actual 2010-06-25 00:00:00\n",
      "Fecha actual 2010-06-28 00:00:00\n",
      "Fecha actual 2010-06-29 00:00:00\n",
      "Fecha actual 2010-06-30 00:00:00\n",
      "Fecha actual 2010-07-01 00:00:00\n",
      "Fecha actual 2010-07-02 00:00:00\n",
      "Fecha actual 2010-07-06 00:00:00\n",
      "Fecha actual 2010-07-07 00:00:00\n",
      "Fecha actual 2010-07-08 00:00:00\n",
      "Fecha actual 2010-07-09 00:00:00\n",
      "Fecha actual 2010-07-12 00:00:00\n",
      "Fecha actual 2010-07-13 00:00:00\n",
      "Fecha actual 2010-07-14 00:00:00\n",
      "Fecha actual 2010-07-15 00:00:00\n",
      "Fecha actual 2010-07-16 00:00:00\n",
      "Fecha actual 2010-07-19 00:00:00\n",
      "Fecha actual 2010-07-20 00:00:00\n",
      "Fecha actual 2010-07-21 00:00:00\n",
      "Fecha actual 2010-07-22 00:00:00\n",
      "Fecha actual 2010-07-23 00:00:00\n",
      "Fecha actual 2010-07-26 00:00:00\n",
      "Fecha actual 2010-07-27 00:00:00\n",
      "Fecha actual 2010-07-28 00:00:00\n",
      "Fecha actual 2010-07-29 00:00:00\n",
      "Fecha actual 2010-07-30 00:00:00\n",
      "Fecha actual 2010-08-02 00:00:00\n",
      "Fecha actual 2010-08-03 00:00:00\n",
      "Fecha actual 2010-08-04 00:00:00\n",
      "Fecha actual 2010-08-05 00:00:00\n",
      "Fecha actual 2010-08-06 00:00:00\n",
      "Fecha actual 2010-08-09 00:00:00\n",
      "Fecha actual 2010-08-10 00:00:00\n",
      "Fecha actual 2010-08-11 00:00:00\n",
      "Fecha actual 2010-08-12 00:00:00\n",
      "Fecha actual 2010-08-13 00:00:00\n",
      "Fecha actual 2010-08-16 00:00:00\n",
      "Fecha actual 2010-08-17 00:00:00\n",
      "Fecha actual 2010-08-18 00:00:00\n",
      "Fecha actual 2010-08-19 00:00:00\n",
      "Fecha actual 2010-08-20 00:00:00\n",
      "Fecha actual 2010-08-23 00:00:00\n",
      "Fecha actual 2010-08-24 00:00:00\n",
      "Fecha actual 2010-08-25 00:00:00\n",
      "Fecha actual 2010-08-26 00:00:00\n",
      "Fecha actual 2010-08-27 00:00:00\n",
      "Fecha actual 2010-08-30 00:00:00\n",
      "Fecha actual 2010-08-31 00:00:00\n",
      "Fecha actual 2010-09-01 00:00:00\n",
      "Fecha actual 2010-09-02 00:00:00\n",
      "Fecha actual 2010-09-03 00:00:00\n",
      "Fecha actual 2010-09-07 00:00:00\n",
      "Fecha actual 2010-09-08 00:00:00\n",
      "Fecha actual 2010-09-09 00:00:00\n",
      "Fecha actual 2010-09-10 00:00:00\n",
      "Fecha actual 2010-09-13 00:00:00\n",
      "Fecha actual 2010-09-14 00:00:00\n",
      "Fecha actual 2010-09-15 00:00:00\n",
      "Fecha actual 2010-09-16 00:00:00\n",
      "Fecha actual 2010-09-17 00:00:00\n",
      "Fecha actual 2010-09-20 00:00:00\n",
      "Fecha actual 2010-09-21 00:00:00\n",
      "Fecha actual 2010-09-22 00:00:00\n",
      "Fecha actual 2010-09-23 00:00:00\n",
      "Fecha actual 2010-09-24 00:00:00\n",
      "Fecha actual 2010-09-27 00:00:00\n",
      "Fecha actual 2010-09-28 00:00:00\n",
      "Fecha actual 2010-09-29 00:00:00\n",
      "Fecha actual 2010-09-30 00:00:00\n",
      "Fecha actual 2010-10-01 00:00:00\n",
      "Fecha actual 2010-10-04 00:00:00\n",
      "Fecha actual 2010-10-05 00:00:00\n",
      "Fecha actual 2010-10-06 00:00:00\n",
      "Fecha actual 2010-10-07 00:00:00\n",
      "Fecha actual 2010-10-08 00:00:00\n",
      "Fecha actual 2010-10-11 00:00:00\n",
      "Fecha actual 2010-10-12 00:00:00\n",
      "Fecha actual 2010-10-13 00:00:00\n",
      "Fecha actual 2010-10-14 00:00:00\n",
      "Fecha actual 2010-10-15 00:00:00\n",
      "Fecha actual 2010-10-18 00:00:00\n",
      "Fecha actual 2010-10-19 00:00:00\n",
      "Fecha actual 2010-10-20 00:00:00\n",
      "Fecha actual 2010-10-21 00:00:00\n",
      "Fecha actual 2010-10-22 00:00:00\n",
      "Fecha actual 2010-10-25 00:00:00\n",
      "Fecha actual 2010-10-26 00:00:00\n",
      "Fecha actual 2010-10-27 00:00:00\n",
      "Fecha actual 2010-10-28 00:00:00\n",
      "Fecha actual 2010-10-29 00:00:00\n",
      "Fecha actual 2010-11-01 00:00:00\n",
      "Fecha actual 2010-11-02 00:00:00\n",
      "Fecha actual 2010-11-03 00:00:00\n",
      "Fecha actual 2010-11-04 00:00:00\n",
      "Fecha actual 2010-11-05 00:00:00\n",
      "Fecha actual 2010-11-08 00:00:00\n",
      "Fecha actual 2010-11-09 00:00:00\n",
      "Fecha actual 2010-11-10 00:00:00\n",
      "Fecha actual 2010-11-11 00:00:00\n",
      "Fecha actual 2010-11-12 00:00:00\n",
      "Fecha actual 2010-11-15 00:00:00\n",
      "Fecha actual 2010-11-16 00:00:00\n",
      "Fecha actual 2010-11-17 00:00:00\n",
      "Fecha actual 2010-11-18 00:00:00\n",
      "Fecha actual 2010-11-19 00:00:00\n",
      "Fecha actual 2010-11-22 00:00:00\n",
      "Fecha actual 2010-11-23 00:00:00\n",
      "Fecha actual 2010-11-24 00:00:00\n",
      "Fecha actual 2010-11-26 00:00:00\n",
      "Fecha actual 2010-11-29 00:00:00\n",
      "Fecha actual 2010-11-30 00:00:00\n",
      "Fecha actual 2010-12-01 00:00:00\n",
      "Fecha actual 2010-12-02 00:00:00\n",
      "Fecha actual 2010-12-03 00:00:00\n",
      "Fecha actual 2010-12-06 00:00:00\n",
      "Fecha actual 2010-12-07 00:00:00\n",
      "Fecha actual 2010-12-08 00:00:00\n",
      "Fecha actual 2010-12-09 00:00:00\n",
      "Fecha actual 2010-12-10 00:00:00\n",
      "Fecha actual 2010-12-13 00:00:00\n",
      "Fecha actual 2010-12-14 00:00:00\n",
      "Fecha actual 2010-12-15 00:00:00\n",
      "Fecha actual 2010-12-16 00:00:00\n",
      "Fecha actual 2010-12-17 00:00:00\n",
      "Fecha actual 2010-12-20 00:00:00\n",
      "Fecha actual 2010-12-21 00:00:00\n",
      "Fecha actual 2010-12-22 00:00:00\n",
      "Fecha actual 2010-12-23 00:00:00\n",
      "Fecha actual 2010-12-27 00:00:00\n",
      "Fecha actual 2010-12-28 00:00:00\n",
      "Fecha actual 2010-12-29 00:00:00\n",
      "Fecha actual 2010-12-30 00:00:00\n",
      "Fecha actual 2010-12-31 00:00:00\n",
      "Fecha actual 2011-01-03 00:00:00\n",
      "Fecha actual 2011-01-04 00:00:00\n",
      "Fecha actual 2011-01-05 00:00:00\n",
      "Fecha actual 2011-01-06 00:00:00\n",
      "Fecha actual 2011-01-07 00:00:00\n",
      "Fecha actual 2011-01-10 00:00:00\n",
      "Fecha actual 2011-01-11 00:00:00\n",
      "Fecha actual 2011-01-12 00:00:00\n",
      "Fecha actual 2011-01-13 00:00:00\n",
      "Fecha actual 2011-01-14 00:00:00\n",
      "Fecha actual 2011-01-18 00:00:00\n",
      "Fecha actual 2011-01-19 00:00:00\n",
      "Fecha actual 2011-01-20 00:00:00\n",
      "Fecha actual 2011-01-21 00:00:00\n",
      "Fecha actual 2011-01-24 00:00:00\n",
      "Fecha actual 2011-01-25 00:00:00\n",
      "Fecha actual 2011-01-26 00:00:00\n",
      "Fecha actual 2011-01-27 00:00:00\n",
      "Fecha actual 2011-01-28 00:00:00\n",
      "Fecha actual 2011-01-31 00:00:00\n",
      "Fecha actual 2011-02-01 00:00:00\n",
      "Fecha actual 2011-02-02 00:00:00\n",
      "Fecha actual 2011-02-03 00:00:00\n",
      "Fecha actual 2011-02-04 00:00:00\n",
      "Fecha actual 2011-02-07 00:00:00\n",
      "Fecha actual 2011-02-08 00:00:00\n",
      "Fecha actual 2011-02-09 00:00:00\n",
      "Fecha actual 2011-02-10 00:00:00\n",
      "Fecha actual 2011-02-11 00:00:00\n",
      "Fecha actual 2011-02-14 00:00:00\n",
      "Fecha actual 2011-02-15 00:00:00\n",
      "Fecha actual 2011-02-16 00:00:00\n",
      "Fecha actual 2011-02-17 00:00:00\n",
      "Fecha actual 2011-02-18 00:00:00\n",
      "Fecha actual 2011-02-22 00:00:00\n",
      "Fecha actual 2011-02-23 00:00:00\n",
      "Fecha actual 2011-02-24 00:00:00\n",
      "Fecha actual 2011-02-25 00:00:00\n",
      "Fecha actual 2011-02-28 00:00:00\n",
      "Fecha actual 2011-03-01 00:00:00\n",
      "Fecha actual 2011-03-02 00:00:00\n",
      "Fecha actual 2011-03-03 00:00:00\n",
      "Fecha actual 2011-03-04 00:00:00\n",
      "Fecha actual 2011-03-07 00:00:00\n",
      "Fecha actual 2011-03-08 00:00:00\n",
      "Fecha actual 2011-03-09 00:00:00\n",
      "Fecha actual 2011-03-10 00:00:00\n",
      "Fecha actual 2011-03-11 00:00:00\n",
      "Fecha actual 2011-03-14 00:00:00\n",
      "Fecha actual 2011-03-15 00:00:00\n",
      "Fecha actual 2011-03-16 00:00:00\n",
      "Fecha actual 2011-03-17 00:00:00\n",
      "Fecha actual 2011-03-18 00:00:00\n",
      "Fecha actual 2011-03-21 00:00:00\n",
      "Fecha actual 2011-03-22 00:00:00\n",
      "Fecha actual 2011-03-23 00:00:00\n",
      "Fecha actual 2011-03-24 00:00:00\n",
      "Fecha actual 2011-03-25 00:00:00\n",
      "Fecha actual 2011-03-28 00:00:00\n",
      "Fecha actual 2011-03-29 00:00:00\n",
      "Fecha actual 2011-03-30 00:00:00\n",
      "Fecha actual 2011-03-31 00:00:00\n",
      "Fecha actual 2011-04-01 00:00:00\n",
      "Fecha actual 2011-04-04 00:00:00\n",
      "Fecha actual 2011-04-05 00:00:00\n",
      "Fecha actual 2011-04-06 00:00:00\n",
      "Fecha actual 2011-04-07 00:00:00\n",
      "Fecha actual 2011-04-08 00:00:00\n",
      "Fecha actual 2011-04-11 00:00:00\n",
      "Fecha actual 2011-04-12 00:00:00\n",
      "Fecha actual 2011-04-13 00:00:00\n",
      "Fecha actual 2011-04-14 00:00:00\n",
      "Fecha actual 2011-04-15 00:00:00\n",
      "Fecha actual 2011-04-18 00:00:00\n",
      "Fecha actual 2011-04-19 00:00:00\n",
      "Fecha actual 2011-04-20 00:00:00\n",
      "Fecha actual 2011-04-21 00:00:00\n",
      "Fecha actual 2011-04-25 00:00:00\n",
      "Fecha actual 2011-04-26 00:00:00\n",
      "Fecha actual 2011-04-27 00:00:00\n",
      "Fecha actual 2011-04-28 00:00:00\n",
      "Fecha actual 2011-04-29 00:00:00\n",
      "Fecha actual 2011-05-02 00:00:00\n",
      "Fecha actual 2011-05-03 00:00:00\n",
      "Fecha actual 2011-05-04 00:00:00\n",
      "Fecha actual 2011-05-05 00:00:00\n",
      "Fecha actual 2011-05-06 00:00:00\n",
      "Fecha actual 2011-05-09 00:00:00\n",
      "Fecha actual 2011-05-10 00:00:00\n",
      "Fecha actual 2011-05-11 00:00:00\n",
      "Fecha actual 2011-05-12 00:00:00\n",
      "Fecha actual 2011-05-13 00:00:00\n",
      "Fecha actual 2011-05-16 00:00:00\n",
      "Fecha actual 2011-05-17 00:00:00\n",
      "Fecha actual 2011-05-18 00:00:00\n",
      "Fecha actual 2011-05-19 00:00:00\n",
      "Fecha actual 2011-05-20 00:00:00\n",
      "Fecha actual 2011-05-23 00:00:00\n",
      "Fecha actual 2011-05-24 00:00:00\n",
      "Fecha actual 2011-05-25 00:00:00\n",
      "Fecha actual 2011-05-26 00:00:00\n",
      "Fecha actual 2011-05-27 00:00:00\n",
      "Fecha actual 2011-05-31 00:00:00\n",
      "Fecha actual 2011-06-01 00:00:00\n",
      "Fecha actual 2011-06-02 00:00:00\n",
      "Fecha actual 2011-06-03 00:00:00\n",
      "Fecha actual 2011-06-06 00:00:00\n",
      "Fecha actual 2011-06-07 00:00:00\n",
      "Fecha actual 2011-06-08 00:00:00\n",
      "Fecha actual 2011-06-09 00:00:00\n",
      "Fecha actual 2011-06-10 00:00:00\n",
      "Fecha actual 2011-06-13 00:00:00\n",
      "Fecha actual 2011-06-14 00:00:00\n",
      "Fecha actual 2011-06-15 00:00:00\n",
      "Fecha actual 2011-06-16 00:00:00\n",
      "Fecha actual 2011-06-17 00:00:00\n",
      "Fecha actual 2011-06-20 00:00:00\n",
      "Fecha actual 2011-06-21 00:00:00\n",
      "Fecha actual 2011-06-22 00:00:00\n",
      "Fecha actual 2011-06-23 00:00:00\n",
      "Fecha actual 2011-06-24 00:00:00\n",
      "Fecha actual 2011-06-27 00:00:00\n",
      "Fecha actual 2011-06-28 00:00:00\n",
      "Fecha actual 2011-06-29 00:00:00\n",
      "Fecha actual 2011-06-30 00:00:00\n",
      "Fecha actual 2011-07-01 00:00:00\n",
      "Fecha actual 2011-07-05 00:00:00\n",
      "Fecha actual 2011-07-06 00:00:00\n",
      "Fecha actual 2011-07-07 00:00:00\n",
      "Fecha actual 2011-07-08 00:00:00\n",
      "Fecha actual 2011-07-11 00:00:00\n",
      "Fecha actual 2011-07-12 00:00:00\n",
      "Fecha actual 2011-07-13 00:00:00\n",
      "Fecha actual 2011-07-14 00:00:00\n",
      "Fecha actual 2011-07-15 00:00:00\n",
      "Fecha actual 2011-07-18 00:00:00\n",
      "Fecha actual 2011-07-19 00:00:00\n",
      "Fecha actual 2011-07-20 00:00:00\n",
      "Fecha actual 2011-07-21 00:00:00\n",
      "Fecha actual 2011-07-22 00:00:00\n",
      "Fecha actual 2011-07-25 00:00:00\n",
      "Fecha actual 2011-07-26 00:00:00\n",
      "Fecha actual 2011-07-27 00:00:00\n",
      "Fecha actual 2011-07-28 00:00:00\n",
      "Fecha actual 2011-07-29 00:00:00\n",
      "Fecha actual 2011-08-01 00:00:00\n",
      "Fecha actual 2011-08-02 00:00:00\n",
      "Fecha actual 2011-08-03 00:00:00\n",
      "Fecha actual 2011-08-04 00:00:00\n",
      "Fecha actual 2011-08-05 00:00:00\n",
      "Fecha actual 2011-08-08 00:00:00\n",
      "Fecha actual 2011-08-09 00:00:00\n",
      "Fecha actual 2011-08-10 00:00:00\n",
      "Fecha actual 2011-08-11 00:00:00\n",
      "Fecha actual 2011-08-12 00:00:00\n",
      "Fecha actual 2011-08-15 00:00:00\n",
      "Fecha actual 2011-08-16 00:00:00\n",
      "Fecha actual 2011-08-17 00:00:00\n",
      "Fecha actual 2011-08-18 00:00:00\n",
      "Fecha actual 2011-08-19 00:00:00\n",
      "Fecha actual 2011-08-22 00:00:00\n",
      "Fecha actual 2011-08-23 00:00:00\n",
      "Fecha actual 2011-08-24 00:00:00\n",
      "Fecha actual 2011-08-25 00:00:00\n",
      "Fecha actual 2011-08-26 00:00:00\n",
      "Fecha actual 2011-08-29 00:00:00\n",
      "Fecha actual 2011-08-30 00:00:00\n",
      "Fecha actual 2011-08-31 00:00:00\n",
      "Fecha actual 2011-09-01 00:00:00\n",
      "Fecha actual 2011-09-02 00:00:00\n",
      "Fecha actual 2011-09-06 00:00:00\n",
      "Fecha actual 2011-09-07 00:00:00\n",
      "Fecha actual 2011-09-08 00:00:00\n",
      "Fecha actual 2011-09-09 00:00:00\n",
      "Fecha actual 2011-09-12 00:00:00\n",
      "Fecha actual 2011-09-13 00:00:00\n",
      "Fecha actual 2011-09-14 00:00:00\n",
      "Fecha actual 2011-09-15 00:00:00\n",
      "Fecha actual 2011-09-16 00:00:00\n",
      "Fecha actual 2011-09-19 00:00:00\n",
      "Fecha actual 2011-09-20 00:00:00\n",
      "Fecha actual 2011-09-21 00:00:00\n",
      "Fecha actual 2011-09-22 00:00:00\n",
      "Fecha actual 2011-09-23 00:00:00\n",
      "Fecha actual 2011-09-26 00:00:00\n",
      "Fecha actual 2011-09-27 00:00:00\n",
      "Fecha actual 2011-09-28 00:00:00\n",
      "Fecha actual 2011-09-29 00:00:00\n",
      "Fecha actual 2011-09-30 00:00:00\n",
      "Fecha actual 2011-10-03 00:00:00\n",
      "Fecha actual 2011-10-04 00:00:00\n",
      "Fecha actual 2011-10-05 00:00:00\n",
      "Fecha actual 2011-10-06 00:00:00\n",
      "Fecha actual 2011-10-07 00:00:00\n",
      "Fecha actual 2011-10-10 00:00:00\n",
      "Fecha actual 2011-10-11 00:00:00\n",
      "Fecha actual 2011-10-12 00:00:00\n",
      "Fecha actual 2011-10-13 00:00:00\n",
      "Fecha actual 2011-10-14 00:00:00\n",
      "Fecha actual 2011-10-17 00:00:00\n",
      "Fecha actual 2011-10-18 00:00:00\n",
      "Fecha actual 2011-10-19 00:00:00\n",
      "Fecha actual 2011-10-20 00:00:00\n",
      "Fecha actual 2011-10-21 00:00:00\n",
      "Fecha actual 2011-10-24 00:00:00\n",
      "Fecha actual 2011-10-25 00:00:00\n",
      "Fecha actual 2011-10-26 00:00:00\n",
      "Fecha actual 2011-10-27 00:00:00\n",
      "Fecha actual 2011-10-28 00:00:00\n",
      "Fecha actual 2011-10-31 00:00:00\n",
      "Fecha actual 2011-11-01 00:00:00\n",
      "Fecha actual 2011-11-02 00:00:00\n",
      "Fecha actual 2011-11-03 00:00:00\n",
      "Fecha actual 2011-11-04 00:00:00\n",
      "Fecha actual 2011-11-07 00:00:00\n",
      "Fecha actual 2011-11-08 00:00:00\n",
      "Fecha actual 2011-11-09 00:00:00\n",
      "Fecha actual 2011-11-10 00:00:00\n",
      "Fecha actual 2011-11-11 00:00:00\n",
      "Fecha actual 2011-11-14 00:00:00\n",
      "Fecha actual 2011-11-15 00:00:00\n",
      "Fecha actual 2011-11-16 00:00:00\n",
      "Fecha actual 2011-11-17 00:00:00\n",
      "Fecha actual 2011-11-18 00:00:00\n",
      "Fecha actual 2011-11-21 00:00:00\n",
      "Fecha actual 2011-11-22 00:00:00\n",
      "Fecha actual 2011-11-23 00:00:00\n",
      "Fecha actual 2011-11-25 00:00:00\n",
      "Fecha actual 2011-11-28 00:00:00\n",
      "Fecha actual 2011-11-29 00:00:00\n",
      "Fecha actual 2011-11-30 00:00:00\n",
      "Fecha actual 2011-12-01 00:00:00\n",
      "Fecha actual 2011-12-02 00:00:00\n",
      "Fecha actual 2011-12-05 00:00:00\n",
      "Fecha actual 2011-12-06 00:00:00\n",
      "Fecha actual 2011-12-07 00:00:00\n",
      "Fecha actual 2011-12-08 00:00:00\n",
      "Fecha actual 2011-12-09 00:00:00\n",
      "Fecha actual 2011-12-12 00:00:00\n",
      "Fecha actual 2011-12-13 00:00:00\n",
      "Fecha actual 2011-12-14 00:00:00\n",
      "Fecha actual 2011-12-15 00:00:00\n",
      "Fecha actual 2011-12-16 00:00:00\n",
      "Fecha actual 2011-12-19 00:00:00\n",
      "Fecha actual 2011-12-20 00:00:00\n",
      "Fecha actual 2011-12-21 00:00:00\n",
      "Fecha actual 2011-12-22 00:00:00\n",
      "Fecha actual 2011-12-23 00:00:00\n",
      "Fecha actual 2011-12-27 00:00:00\n",
      "Fecha actual 2011-12-28 00:00:00\n",
      "Fecha actual 2011-12-29 00:00:00\n",
      "Fecha actual 2011-12-30 00:00:00\n",
      "Fecha actual 2012-01-03 00:00:00\n",
      "Fecha actual 2012-01-04 00:00:00\n",
      "Fecha actual 2012-01-05 00:00:00\n",
      "Fecha actual 2012-01-06 00:00:00\n",
      "Fecha actual 2012-01-09 00:00:00\n",
      "Fecha actual 2012-01-10 00:00:00\n",
      "Fecha actual 2012-01-11 00:00:00\n",
      "Fecha actual 2012-01-12 00:00:00\n",
      "Fecha actual 2012-01-13 00:00:00\n",
      "Fecha actual 2012-01-17 00:00:00\n",
      "Fecha actual 2012-01-18 00:00:00\n",
      "Fecha actual 2012-01-19 00:00:00\n",
      "Fecha actual 2012-01-20 00:00:00\n",
      "Fecha actual 2012-01-23 00:00:00\n",
      "Fecha actual 2012-01-24 00:00:00\n",
      "Fecha actual 2012-01-25 00:00:00\n",
      "Fecha actual 2012-01-26 00:00:00\n",
      "Fecha actual 2012-01-27 00:00:00\n",
      "Fecha actual 2012-01-30 00:00:00\n",
      "Fecha actual 2012-01-31 00:00:00\n",
      "Fecha actual 2012-02-01 00:00:00\n",
      "Fecha actual 2012-02-02 00:00:00\n",
      "Fecha actual 2012-02-03 00:00:00\n",
      "Fecha actual 2012-02-06 00:00:00\n",
      "Fecha actual 2012-02-07 00:00:00\n",
      "Fecha actual 2012-02-08 00:00:00\n",
      "Fecha actual 2012-02-09 00:00:00\n",
      "Fecha actual 2012-02-10 00:00:00\n",
      "Fecha actual 2012-02-13 00:00:00\n",
      "Fecha actual 2012-02-14 00:00:00\n",
      "Fecha actual 2012-02-15 00:00:00\n",
      "Fecha actual 2012-02-16 00:00:00\n",
      "Fecha actual 2012-02-17 00:00:00\n",
      "Fecha actual 2012-02-21 00:00:00\n",
      "Fecha actual 2012-02-22 00:00:00\n",
      "Fecha actual 2012-02-23 00:00:00\n",
      "Fecha actual 2012-02-24 00:00:00\n",
      "Fecha actual 2012-02-27 00:00:00\n",
      "Fecha actual 2012-02-28 00:00:00\n",
      "Fecha actual 2012-02-29 00:00:00\n",
      "Fecha actual 2012-03-01 00:00:00\n",
      "Fecha actual 2012-03-02 00:00:00\n",
      "Fecha actual 2012-03-05 00:00:00\n",
      "Fecha actual 2012-03-06 00:00:00\n",
      "Fecha actual 2012-03-07 00:00:00\n",
      "Fecha actual 2012-03-08 00:00:00\n",
      "Fecha actual 2012-03-09 00:00:00\n",
      "Fecha actual 2012-03-12 00:00:00\n",
      "Fecha actual 2012-03-13 00:00:00\n",
      "Fecha actual 2012-03-14 00:00:00\n",
      "Fecha actual 2012-03-15 00:00:00\n",
      "Fecha actual 2012-03-16 00:00:00\n",
      "Fecha actual 2012-03-19 00:00:00\n",
      "Fecha actual 2012-03-20 00:00:00\n",
      "Fecha actual 2012-03-21 00:00:00\n",
      "Fecha actual 2012-03-22 00:00:00\n",
      "Fecha actual 2012-03-23 00:00:00\n",
      "Fecha actual 2012-03-26 00:00:00\n",
      "Fecha actual 2012-03-27 00:00:00\n",
      "Fecha actual 2012-03-28 00:00:00\n",
      "Fecha actual 2012-03-29 00:00:00\n",
      "Fecha actual 2012-03-30 00:00:00\n",
      "Fecha actual 2012-04-02 00:00:00\n",
      "Fecha actual 2012-04-03 00:00:00\n",
      "Fecha actual 2012-04-04 00:00:00\n",
      "Fecha actual 2012-04-05 00:00:00\n",
      "Fecha actual 2012-04-09 00:00:00\n",
      "Fecha actual 2012-04-10 00:00:00\n",
      "Fecha actual 2012-04-11 00:00:00\n",
      "Fecha actual 2012-04-12 00:00:00\n",
      "Fecha actual 2012-04-13 00:00:00\n",
      "Fecha actual 2012-04-16 00:00:00\n",
      "Fecha actual 2012-04-17 00:00:00\n",
      "Fecha actual 2012-04-18 00:00:00\n",
      "Fecha actual 2012-04-19 00:00:00\n",
      "Fecha actual 2012-04-20 00:00:00\n",
      "Fecha actual 2012-04-23 00:00:00\n",
      "Fecha actual 2012-04-24 00:00:00\n",
      "Fecha actual 2012-04-25 00:00:00\n",
      "Fecha actual 2012-04-26 00:00:00\n",
      "Fecha actual 2012-04-27 00:00:00\n",
      "Fecha actual 2012-04-30 00:00:00\n",
      "Fecha actual 2012-05-01 00:00:00\n",
      "Fecha actual 2012-05-02 00:00:00\n",
      "Fecha actual 2012-05-03 00:00:00\n",
      "Fecha actual 2012-05-04 00:00:00\n",
      "Fecha actual 2012-05-07 00:00:00\n",
      "Fecha actual 2012-05-08 00:00:00\n",
      "Fecha actual 2012-05-09 00:00:00\n",
      "Fecha actual 2012-05-10 00:00:00\n",
      "Fecha actual 2012-05-11 00:00:00\n",
      "Fecha actual 2012-05-14 00:00:00\n",
      "Fecha actual 2012-05-15 00:00:00\n",
      "Fecha actual 2012-05-16 00:00:00\n",
      "Fecha actual 2012-05-17 00:00:00\n",
      "Fecha actual 2012-05-18 00:00:00\n",
      "Fecha actual 2012-05-21 00:00:00\n",
      "Fecha actual 2012-05-22 00:00:00\n",
      "Fecha actual 2012-05-23 00:00:00\n",
      "Fecha actual 2012-05-24 00:00:00\n",
      "Fecha actual 2012-05-25 00:00:00\n",
      "Fecha actual 2012-05-29 00:00:00\n",
      "Fecha actual 2012-05-30 00:00:00\n",
      "Fecha actual 2012-05-31 00:00:00\n",
      "Fecha actual 2012-06-01 00:00:00\n",
      "Fecha actual 2012-06-04 00:00:00\n",
      "Fecha actual 2012-06-05 00:00:00\n",
      "Fecha actual 2012-06-06 00:00:00\n",
      "Fecha actual 2012-06-07 00:00:00\n",
      "Fecha actual 2012-06-08 00:00:00\n",
      "Fecha actual 2012-06-11 00:00:00\n",
      "Fecha actual 2012-06-12 00:00:00\n",
      "Fecha actual 2012-06-13 00:00:00\n",
      "Fecha actual 2012-06-14 00:00:00\n",
      "Fecha actual 2012-06-15 00:00:00\n",
      "Fecha actual 2012-06-18 00:00:00\n",
      "Fecha actual 2012-06-19 00:00:00\n",
      "Fecha actual 2012-06-20 00:00:00\n",
      "Fecha actual 2012-06-21 00:00:00\n",
      "Fecha actual 2012-06-22 00:00:00\n",
      "Fecha actual 2012-06-25 00:00:00\n",
      "Fecha actual 2012-06-26 00:00:00\n",
      "Fecha actual 2012-06-27 00:00:00\n",
      "Fecha actual 2012-06-28 00:00:00\n",
      "Fecha actual 2012-06-29 00:00:00\n",
      "Fecha actual 2012-07-02 00:00:00\n",
      "Fecha actual 2012-07-03 00:00:00\n",
      "Fecha actual 2012-07-05 00:00:00\n",
      "Fecha actual 2012-07-06 00:00:00\n",
      "Fecha actual 2012-07-09 00:00:00\n",
      "Fecha actual 2012-07-10 00:00:00\n",
      "Fecha actual 2012-07-11 00:00:00\n",
      "Fecha actual 2012-07-12 00:00:00\n",
      "Fecha actual 2012-07-13 00:00:00\n",
      "Fecha actual 2012-07-16 00:00:00\n",
      "Fecha actual 2012-07-17 00:00:00\n",
      "Fecha actual 2012-07-18 00:00:00\n",
      "Fecha actual 2012-07-19 00:00:00\n",
      "Fecha actual 2012-07-20 00:00:00\n",
      "Fecha actual 2012-07-23 00:00:00\n",
      "Fecha actual 2012-07-24 00:00:00\n",
      "Fecha actual 2012-07-25 00:00:00\n",
      "Fecha actual 2012-07-26 00:00:00\n",
      "Fecha actual 2012-07-27 00:00:00\n",
      "Fecha actual 2012-07-30 00:00:00\n",
      "Fecha actual 2012-07-31 00:00:00\n",
      "Fecha actual 2012-08-01 00:00:00\n",
      "Fecha actual 2012-08-02 00:00:00\n",
      "Fecha actual 2012-08-03 00:00:00\n",
      "Fecha actual 2012-08-06 00:00:00\n",
      "Fecha actual 2012-08-07 00:00:00\n",
      "Fecha actual 2012-08-08 00:00:00\n",
      "Fecha actual 2012-08-09 00:00:00\n",
      "Fecha actual 2012-08-10 00:00:00\n",
      "Fecha actual 2012-08-13 00:00:00\n",
      "Fecha actual 2012-08-14 00:00:00\n",
      "Fecha actual 2012-08-15 00:00:00\n",
      "Fecha actual 2012-08-16 00:00:00\n",
      "Fecha actual 2012-08-17 00:00:00\n",
      "Fecha actual 2012-08-20 00:00:00\n",
      "Fecha actual 2012-08-21 00:00:00\n",
      "Fecha actual 2012-08-22 00:00:00\n",
      "Fecha actual 2012-08-23 00:00:00\n",
      "Fecha actual 2012-08-24 00:00:00\n",
      "Fecha actual 2012-08-27 00:00:00\n",
      "Fecha actual 2012-08-28 00:00:00\n",
      "Fecha actual 2012-08-29 00:00:00\n",
      "Fecha actual 2012-08-30 00:00:00\n",
      "Fecha actual 2012-08-31 00:00:00\n",
      "Fecha actual 2012-09-04 00:00:00\n",
      "Fecha actual 2012-09-05 00:00:00\n",
      "Fecha actual 2012-09-06 00:00:00\n",
      "Fecha actual 2012-09-07 00:00:00\n",
      "Fecha actual 2012-09-10 00:00:00\n",
      "Fecha actual 2012-09-11 00:00:00\n",
      "Fecha actual 2012-09-12 00:00:00\n",
      "Fecha actual 2012-09-13 00:00:00\n",
      "Fecha actual 2012-09-14 00:00:00\n",
      "Fecha actual 2012-09-17 00:00:00\n",
      "Fecha actual 2012-09-18 00:00:00\n",
      "Fecha actual 2012-09-19 00:00:00\n",
      "Fecha actual 2012-09-20 00:00:00\n",
      "Fecha actual 2012-09-21 00:00:00\n",
      "Fecha actual 2012-09-24 00:00:00\n",
      "Fecha actual 2012-09-25 00:00:00\n",
      "Fecha actual 2012-09-26 00:00:00\n",
      "Fecha actual 2012-09-27 00:00:00\n",
      "Fecha actual 2012-09-28 00:00:00\n",
      "Fecha actual 2012-10-01 00:00:00\n",
      "Fecha actual 2012-10-02 00:00:00\n",
      "Fecha actual 2012-10-03 00:00:00\n",
      "Fecha actual 2012-10-04 00:00:00\n",
      "Fecha actual 2012-10-05 00:00:00\n",
      "Fecha actual 2012-10-08 00:00:00\n",
      "Fecha actual 2012-10-09 00:00:00\n",
      "Fecha actual 2012-10-10 00:00:00\n",
      "Fecha actual 2012-10-11 00:00:00\n",
      "Fecha actual 2012-10-12 00:00:00\n",
      "Fecha actual 2012-10-15 00:00:00\n",
      "Fecha actual 2012-10-16 00:00:00\n",
      "Fecha actual 2012-10-17 00:00:00\n",
      "Fecha actual 2012-10-18 00:00:00\n",
      "Fecha actual 2012-10-19 00:00:00\n",
      "Fecha actual 2012-10-22 00:00:00\n",
      "Fecha actual 2012-10-23 00:00:00\n",
      "Fecha actual 2012-10-24 00:00:00\n",
      "Fecha actual 2012-10-25 00:00:00\n",
      "Fecha actual 2012-10-26 00:00:00\n",
      "Fecha actual 2012-10-31 00:00:00\n",
      "Fecha actual 2012-11-01 00:00:00\n",
      "Fecha actual 2012-11-02 00:00:00\n",
      "Fecha actual 2012-11-05 00:00:00\n",
      "Fecha actual 2012-11-06 00:00:00\n",
      "Fecha actual 2012-11-07 00:00:00\n",
      "Fecha actual 2012-11-08 00:00:00\n",
      "Fecha actual 2012-11-09 00:00:00\n",
      "Fecha actual 2012-11-12 00:00:00\n",
      "Fecha actual 2012-11-13 00:00:00\n",
      "Fecha actual 2012-11-14 00:00:00\n",
      "Fecha actual 2012-11-15 00:00:00\n",
      "Fecha actual 2012-11-16 00:00:00\n",
      "Fecha actual 2012-11-19 00:00:00\n",
      "Fecha actual 2012-11-20 00:00:00\n",
      "Fecha actual 2012-11-21 00:00:00\n",
      "Fecha actual 2012-11-23 00:00:00\n",
      "Fecha actual 2012-11-26 00:00:00\n",
      "Fecha actual 2012-11-27 00:00:00\n",
      "Fecha actual 2012-11-28 00:00:00\n",
      "Fecha actual 2012-11-29 00:00:00\n",
      "Fecha actual 2012-11-30 00:00:00\n",
      "Fecha actual 2012-12-03 00:00:00\n",
      "Fecha actual 2012-12-04 00:00:00\n",
      "Fecha actual 2012-12-05 00:00:00\n",
      "Fecha actual 2012-12-06 00:00:00\n",
      "Fecha actual 2012-12-07 00:00:00\n",
      "Fecha actual 2012-12-10 00:00:00\n",
      "Fecha actual 2012-12-11 00:00:00\n",
      "Fecha actual 2012-12-12 00:00:00\n",
      "Fecha actual 2012-12-13 00:00:00\n",
      "Fecha actual 2012-12-14 00:00:00\n",
      "Fecha actual 2012-12-17 00:00:00\n",
      "Fecha actual 2012-12-18 00:00:00\n",
      "Fecha actual 2012-12-19 00:00:00\n",
      "Fecha actual 2012-12-20 00:00:00\n",
      "Fecha actual 2012-12-21 00:00:00\n",
      "Fecha actual 2012-12-24 00:00:00\n",
      "Fecha actual 2012-12-26 00:00:00\n",
      "Fecha actual 2012-12-27 00:00:00\n",
      "Fecha actual 2012-12-28 00:00:00\n",
      "Fecha actual 2012-12-31 00:00:00\n",
      "Fecha actual 2013-01-02 00:00:00\n",
      "Fecha actual 2013-01-03 00:00:00\n",
      "Fecha actual 2013-01-04 00:00:00\n",
      "Fecha actual 2013-01-07 00:00:00\n",
      "Fecha actual 2013-01-08 00:00:00\n",
      "Fecha actual 2013-01-09 00:00:00\n",
      "Fecha actual 2013-01-10 00:00:00\n",
      "Fecha actual 2013-01-11 00:00:00\n",
      "Fecha actual 2013-01-14 00:00:00\n",
      "Fecha actual 2013-01-15 00:00:00\n",
      "Fecha actual 2013-01-16 00:00:00\n",
      "Fecha actual 2013-01-17 00:00:00\n",
      "Fecha actual 2013-01-18 00:00:00\n",
      "Fecha actual 2013-01-22 00:00:00\n",
      "Fecha actual 2013-01-23 00:00:00\n",
      "Fecha actual 2013-01-24 00:00:00\n",
      "Fecha actual 2013-01-25 00:00:00\n",
      "Fecha actual 2013-01-28 00:00:00\n",
      "Fecha actual 2013-01-29 00:00:00\n",
      "Fecha actual 2013-01-30 00:00:00\n",
      "Fecha actual 2013-01-31 00:00:00\n",
      "Fecha actual 2013-02-01 00:00:00\n",
      "Fecha actual 2013-02-04 00:00:00\n",
      "Fecha actual 2013-02-05 00:00:00\n",
      "Fecha actual 2013-02-06 00:00:00\n",
      "Fecha actual 2013-02-07 00:00:00\n",
      "Fecha actual 2013-02-08 00:00:00\n",
      "Fecha actual 2013-02-11 00:00:00\n",
      "Fecha actual 2013-02-12 00:00:00\n",
      "Fecha actual 2013-02-13 00:00:00\n",
      "Fecha actual 2013-02-14 00:00:00\n",
      "Fecha actual 2013-02-15 00:00:00\n",
      "Fecha actual 2013-02-19 00:00:00\n",
      "Fecha actual 2013-02-20 00:00:00\n",
      "Fecha actual 2013-02-21 00:00:00\n",
      "Fecha actual 2013-02-22 00:00:00\n",
      "Fecha actual 2013-02-25 00:00:00\n",
      "Fecha actual 2013-02-26 00:00:00\n",
      "Fecha actual 2013-02-27 00:00:00\n",
      "Fecha actual 2013-02-28 00:00:00\n",
      "Fecha actual 2013-03-01 00:00:00\n",
      "Fecha actual 2013-03-04 00:00:00\n",
      "Fecha actual 2013-03-05 00:00:00\n",
      "Fecha actual 2013-03-06 00:00:00\n",
      "Fecha actual 2013-03-07 00:00:00\n",
      "Fecha actual 2013-03-08 00:00:00\n",
      "Fecha actual 2013-03-11 00:00:00\n",
      "Fecha actual 2013-03-12 00:00:00\n",
      "Fecha actual 2013-03-13 00:00:00\n",
      "Fecha actual 2013-03-14 00:00:00\n",
      "Fecha actual 2013-03-15 00:00:00\n",
      "Fecha actual 2013-03-18 00:00:00\n",
      "Fecha actual 2013-03-19 00:00:00\n",
      "Fecha actual 2013-03-20 00:00:00\n",
      "Fecha actual 2013-03-21 00:00:00\n",
      "Fecha actual 2013-03-22 00:00:00\n",
      "Fecha actual 2013-03-25 00:00:00\n",
      "Fecha actual 2013-03-26 00:00:00\n",
      "Fecha actual 2013-03-27 00:00:00\n",
      "Fecha actual 2013-03-28 00:00:00\n",
      "Fecha actual 2013-04-01 00:00:00\n",
      "Fecha actual 2013-04-02 00:00:00\n",
      "Fecha actual 2013-04-03 00:00:00\n",
      "Fecha actual 2013-04-04 00:00:00\n",
      "Fecha actual 2013-04-05 00:00:00\n",
      "Fecha actual 2013-04-08 00:00:00\n",
      "Fecha actual 2013-04-09 00:00:00\n",
      "Fecha actual 2013-04-10 00:00:00\n",
      "Fecha actual 2013-04-11 00:00:00\n",
      "Fecha actual 2013-04-12 00:00:00\n",
      "Fecha actual 2013-04-15 00:00:00\n",
      "Fecha actual 2013-04-16 00:00:00\n",
      "Fecha actual 2013-04-17 00:00:00\n",
      "Fecha actual 2013-04-18 00:00:00\n",
      "Fecha actual 2013-04-19 00:00:00\n",
      "Fecha actual 2013-04-22 00:00:00\n",
      "Fecha actual 2013-04-23 00:00:00\n",
      "Fecha actual 2013-04-24 00:00:00\n",
      "Fecha actual 2013-04-25 00:00:00\n",
      "Fecha actual 2013-04-26 00:00:00\n",
      "Fecha actual 2013-04-29 00:00:00\n",
      "Fecha actual 2013-04-30 00:00:00\n",
      "Fecha actual 2013-05-01 00:00:00\n",
      "Fecha actual 2013-05-02 00:00:00\n",
      "Fecha actual 2013-05-03 00:00:00\n",
      "Fecha actual 2013-05-06 00:00:00\n",
      "Fecha actual 2013-05-07 00:00:00\n",
      "Fecha actual 2013-05-08 00:00:00\n",
      "Fecha actual 2013-05-09 00:00:00\n",
      "Fecha actual 2013-05-10 00:00:00\n",
      "Fecha actual 2013-05-13 00:00:00\n",
      "Fecha actual 2013-05-14 00:00:00\n",
      "Fecha actual 2013-05-15 00:00:00\n",
      "Fecha actual 2013-05-16 00:00:00\n",
      "Fecha actual 2013-05-17 00:00:00\n",
      "Fecha actual 2013-05-20 00:00:00\n",
      "Fecha actual 2013-05-21 00:00:00\n",
      "Fecha actual 2013-05-22 00:00:00\n",
      "Fecha actual 2013-05-23 00:00:00\n",
      "Fecha actual 2013-05-24 00:00:00\n",
      "Fecha actual 2013-05-28 00:00:00\n",
      "Fecha actual 2013-05-29 00:00:00\n",
      "Fecha actual 2013-05-30 00:00:00\n",
      "Fecha actual 2013-05-31 00:00:00\n",
      "Fecha actual 2013-06-03 00:00:00\n",
      "Fecha actual 2013-06-04 00:00:00\n",
      "Fecha actual 2013-06-05 00:00:00\n",
      "Fecha actual 2013-06-06 00:00:00\n",
      "Fecha actual 2013-06-07 00:00:00\n",
      "Fecha actual 2013-06-10 00:00:00\n",
      "Fecha actual 2013-06-11 00:00:00\n",
      "Fecha actual 2013-06-12 00:00:00\n",
      "Fecha actual 2013-06-13 00:00:00\n",
      "Fecha actual 2013-06-14 00:00:00\n",
      "Fecha actual 2013-06-17 00:00:00\n",
      "Fecha actual 2013-06-18 00:00:00\n",
      "Fecha actual 2013-06-19 00:00:00\n",
      "Fecha actual 2013-06-20 00:00:00\n",
      "Fecha actual 2013-06-21 00:00:00\n",
      "Fecha actual 2013-06-24 00:00:00\n",
      "Fecha actual 2013-06-25 00:00:00\n",
      "Fecha actual 2013-06-26 00:00:00\n",
      "Fecha actual 2013-06-27 00:00:00\n",
      "Fecha actual 2013-06-28 00:00:00\n",
      "Fecha actual 2013-07-01 00:00:00\n",
      "Fecha actual 2013-07-02 00:00:00\n",
      "Fecha actual 2013-07-03 00:00:00\n",
      "Fecha actual 2013-07-05 00:00:00\n",
      "Fecha actual 2013-07-08 00:00:00\n",
      "Fecha actual 2013-07-09 00:00:00\n",
      "Fecha actual 2013-07-10 00:00:00\n",
      "Fecha actual 2013-07-11 00:00:00\n",
      "Fecha actual 2013-07-12 00:00:00\n",
      "Fecha actual 2013-07-15 00:00:00\n",
      "Fecha actual 2013-07-16 00:00:00\n",
      "Fecha actual 2013-07-17 00:00:00\n",
      "Fecha actual 2013-07-18 00:00:00\n",
      "Fecha actual 2013-07-19 00:00:00\n",
      "Fecha actual 2013-07-22 00:00:00\n",
      "Fecha actual 2013-07-23 00:00:00\n",
      "Fecha actual 2013-07-24 00:00:00\n",
      "Fecha actual 2013-07-25 00:00:00\n",
      "Fecha actual 2013-07-26 00:00:00\n",
      "Fecha actual 2013-07-29 00:00:00\n",
      "Fecha actual 2013-07-30 00:00:00\n",
      "Fecha actual 2013-07-31 00:00:00\n",
      "Fecha actual 2013-08-01 00:00:00\n",
      "Fecha actual 2013-08-02 00:00:00\n",
      "Fecha actual 2013-08-05 00:00:00\n",
      "Fecha actual 2013-08-06 00:00:00\n",
      "Fecha actual 2013-08-07 00:00:00\n",
      "Fecha actual 2013-08-08 00:00:00\n",
      "Fecha actual 2013-08-09 00:00:00\n",
      "Fecha actual 2013-08-12 00:00:00\n",
      "Fecha actual 2013-08-13 00:00:00\n",
      "Fecha actual 2013-08-14 00:00:00\n",
      "Fecha actual 2013-08-15 00:00:00\n",
      "Fecha actual 2013-08-16 00:00:00\n",
      "Fecha actual 2013-08-19 00:00:00\n",
      "Fecha actual 2013-08-20 00:00:00\n",
      "Fecha actual 2013-08-21 00:00:00\n",
      "Fecha actual 2013-08-22 00:00:00\n",
      "Fecha actual 2013-08-23 00:00:00\n",
      "Fecha actual 2013-08-26 00:00:00\n",
      "Fecha actual 2013-08-27 00:00:00\n",
      "Fecha actual 2013-08-28 00:00:00\n",
      "Fecha actual 2013-08-29 00:00:00\n",
      "Fecha actual 2013-08-30 00:00:00\n",
      "Fecha actual 2013-09-03 00:00:00\n",
      "Fecha actual 2013-09-04 00:00:00\n",
      "Fecha actual 2013-09-05 00:00:00\n",
      "Fecha actual 2013-09-06 00:00:00\n",
      "Fecha actual 2013-09-09 00:00:00\n",
      "Fecha actual 2013-09-10 00:00:00\n",
      "Fecha actual 2013-09-11 00:00:00\n",
      "Fecha actual 2013-09-12 00:00:00\n",
      "Fecha actual 2013-09-13 00:00:00\n",
      "Fecha actual 2013-09-16 00:00:00\n",
      "Fecha actual 2013-09-17 00:00:00\n",
      "Fecha actual 2013-09-18 00:00:00\n",
      "Fecha actual 2013-09-19 00:00:00\n",
      "Fecha actual 2013-09-20 00:00:00\n",
      "Fecha actual 2013-09-23 00:00:00\n",
      "Fecha actual 2013-09-24 00:00:00\n",
      "Fecha actual 2013-09-25 00:00:00\n",
      "Fecha actual 2013-09-26 00:00:00\n",
      "Fecha actual 2013-09-27 00:00:00\n",
      "Fecha actual 2013-09-30 00:00:00\n",
      "Fecha actual 2013-10-01 00:00:00\n",
      "Fecha actual 2013-10-02 00:00:00\n",
      "Fecha actual 2013-10-03 00:00:00\n",
      "Fecha actual 2013-10-04 00:00:00\n",
      "Fecha actual 2013-10-07 00:00:00\n",
      "Fecha actual 2013-10-08 00:00:00\n",
      "Fecha actual 2013-10-09 00:00:00\n",
      "Fecha actual 2013-10-10 00:00:00\n",
      "Fecha actual 2013-10-11 00:00:00\n",
      "Fecha actual 2013-10-14 00:00:00\n",
      "Fecha actual 2013-10-15 00:00:00\n",
      "Fecha actual 2013-10-16 00:00:00\n",
      "Fecha actual 2013-10-17 00:00:00\n",
      "Fecha actual 2013-10-18 00:00:00\n",
      "Fecha actual 2013-10-21 00:00:00\n",
      "Fecha actual 2013-10-22 00:00:00\n",
      "Fecha actual 2013-10-23 00:00:00\n",
      "Fecha actual 2013-10-24 00:00:00\n",
      "Fecha actual 2013-10-25 00:00:00\n",
      "Fecha actual 2013-10-28 00:00:00\n",
      "Fecha actual 2013-10-29 00:00:00\n",
      "Fecha actual 2013-10-30 00:00:00\n",
      "Fecha actual 2013-10-31 00:00:00\n",
      "Fecha actual 2013-11-01 00:00:00\n",
      "Fecha actual 2013-11-04 00:00:00\n",
      "Fecha actual 2013-11-05 00:00:00\n",
      "Fecha actual 2013-11-06 00:00:00\n",
      "Fecha actual 2013-11-07 00:00:00\n",
      "Fecha actual 2013-11-08 00:00:00\n",
      "Fecha actual 2013-11-11 00:00:00\n",
      "Fecha actual 2013-11-12 00:00:00\n",
      "Fecha actual 2013-11-13 00:00:00\n",
      "Fecha actual 2013-11-14 00:00:00\n",
      "Fecha actual 2013-11-15 00:00:00\n",
      "Fecha actual 2013-11-18 00:00:00\n",
      "Fecha actual 2013-11-19 00:00:00\n",
      "Fecha actual 2013-11-20 00:00:00\n",
      "Fecha actual 2013-11-21 00:00:00\n",
      "Fecha actual 2013-11-22 00:00:00\n",
      "Fecha actual 2013-11-25 00:00:00\n",
      "Fecha actual 2013-11-26 00:00:00\n",
      "Fecha actual 2013-11-27 00:00:00\n",
      "Fecha actual 2013-11-29 00:00:00\n",
      "Fecha actual 2013-12-02 00:00:00\n",
      "Fecha actual 2013-12-03 00:00:00\n",
      "Fecha actual 2013-12-04 00:00:00\n",
      "Fecha actual 2013-12-05 00:00:00\n",
      "Fecha actual 2013-12-06 00:00:00\n",
      "Fecha actual 2013-12-09 00:00:00\n",
      "Fecha actual 2013-12-10 00:00:00\n",
      "Fecha actual 2013-12-11 00:00:00\n",
      "Fecha actual 2013-12-12 00:00:00\n",
      "Fecha actual 2013-12-13 00:00:00\n",
      "Fecha actual 2013-12-16 00:00:00\n",
      "Fecha actual 2013-12-17 00:00:00\n",
      "Fecha actual 2013-12-18 00:00:00\n",
      "Fecha actual 2013-12-19 00:00:00\n",
      "Fecha actual 2013-12-20 00:00:00\n",
      "Fecha actual 2013-12-23 00:00:00\n",
      "Fecha actual 2013-12-24 00:00:00\n",
      "Fecha actual 2013-12-26 00:00:00\n",
      "Fecha actual 2013-12-27 00:00:00\n",
      "Fecha actual 2013-12-30 00:00:00\n",
      "Fecha actual 2013-12-31 00:00:00\n",
      "Fecha actual 2014-01-02 00:00:00\n",
      "Fecha actual 2014-01-03 00:00:00\n",
      "Fecha actual 2014-01-06 00:00:00\n",
      "Fecha actual 2014-01-07 00:00:00\n",
      "Fecha actual 2014-01-08 00:00:00\n",
      "Fecha actual 2014-01-09 00:00:00\n",
      "Fecha actual 2014-01-10 00:00:00\n",
      "Fecha actual 2014-01-13 00:00:00\n",
      "Fecha actual 2014-01-14 00:00:00\n",
      "Fecha actual 2014-01-15 00:00:00\n",
      "Fecha actual 2014-01-16 00:00:00\n",
      "Fecha actual 2014-01-17 00:00:00\n",
      "Fecha actual 2014-01-21 00:00:00\n",
      "Fecha actual 2014-01-22 00:00:00\n",
      "Fecha actual 2014-01-23 00:00:00\n",
      "Fecha actual 2014-01-24 00:00:00\n",
      "Fecha actual 2014-01-27 00:00:00\n",
      "Fecha actual 2014-01-28 00:00:00\n",
      "Fecha actual 2014-01-29 00:00:00\n",
      "Fecha actual 2014-01-30 00:00:00\n",
      "Fecha actual 2014-01-31 00:00:00\n",
      "Fecha actual 2014-02-03 00:00:00\n",
      "Fecha actual 2014-02-04 00:00:00\n",
      "Fecha actual 2014-02-05 00:00:00\n",
      "Fecha actual 2014-02-06 00:00:00\n",
      "Fecha actual 2014-02-07 00:00:00\n",
      "Fecha actual 2014-02-10 00:00:00\n",
      "Fecha actual 2014-02-11 00:00:00\n",
      "Fecha actual 2014-02-12 00:00:00\n",
      "Fecha actual 2014-02-13 00:00:00\n",
      "Fecha actual 2014-02-14 00:00:00\n",
      "Fecha actual 2014-02-18 00:00:00\n",
      "Fecha actual 2014-02-19 00:00:00\n",
      "Fecha actual 2014-02-20 00:00:00\n",
      "Fecha actual 2014-02-21 00:00:00\n",
      "Fecha actual 2014-02-24 00:00:00\n",
      "Fecha actual 2014-02-25 00:00:00\n",
      "Fecha actual 2014-02-26 00:00:00\n",
      "Fecha actual 2014-02-27 00:00:00\n",
      "Fecha actual 2014-02-28 00:00:00\n",
      "Fecha actual 2014-03-03 00:00:00\n",
      "Fecha actual 2014-03-04 00:00:00\n",
      "Fecha actual 2014-03-05 00:00:00\n",
      "Fecha actual 2014-03-06 00:00:00\n",
      "Fecha actual 2014-03-07 00:00:00\n",
      "Fecha actual 2014-03-10 00:00:00\n",
      "Fecha actual 2014-03-11 00:00:00\n",
      "Fecha actual 2014-03-12 00:00:00\n",
      "Fecha actual 2014-03-13 00:00:00\n",
      "Fecha actual 2014-03-14 00:00:00\n",
      "Fecha actual 2014-03-17 00:00:00\n",
      "Fecha actual 2014-03-18 00:00:00\n",
      "Fecha actual 2014-03-19 00:00:00\n",
      "Fecha actual 2014-03-20 00:00:00\n",
      "Fecha actual 2014-03-21 00:00:00\n",
      "Fecha actual 2014-03-24 00:00:00\n",
      "Fecha actual 2014-03-25 00:00:00\n",
      "Fecha actual 2014-03-26 00:00:00\n",
      "Fecha actual 2014-03-27 00:00:00\n",
      "Fecha actual 2014-03-28 00:00:00\n",
      "Fecha actual 2014-03-31 00:00:00\n",
      "Fecha actual 2014-04-01 00:00:00\n",
      "Fecha actual 2014-04-02 00:00:00\n",
      "Fecha actual 2014-04-03 00:00:00\n",
      "Fecha actual 2014-04-04 00:00:00\n",
      "Fecha actual 2014-04-07 00:00:00\n",
      "Fecha actual 2014-04-08 00:00:00\n",
      "Fecha actual 2014-04-09 00:00:00\n",
      "Fecha actual 2014-04-10 00:00:00\n",
      "Fecha actual 2014-04-11 00:00:00\n",
      "Fecha actual 2014-04-14 00:00:00\n",
      "Fecha actual 2014-04-15 00:00:00\n",
      "Fecha actual 2014-04-16 00:00:00\n",
      "Fecha actual 2014-04-17 00:00:00\n",
      "Fecha actual 2014-04-21 00:00:00\n",
      "Fecha actual 2014-04-22 00:00:00\n",
      "Fecha actual 2014-04-23 00:00:00\n",
      "Fecha actual 2014-04-24 00:00:00\n",
      "Fecha actual 2014-04-25 00:00:00\n",
      "Fecha actual 2014-04-28 00:00:00\n",
      "Fecha actual 2014-04-29 00:00:00\n",
      "Fecha actual 2014-04-30 00:00:00\n",
      "Fecha actual 2014-05-01 00:00:00\n",
      "Fecha actual 2014-05-02 00:00:00\n",
      "Fecha actual 2014-05-05 00:00:00\n",
      "Fecha actual 2014-05-06 00:00:00\n",
      "Fecha actual 2014-05-07 00:00:00\n",
      "Fecha actual 2014-05-08 00:00:00\n",
      "Fecha actual 2014-05-09 00:00:00\n",
      "Fecha actual 2014-05-12 00:00:00\n",
      "Fecha actual 2014-05-13 00:00:00\n",
      "Fecha actual 2014-05-14 00:00:00\n",
      "Fecha actual 2014-05-15 00:00:00\n",
      "Fecha actual 2014-05-16 00:00:00\n",
      "Fecha actual 2014-05-19 00:00:00\n",
      "Fecha actual 2014-05-20 00:00:00\n",
      "Fecha actual 2014-05-21 00:00:00\n",
      "Fecha actual 2014-05-22 00:00:00\n",
      "Fecha actual 2014-05-23 00:00:00\n",
      "Fecha actual 2014-05-27 00:00:00\n",
      "Fecha actual 2014-05-28 00:00:00\n",
      "Fecha actual 2014-05-29 00:00:00\n",
      "Fecha actual 2014-05-30 00:00:00\n",
      "Fecha actual 2014-06-02 00:00:00\n",
      "Fecha actual 2014-06-03 00:00:00\n",
      "Fecha actual 2014-06-04 00:00:00\n",
      "Fecha actual 2014-06-05 00:00:00\n",
      "Fecha actual 2014-06-06 00:00:00\n",
      "Fecha actual 2014-06-09 00:00:00\n",
      "Fecha actual 2014-06-10 00:00:00\n",
      "Fecha actual 2014-06-11 00:00:00\n",
      "Fecha actual 2014-06-12 00:00:00\n",
      "Fecha actual 2014-06-13 00:00:00\n",
      "Fecha actual 2014-06-16 00:00:00\n",
      "Fecha actual 2014-06-17 00:00:00\n",
      "Fecha actual 2014-06-18 00:00:00\n",
      "Fecha actual 2014-06-19 00:00:00\n",
      "Fecha actual 2014-06-20 00:00:00\n",
      "Fecha actual 2014-06-23 00:00:00\n",
      "Fecha actual 2014-06-24 00:00:00\n",
      "Fecha actual 2014-06-25 00:00:00\n",
      "Fecha actual 2014-06-26 00:00:00\n",
      "Fecha actual 2014-06-27 00:00:00\n",
      "Fecha actual 2014-06-30 00:00:00\n",
      "Fecha actual 2014-07-01 00:00:00\n",
      "Fecha actual 2014-07-02 00:00:00\n",
      "Fecha actual 2014-07-03 00:00:00\n",
      "Fecha actual 2014-07-07 00:00:00\n",
      "Fecha actual 2014-07-08 00:00:00\n",
      "Fecha actual 2014-07-09 00:00:00\n",
      "Fecha actual 2014-07-10 00:00:00\n",
      "Fecha actual 2014-07-11 00:00:00\n",
      "Fecha actual 2014-07-14 00:00:00\n",
      "Fecha actual 2014-07-15 00:00:00\n",
      "Fecha actual 2014-07-16 00:00:00\n",
      "Fecha actual 2014-07-17 00:00:00\n",
      "Fecha actual 2014-07-18 00:00:00\n",
      "Fecha actual 2014-07-21 00:00:00\n",
      "Fecha actual 2014-07-22 00:00:00\n",
      "Fecha actual 2014-07-23 00:00:00\n",
      "Fecha actual 2014-07-24 00:00:00\n",
      "Fecha actual 2014-07-25 00:00:00\n",
      "Fecha actual 2014-07-28 00:00:00\n",
      "Fecha actual 2014-07-29 00:00:00\n",
      "Fecha actual 2014-07-30 00:00:00\n",
      "Fecha actual 2014-07-31 00:00:00\n",
      "Fecha actual 2014-08-01 00:00:00\n",
      "Fecha actual 2014-08-04 00:00:00\n",
      "Fecha actual 2014-08-05 00:00:00\n",
      "Fecha actual 2014-08-06 00:00:00\n",
      "Fecha actual 2014-08-07 00:00:00\n",
      "Fecha actual 2014-08-08 00:00:00\n",
      "Fecha actual 2014-08-11 00:00:00\n",
      "Fecha actual 2014-08-12 00:00:00\n",
      "Fecha actual 2014-08-13 00:00:00\n",
      "Fecha actual 2014-08-14 00:00:00\n",
      "Fecha actual 2014-08-15 00:00:00\n",
      "Fecha actual 2014-08-18 00:00:00\n",
      "Fecha actual 2014-08-19 00:00:00\n",
      "Fecha actual 2014-08-20 00:00:00\n",
      "Fecha actual 2014-08-21 00:00:00\n",
      "Fecha actual 2014-08-22 00:00:00\n",
      "Fecha actual 2014-08-25 00:00:00\n",
      "Fecha actual 2014-08-26 00:00:00\n",
      "Fecha actual 2014-08-27 00:00:00\n",
      "Fecha actual 2014-08-28 00:00:00\n",
      "Fecha actual 2014-08-29 00:00:00\n",
      "Fecha actual 2014-09-02 00:00:00\n",
      "Fecha actual 2014-09-03 00:00:00\n",
      "Fecha actual 2014-09-04 00:00:00\n",
      "Fecha actual 2014-09-05 00:00:00\n",
      "Fecha actual 2014-09-08 00:00:00\n",
      "Fecha actual 2014-09-09 00:00:00\n",
      "Fecha actual 2014-09-10 00:00:00\n",
      "Fecha actual 2014-09-11 00:00:00\n",
      "Fecha actual 2014-09-12 00:00:00\n",
      "Fecha actual 2014-09-15 00:00:00\n",
      "Fecha actual 2014-09-16 00:00:00\n",
      "Fecha actual 2014-09-17 00:00:00\n",
      "Fecha actual 2014-09-18 00:00:00\n",
      "Fecha actual 2014-09-19 00:00:00\n",
      "Fecha actual 2014-09-22 00:00:00\n",
      "Fecha actual 2014-09-23 00:00:00\n",
      "Fecha actual 2014-09-24 00:00:00\n",
      "Fecha actual 2014-09-25 00:00:00\n",
      "Fecha actual 2014-09-26 00:00:00\n",
      "Fecha actual 2014-09-29 00:00:00\n",
      "Fecha actual 2014-09-30 00:00:00\n",
      "Fecha actual 2014-10-01 00:00:00\n",
      "Fecha actual 2014-10-02 00:00:00\n",
      "Fecha actual 2014-10-03 00:00:00\n",
      "Fecha actual 2014-10-06 00:00:00\n",
      "Fecha actual 2014-10-07 00:00:00\n",
      "Fecha actual 2014-10-08 00:00:00\n",
      "Fecha actual 2014-10-09 00:00:00\n",
      "Fecha actual 2014-10-10 00:00:00\n",
      "Fecha actual 2014-10-13 00:00:00\n",
      "Fecha actual 2014-10-14 00:00:00\n",
      "Fecha actual 2014-10-15 00:00:00\n",
      "Fecha actual 2014-10-16 00:00:00\n",
      "Fecha actual 2014-10-17 00:00:00\n",
      "Fecha actual 2014-10-20 00:00:00\n",
      "Fecha actual 2014-10-21 00:00:00\n",
      "Fecha actual 2014-10-22 00:00:00\n",
      "Fecha actual 2014-10-23 00:00:00\n",
      "Fecha actual 2014-10-24 00:00:00\n",
      "Fecha actual 2014-10-27 00:00:00\n",
      "Fecha actual 2014-10-28 00:00:00\n",
      "Fecha actual 2014-10-29 00:00:00\n",
      "Fecha actual 2014-10-30 00:00:00\n",
      "Fecha actual 2014-10-31 00:00:00\n",
      "Fecha actual 2014-11-03 00:00:00\n",
      "Fecha actual 2014-11-04 00:00:00\n",
      "Fecha actual 2014-11-05 00:00:00\n",
      "Fecha actual 2014-11-06 00:00:00\n",
      "Fecha actual 2014-11-07 00:00:00\n",
      "Fecha actual 2014-11-10 00:00:00\n",
      "Fecha actual 2014-11-11 00:00:00\n",
      "Fecha actual 2014-11-12 00:00:00\n",
      "Fecha actual 2014-11-13 00:00:00\n",
      "Fecha actual 2014-11-14 00:00:00\n",
      "Fecha actual 2014-11-17 00:00:00\n",
      "Fecha actual 2014-11-18 00:00:00\n",
      "Fecha actual 2014-11-19 00:00:00\n",
      "Fecha actual 2014-11-20 00:00:00\n",
      "Fecha actual 2014-11-21 00:00:00\n",
      "Fecha actual 2014-11-24 00:00:00\n",
      "Fecha actual 2014-11-25 00:00:00\n",
      "Fecha actual 2014-11-26 00:00:00\n",
      "Fecha actual 2014-11-28 00:00:00\n",
      "Fecha actual 2014-12-01 00:00:00\n",
      "Fecha actual 2014-12-02 00:00:00\n",
      "Fecha actual 2014-12-03 00:00:00\n",
      "Fecha actual 2014-12-04 00:00:00\n",
      "Fecha actual 2014-12-05 00:00:00\n",
      "Fecha actual 2014-12-08 00:00:00\n",
      "Fecha actual 2014-12-09 00:00:00\n",
      "Fecha actual 2014-12-10 00:00:00\n",
      "Fecha actual 2014-12-11 00:00:00\n",
      "Fecha actual 2014-12-12 00:00:00\n",
      "Fecha actual 2014-12-15 00:00:00\n",
      "Fecha actual 2014-12-16 00:00:00\n",
      "Fecha actual 2014-12-17 00:00:00\n",
      "Fecha actual 2014-12-18 00:00:00\n",
      "Fecha actual 2014-12-19 00:00:00\n",
      "Fecha actual 2014-12-22 00:00:00\n",
      "Fecha actual 2014-12-23 00:00:00\n",
      "Fecha actual 2014-12-24 00:00:00\n",
      "Fecha actual 2014-12-26 00:00:00\n",
      "Fecha actual 2014-12-29 00:00:00\n",
      "Fecha actual 2014-12-30 00:00:00\n",
      "Fecha actual 2014-12-31 00:00:00\n",
      "Fecha actual 2015-01-02 00:00:00\n",
      "Fecha actual 2015-01-05 00:00:00\n",
      "Fecha actual 2015-01-06 00:00:00\n",
      "Fecha actual 2015-01-07 00:00:00\n",
      "Fecha actual 2015-01-08 00:00:00\n",
      "Fecha actual 2015-01-09 00:00:00\n",
      "Fecha actual 2015-01-12 00:00:00\n",
      "Fecha actual 2015-01-13 00:00:00\n",
      "Fecha actual 2015-01-14 00:00:00\n",
      "Fecha actual 2015-01-15 00:00:00\n",
      "Fecha actual 2015-01-16 00:00:00\n",
      "Fecha actual 2015-01-20 00:00:00\n",
      "Fecha actual 2015-01-21 00:00:00\n",
      "Fecha actual 2015-01-22 00:00:00\n",
      "Fecha actual 2015-01-23 00:00:00\n",
      "Fecha actual 2015-01-26 00:00:00\n",
      "Fecha actual 2015-01-27 00:00:00\n",
      "Fecha actual 2015-01-28 00:00:00\n",
      "Fecha actual 2015-01-29 00:00:00\n",
      "Fecha actual 2015-01-30 00:00:00\n",
      "Fecha actual 2015-02-02 00:00:00\n",
      "Fecha actual 2015-02-03 00:00:00\n",
      "Fecha actual 2015-02-04 00:00:00\n",
      "Fecha actual 2015-02-05 00:00:00\n",
      "Fecha actual 2015-02-06 00:00:00\n",
      "Fecha actual 2015-02-09 00:00:00\n",
      "Fecha actual 2015-02-10 00:00:00\n",
      "Fecha actual 2015-02-11 00:00:00\n",
      "Fecha actual 2015-02-12 00:00:00\n",
      "Fecha actual 2015-02-13 00:00:00\n",
      "Fecha actual 2015-02-17 00:00:00\n",
      "Fecha actual 2015-02-18 00:00:00\n",
      "Fecha actual 2015-02-19 00:00:00\n",
      "Fecha actual 2015-02-20 00:00:00\n",
      "Fecha actual 2015-02-23 00:00:00\n",
      "Fecha actual 2015-02-24 00:00:00\n",
      "Fecha actual 2015-02-25 00:00:00\n",
      "Fecha actual 2015-02-26 00:00:00\n",
      "Fecha actual 2015-02-27 00:00:00\n",
      "Fecha actual 2015-03-02 00:00:00\n",
      "Fecha actual 2015-03-03 00:00:00\n",
      "Fecha actual 2015-03-04 00:00:00\n",
      "Fecha actual 2015-03-05 00:00:00\n",
      "Fecha actual 2015-03-06 00:00:00\n",
      "Fecha actual 2015-03-09 00:00:00\n",
      "Fecha actual 2015-03-10 00:00:00\n",
      "Fecha actual 2015-03-11 00:00:00\n",
      "Fecha actual 2015-03-12 00:00:00\n",
      "Fecha actual 2015-03-13 00:00:00\n",
      "Fecha actual 2015-03-16 00:00:00\n",
      "Fecha actual 2015-03-17 00:00:00\n",
      "Fecha actual 2015-03-18 00:00:00\n",
      "Fecha actual 2015-03-19 00:00:00\n",
      "Fecha actual 2015-03-20 00:00:00\n",
      "Fecha actual 2015-03-23 00:00:00\n",
      "Fecha actual 2015-03-24 00:00:00\n",
      "Fecha actual 2015-03-25 00:00:00\n",
      "Fecha actual 2015-03-26 00:00:00\n",
      "Fecha actual 2015-03-27 00:00:00\n",
      "Fecha actual 2015-03-30 00:00:00\n",
      "Fecha actual 2015-03-31 00:00:00\n",
      "Fecha actual 2015-04-01 00:00:00\n",
      "Fecha actual 2015-04-02 00:00:00\n",
      "Fecha actual 2015-04-06 00:00:00\n",
      "Fecha actual 2015-04-07 00:00:00\n",
      "Fecha actual 2015-04-08 00:00:00\n",
      "Fecha actual 2015-04-09 00:00:00\n",
      "Fecha actual 2015-04-10 00:00:00\n",
      "Fecha actual 2015-04-13 00:00:00\n",
      "Fecha actual 2015-04-14 00:00:00\n",
      "Fecha actual 2015-04-15 00:00:00\n",
      "Fecha actual 2015-04-16 00:00:00\n",
      "Fecha actual 2015-04-17 00:00:00\n",
      "Fecha actual 2015-04-20 00:00:00\n",
      "Fecha actual 2015-04-21 00:00:00\n",
      "Fecha actual 2015-04-22 00:00:00\n",
      "Fecha actual 2015-04-23 00:00:00\n",
      "Fecha actual 2015-04-24 00:00:00\n",
      "Fecha actual 2015-04-27 00:00:00\n",
      "Fecha actual 2015-04-28 00:00:00\n",
      "Fecha actual 2015-04-29 00:00:00\n",
      "Fecha actual 2015-04-30 00:00:00\n",
      "Fecha actual 2015-05-01 00:00:00\n",
      "Fecha actual 2015-05-04 00:00:00\n",
      "Fecha actual 2015-05-05 00:00:00\n",
      "Fecha actual 2015-05-06 00:00:00\n",
      "Fecha actual 2015-05-07 00:00:00\n",
      "Fecha actual 2015-05-08 00:00:00\n",
      "Fecha actual 2015-05-11 00:00:00\n",
      "Fecha actual 2015-05-12 00:00:00\n",
      "Fecha actual 2015-05-13 00:00:00\n",
      "Fecha actual 2015-05-14 00:00:00\n",
      "Fecha actual 2015-05-15 00:00:00\n",
      "Fecha actual 2015-05-18 00:00:00\n",
      "Fecha actual 2015-05-19 00:00:00\n",
      "Fecha actual 2015-05-20 00:00:00\n",
      "Fecha actual 2015-05-21 00:00:00\n",
      "Fecha actual 2015-05-22 00:00:00\n",
      "Fecha actual 2015-05-26 00:00:00\n",
      "Fecha actual 2015-05-27 00:00:00\n",
      "Fecha actual 2015-05-28 00:00:00\n",
      "Fecha actual 2015-05-29 00:00:00\n",
      "Fecha actual 2015-06-01 00:00:00\n",
      "Fecha actual 2015-06-02 00:00:00\n",
      "Fecha actual 2015-06-03 00:00:00\n",
      "Fecha actual 2015-06-04 00:00:00\n",
      "Fecha actual 2015-06-05 00:00:00\n",
      "Fecha actual 2015-06-08 00:00:00\n",
      "Fecha actual 2015-06-09 00:00:00\n",
      "Fecha actual 2015-06-10 00:00:00\n",
      "Fecha actual 2015-06-11 00:00:00\n",
      "Fecha actual 2015-06-12 00:00:00\n",
      "Fecha actual 2015-06-15 00:00:00\n",
      "Fecha actual 2015-06-16 00:00:00\n",
      "Fecha actual 2015-06-17 00:00:00\n",
      "Fecha actual 2015-06-18 00:00:00\n",
      "Fecha actual 2015-06-19 00:00:00\n",
      "Fecha actual 2015-06-22 00:00:00\n",
      "Fecha actual 2015-06-23 00:00:00\n",
      "Fecha actual 2015-06-24 00:00:00\n",
      "Fecha actual 2015-06-25 00:00:00\n",
      "Fecha actual 2015-06-26 00:00:00\n",
      "Fecha actual 2015-06-29 00:00:00\n",
      "Fecha actual 2015-06-30 00:00:00\n",
      "Fecha actual 2015-07-01 00:00:00\n",
      "Fecha actual 2015-07-02 00:00:00\n",
      "Fecha actual 2015-07-06 00:00:00\n",
      "Fecha actual 2015-07-07 00:00:00\n",
      "Fecha actual 2015-07-08 00:00:00\n",
      "Fecha actual 2015-07-09 00:00:00\n",
      "Fecha actual 2015-07-10 00:00:00\n",
      "Fecha actual 2015-07-13 00:00:00\n",
      "Fecha actual 2015-07-14 00:00:00\n",
      "Fecha actual 2015-07-15 00:00:00\n",
      "Fecha actual 2015-07-16 00:00:00\n",
      "Fecha actual 2015-07-17 00:00:00\n",
      "Fecha actual 2015-07-20 00:00:00\n",
      "Fecha actual 2015-07-21 00:00:00\n",
      "Fecha actual 2015-07-22 00:00:00\n",
      "Fecha actual 2015-07-23 00:00:00\n",
      "Fecha actual 2015-07-24 00:00:00\n",
      "Fecha actual 2015-07-27 00:00:00\n",
      "Fecha actual 2015-07-28 00:00:00\n",
      "Fecha actual 2015-07-29 00:00:00\n",
      "Fecha actual 2015-07-30 00:00:00\n",
      "Fecha actual 2015-07-31 00:00:00\n",
      "Fecha actual 2015-08-03 00:00:00\n",
      "Fecha actual 2015-08-04 00:00:00\n",
      "Fecha actual 2015-08-05 00:00:00\n",
      "Fecha actual 2015-08-06 00:00:00\n",
      "Fecha actual 2015-08-07 00:00:00\n",
      "Fecha actual 2015-08-10 00:00:00\n",
      "Fecha actual 2015-08-11 00:00:00\n",
      "Fecha actual 2015-08-12 00:00:00\n",
      "Fecha actual 2015-08-13 00:00:00\n",
      "Fecha actual 2015-08-14 00:00:00\n",
      "Fecha actual 2015-08-17 00:00:00\n",
      "Fecha actual 2015-08-18 00:00:00\n",
      "Fecha actual 2015-08-19 00:00:00\n",
      "Fecha actual 2015-08-20 00:00:00\n",
      "Fecha actual 2015-08-21 00:00:00\n",
      "Fecha actual 2015-08-24 00:00:00\n",
      "Fecha actual 2015-08-25 00:00:00\n",
      "Fecha actual 2015-08-26 00:00:00\n",
      "Fecha actual 2015-08-27 00:00:00\n",
      "Fecha actual 2015-08-28 00:00:00\n",
      "Fecha actual 2015-08-31 00:00:00\n",
      "Fecha actual 2015-09-01 00:00:00\n",
      "Fecha actual 2015-09-02 00:00:00\n",
      "Fecha actual 2015-09-03 00:00:00\n",
      "Fecha actual 2015-09-04 00:00:00\n",
      "Fecha actual 2015-09-08 00:00:00\n",
      "Fecha actual 2015-09-09 00:00:00\n",
      "Fecha actual 2015-09-10 00:00:00\n",
      "Fecha actual 2015-09-11 00:00:00\n",
      "Fecha actual 2015-09-14 00:00:00\n",
      "Fecha actual 2015-09-15 00:00:00\n",
      "Fecha actual 2015-09-16 00:00:00\n",
      "Fecha actual 2015-09-17 00:00:00\n",
      "Fecha actual 2015-09-18 00:00:00\n",
      "Fecha actual 2015-09-21 00:00:00\n",
      "Fecha actual 2015-09-22 00:00:00\n",
      "Fecha actual 2015-09-23 00:00:00\n",
      "Fecha actual 2015-09-24 00:00:00\n",
      "Fecha actual 2015-09-25 00:00:00\n",
      "Fecha actual 2015-09-28 00:00:00\n",
      "Fecha actual 2015-09-29 00:00:00\n",
      "Fecha actual 2015-09-30 00:00:00\n",
      "Fecha actual 2015-10-01 00:00:00\n",
      "Fecha actual 2015-10-02 00:00:00\n",
      "Fecha actual 2015-10-05 00:00:00\n",
      "Fecha actual 2015-10-06 00:00:00\n",
      "Fecha actual 2015-10-07 00:00:00\n",
      "Fecha actual 2015-10-08 00:00:00\n",
      "Fecha actual 2015-10-09 00:00:00\n",
      "Fecha actual 2015-10-12 00:00:00\n",
      "Fecha actual 2015-10-13 00:00:00\n",
      "Fecha actual 2015-10-14 00:00:00\n",
      "Fecha actual 2015-10-15 00:00:00\n",
      "Fecha actual 2015-10-16 00:00:00\n",
      "Fecha actual 2015-10-19 00:00:00\n",
      "Fecha actual 2015-10-20 00:00:00\n",
      "Fecha actual 2015-10-21 00:00:00\n",
      "Fecha actual 2015-10-22 00:00:00\n",
      "Fecha actual 2015-10-23 00:00:00\n",
      "Fecha actual 2015-10-26 00:00:00\n",
      "Fecha actual 2015-10-27 00:00:00\n",
      "Fecha actual 2015-10-28 00:00:00\n",
      "Fecha actual 2015-10-29 00:00:00\n",
      "Fecha actual 2015-10-30 00:00:00\n",
      "Fecha actual 2015-11-02 00:00:00\n",
      "Fecha actual 2015-11-03 00:00:00\n",
      "Fecha actual 2015-11-04 00:00:00\n",
      "Fecha actual 2015-11-05 00:00:00\n",
      "Fecha actual 2015-11-06 00:00:00\n",
      "Fecha actual 2015-11-09 00:00:00\n",
      "Fecha actual 2015-11-10 00:00:00\n",
      "Fecha actual 2015-11-11 00:00:00\n",
      "Fecha actual 2015-11-12 00:00:00\n",
      "Fecha actual 2015-11-13 00:00:00\n",
      "Fecha actual 2015-11-16 00:00:00\n",
      "Fecha actual 2015-11-17 00:00:00\n",
      "Fecha actual 2015-11-18 00:00:00\n",
      "Fecha actual 2015-11-19 00:00:00\n",
      "Fecha actual 2015-11-20 00:00:00\n",
      "Fecha actual 2015-11-23 00:00:00\n",
      "Fecha actual 2015-11-24 00:00:00\n",
      "Fecha actual 2015-11-25 00:00:00\n",
      "Fecha actual 2015-11-27 00:00:00\n",
      "Fecha actual 2015-11-30 00:00:00\n",
      "Fecha actual 2015-12-01 00:00:00\n",
      "Fecha actual 2015-12-02 00:00:00\n",
      "Fecha actual 2015-12-03 00:00:00\n",
      "Fecha actual 2015-12-04 00:00:00\n",
      "Fecha actual 2015-12-07 00:00:00\n",
      "Fecha actual 2015-12-08 00:00:00\n",
      "Fecha actual 2015-12-09 00:00:00\n",
      "Fecha actual 2015-12-10 00:00:00\n",
      "Fecha actual 2015-12-11 00:00:00\n",
      "Fecha actual 2015-12-14 00:00:00\n",
      "Fecha actual 2015-12-15 00:00:00\n",
      "Fecha actual 2015-12-16 00:00:00\n",
      "Fecha actual 2015-12-17 00:00:00\n",
      "Fecha actual 2015-12-18 00:00:00\n",
      "Fecha actual 2015-12-21 00:00:00\n",
      "Fecha actual 2015-12-22 00:00:00\n",
      "Fecha actual 2015-12-23 00:00:00\n",
      "Fecha actual 2015-12-24 00:00:00\n",
      "Fecha actual 2015-12-28 00:00:00\n",
      "Fecha actual 2015-12-29 00:00:00\n",
      "Fecha actual 2015-12-30 00:00:00\n",
      "Fecha actual 2015-12-31 00:00:00\n",
      "Fecha actual 2016-01-04 00:00:00\n",
      "Fecha actual 2016-01-05 00:00:00\n",
      "Fecha actual 2016-01-06 00:00:00\n",
      "Fecha actual 2016-01-07 00:00:00\n",
      "Fecha actual 2016-01-08 00:00:00\n",
      "Fecha actual 2016-01-11 00:00:00\n",
      "Fecha actual 2016-01-12 00:00:00\n",
      "Fecha actual 2016-01-13 00:00:00\n",
      "Fecha actual 2016-01-14 00:00:00\n",
      "Fecha actual 2016-01-15 00:00:00\n",
      "Fecha actual 2016-01-19 00:00:00\n",
      "Fecha actual 2016-01-20 00:00:00\n",
      "Fecha actual 2016-01-21 00:00:00\n",
      "Fecha actual 2016-01-22 00:00:00\n",
      "Fecha actual 2016-01-25 00:00:00\n",
      "Fecha actual 2016-01-26 00:00:00\n",
      "Fecha actual 2016-01-27 00:00:00\n",
      "Fecha actual 2016-01-28 00:00:00\n",
      "Fecha actual 2016-01-29 00:00:00\n",
      "Fecha actual 2016-02-01 00:00:00\n",
      "Fecha actual 2016-02-02 00:00:00\n",
      "Fecha actual 2016-02-03 00:00:00\n",
      "Fecha actual 2016-02-04 00:00:00\n",
      "Fecha actual 2016-02-05 00:00:00\n",
      "Fecha actual 2016-02-08 00:00:00\n",
      "Fecha actual 2016-02-09 00:00:00\n",
      "Fecha actual 2016-02-10 00:00:00\n",
      "Fecha actual 2016-02-11 00:00:00\n",
      "Fecha actual 2016-02-12 00:00:00\n",
      "Fecha actual 2016-02-16 00:00:00\n",
      "Fecha actual 2016-02-17 00:00:00\n",
      "Fecha actual 2016-02-18 00:00:00\n",
      "Fecha actual 2016-02-19 00:00:00\n",
      "Fecha actual 2016-02-22 00:00:00\n",
      "Fecha actual 2016-02-23 00:00:00\n",
      "Fecha actual 2016-02-24 00:00:00\n",
      "Fecha actual 2016-02-25 00:00:00\n",
      "Fecha actual 2016-02-26 00:00:00\n",
      "Fecha actual 2016-02-29 00:00:00\n",
      "Fecha actual 2016-03-01 00:00:00\n",
      "Fecha actual 2016-03-02 00:00:00\n",
      "Fecha actual 2016-03-03 00:00:00\n",
      "Fecha actual 2016-03-04 00:00:00\n",
      "Fecha actual 2016-03-07 00:00:00\n",
      "Fecha actual 2016-03-08 00:00:00\n",
      "Fecha actual 2016-03-09 00:00:00\n",
      "Fecha actual 2016-03-10 00:00:00\n",
      "Fecha actual 2016-03-11 00:00:00\n",
      "Fecha actual 2016-03-14 00:00:00\n",
      "Fecha actual 2016-03-15 00:00:00\n",
      "Fecha actual 2016-03-16 00:00:00\n",
      "Fecha actual 2016-03-17 00:00:00\n",
      "Fecha actual 2016-03-18 00:00:00\n",
      "Fecha actual 2016-03-21 00:00:00\n",
      "Fecha actual 2016-03-22 00:00:00\n",
      "Fecha actual 2016-03-23 00:00:00\n",
      "Fecha actual 2016-03-24 00:00:00\n",
      "Fecha actual 2016-03-28 00:00:00\n",
      "Fecha actual 2016-03-29 00:00:00\n",
      "Fecha actual 2016-03-30 00:00:00\n",
      "Fecha actual 2016-03-31 00:00:00\n",
      "Fecha actual 2016-04-01 00:00:00\n",
      "Fecha actual 2016-04-04 00:00:00\n",
      "Fecha actual 2016-04-05 00:00:00\n",
      "Fecha actual 2016-04-06 00:00:00\n",
      "Fecha actual 2016-04-07 00:00:00\n",
      "Fecha actual 2016-04-08 00:00:00\n",
      "Fecha actual 2016-04-11 00:00:00\n",
      "Fecha actual 2016-04-12 00:00:00\n",
      "Fecha actual 2016-04-13 00:00:00\n",
      "Fecha actual 2016-04-14 00:00:00\n",
      "Fecha actual 2016-04-15 00:00:00\n",
      "Fecha actual 2016-04-18 00:00:00\n",
      "Fecha actual 2016-04-19 00:00:00\n",
      "Fecha actual 2016-04-20 00:00:00\n",
      "Fecha actual 2016-04-21 00:00:00\n",
      "Fecha actual 2016-04-22 00:00:00\n",
      "Fecha actual 2016-04-25 00:00:00\n",
      "Fecha actual 2016-04-26 00:00:00\n",
      "Fecha actual 2016-04-27 00:00:00\n",
      "Fecha actual 2016-04-28 00:00:00\n",
      "Fecha actual 2016-04-29 00:00:00\n",
      "Fecha actual 2016-05-02 00:00:00\n",
      "Fecha actual 2016-05-03 00:00:00\n",
      "Fecha actual 2016-05-04 00:00:00\n",
      "Fecha actual 2016-05-05 00:00:00\n",
      "Fecha actual 2016-05-06 00:00:00\n",
      "Fecha actual 2016-05-09 00:00:00\n",
      "Fecha actual 2016-05-10 00:00:00\n",
      "Fecha actual 2016-05-11 00:00:00\n",
      "Fecha actual 2016-05-12 00:00:00\n",
      "Fecha actual 2016-05-13 00:00:00\n",
      "Fecha actual 2016-05-16 00:00:00\n",
      "Fecha actual 2016-05-17 00:00:00\n",
      "Fecha actual 2016-05-18 00:00:00\n",
      "Fecha actual 2016-05-19 00:00:00\n",
      "Fecha actual 2016-05-20 00:00:00\n",
      "Fecha actual 2016-05-23 00:00:00\n",
      "Fecha actual 2016-05-24 00:00:00\n",
      "Fecha actual 2016-05-25 00:00:00\n",
      "Fecha actual 2016-05-26 00:00:00\n",
      "Fecha actual 2016-05-27 00:00:00\n",
      "Fecha actual 2016-05-31 00:00:00\n",
      "Fecha actual 2016-06-01 00:00:00\n",
      "Fecha actual 2016-06-02 00:00:00\n",
      "Fecha actual 2016-06-03 00:00:00\n",
      "Fecha actual 2016-06-06 00:00:00\n",
      "Fecha actual 2016-06-07 00:00:00\n",
      "Fecha actual 2016-06-08 00:00:00\n",
      "Fecha actual 2016-06-09 00:00:00\n",
      "Fecha actual 2016-06-10 00:00:00\n",
      "Fecha actual 2016-06-13 00:00:00\n",
      "Fecha actual 2016-06-14 00:00:00\n",
      "Fecha actual 2016-06-15 00:00:00\n",
      "Fecha actual 2016-06-16 00:00:00\n",
      "Fecha actual 2016-06-17 00:00:00\n",
      "Fecha actual 2016-06-20 00:00:00\n",
      "Fecha actual 2016-06-21 00:00:00\n",
      "Fecha actual 2016-06-22 00:00:00\n",
      "Fecha actual 2016-06-23 00:00:00\n",
      "Fecha actual 2016-06-24 00:00:00\n",
      "Fecha actual 2016-06-27 00:00:00\n",
      "Fecha actual 2016-06-28 00:00:00\n",
      "Fecha actual 2016-06-29 00:00:00\n",
      "Fecha actual 2016-06-30 00:00:00\n",
      "Fecha actual 2016-07-01 00:00:00\n",
      "Fecha actual 2016-07-05 00:00:00\n",
      "Fecha actual 2016-07-06 00:00:00\n",
      "Fecha actual 2016-07-07 00:00:00\n",
      "Fecha actual 2016-07-08 00:00:00\n",
      "Fecha actual 2016-07-11 00:00:00\n",
      "Fecha actual 2016-07-12 00:00:00\n",
      "Fecha actual 2016-07-13 00:00:00\n",
      "Fecha actual 2016-07-14 00:00:00\n",
      "Fecha actual 2016-07-15 00:00:00\n",
      "Fecha actual 2016-07-18 00:00:00\n",
      "Fecha actual 2016-07-19 00:00:00\n",
      "Fecha actual 2016-07-20 00:00:00\n",
      "Fecha actual 2016-07-21 00:00:00\n",
      "Fecha actual 2016-07-22 00:00:00\n",
      "Fecha actual 2016-07-25 00:00:00\n",
      "Fecha actual 2016-07-26 00:00:00\n",
      "Fecha actual 2016-07-27 00:00:00\n",
      "Fecha actual 2016-07-28 00:00:00\n",
      "Fecha actual 2016-07-29 00:00:00\n",
      "Fecha actual 2016-08-01 00:00:00\n",
      "Fecha actual 2016-08-02 00:00:00\n",
      "Fecha actual 2016-08-03 00:00:00\n",
      "Fecha actual 2016-08-04 00:00:00\n",
      "Fecha actual 2016-08-05 00:00:00\n",
      "Fecha actual 2016-08-08 00:00:00\n",
      "Fecha actual 2016-08-09 00:00:00\n",
      "Fecha actual 2016-08-10 00:00:00\n",
      "Fecha actual 2016-08-11 00:00:00\n",
      "Fecha actual 2016-08-12 00:00:00\n",
      "Fecha actual 2016-08-15 00:00:00\n",
      "Fecha actual 2016-08-16 00:00:00\n",
      "Fecha actual 2016-08-17 00:00:00\n",
      "Fecha actual 2016-08-18 00:00:00\n",
      "Fecha actual 2016-08-19 00:00:00\n",
      "Fecha actual 2016-08-22 00:00:00\n",
      "Fecha actual 2016-08-23 00:00:00\n",
      "Fecha actual 2016-08-24 00:00:00\n",
      "Fecha actual 2016-08-25 00:00:00\n",
      "Fecha actual 2016-08-26 00:00:00\n",
      "Fecha actual 2016-08-29 00:00:00\n",
      "Fecha actual 2016-08-30 00:00:00\n",
      "Fecha actual 2016-08-31 00:00:00\n",
      "Fecha actual 2016-09-01 00:00:00\n",
      "Fecha actual 2016-09-02 00:00:00\n",
      "Fecha actual 2016-09-06 00:00:00\n",
      "Fecha actual 2016-09-07 00:00:00\n",
      "Fecha actual 2016-09-08 00:00:00\n",
      "Fecha actual 2016-09-09 00:00:00\n",
      "Fecha actual 2016-09-12 00:00:00\n",
      "Fecha actual 2016-09-13 00:00:00\n",
      "Fecha actual 2016-09-14 00:00:00\n",
      "Fecha actual 2016-09-15 00:00:00\n",
      "Fecha actual 2016-09-16 00:00:00\n",
      "Fecha actual 2016-09-19 00:00:00\n",
      "Fecha actual 2016-09-20 00:00:00\n",
      "Fecha actual 2016-09-21 00:00:00\n",
      "Fecha actual 2016-09-22 00:00:00\n",
      "Fecha actual 2016-09-23 00:00:00\n",
      "Fecha actual 2016-09-26 00:00:00\n",
      "Fecha actual 2016-09-27 00:00:00\n",
      "Fecha actual 2016-09-28 00:00:00\n",
      "Fecha actual 2016-09-29 00:00:00\n",
      "Fecha actual 2016-09-30 00:00:00\n",
      "Fecha actual 2016-10-03 00:00:00\n",
      "Fecha actual 2016-10-04 00:00:00\n",
      "Fecha actual 2016-10-05 00:00:00\n",
      "Fecha actual 2016-10-06 00:00:00\n",
      "Fecha actual 2016-10-07 00:00:00\n",
      "Fecha actual 2016-10-10 00:00:00\n",
      "Fecha actual 2016-10-11 00:00:00\n",
      "Fecha actual 2016-10-12 00:00:00\n",
      "Fecha actual 2016-10-13 00:00:00\n",
      "Fecha actual 2016-10-14 00:00:00\n",
      "Fecha actual 2016-10-17 00:00:00\n",
      "Fecha actual 2016-10-18 00:00:00\n",
      "Fecha actual 2016-10-19 00:00:00\n",
      "Fecha actual 2016-10-20 00:00:00\n",
      "Fecha actual 2016-10-21 00:00:00\n",
      "Fecha actual 2016-10-24 00:00:00\n",
      "Fecha actual 2016-10-25 00:00:00\n",
      "Fecha actual 2016-10-26 00:00:00\n",
      "Fecha actual 2016-10-27 00:00:00\n",
      "Fecha actual 2016-10-28 00:00:00\n",
      "Fecha actual 2016-10-31 00:00:00\n",
      "Fecha actual 2016-11-01 00:00:00\n",
      "Fecha actual 2016-11-02 00:00:00\n",
      "Fecha actual 2016-11-03 00:00:00\n",
      "Fecha actual 2016-11-04 00:00:00\n",
      "Fecha actual 2016-11-07 00:00:00\n",
      "Fecha actual 2016-11-08 00:00:00\n",
      "Fecha actual 2016-11-09 00:00:00\n",
      "Fecha actual 2016-11-10 00:00:00\n",
      "Fecha actual 2016-11-11 00:00:00\n",
      "Fecha actual 2016-11-14 00:00:00\n",
      "Fecha actual 2016-11-15 00:00:00\n",
      "Fecha actual 2016-11-16 00:00:00\n",
      "Fecha actual 2016-11-17 00:00:00\n",
      "Fecha actual 2016-11-18 00:00:00\n",
      "Fecha actual 2016-11-21 00:00:00\n",
      "Fecha actual 2016-11-22 00:00:00\n",
      "Fecha actual 2016-11-23 00:00:00\n",
      "Fecha actual 2016-11-25 00:00:00\n",
      "Fecha actual 2016-11-28 00:00:00\n",
      "Fecha actual 2016-11-29 00:00:00\n",
      "Fecha actual 2016-11-30 00:00:00\n",
      "Fecha actual 2016-12-01 00:00:00\n",
      "Fecha actual 2016-12-02 00:00:00\n",
      "Fecha actual 2016-12-05 00:00:00\n",
      "Fecha actual 2016-12-06 00:00:00\n",
      "Fecha actual 2016-12-07 00:00:00\n",
      "Fecha actual 2016-12-08 00:00:00\n",
      "Fecha actual 2016-12-09 00:00:00\n",
      "Fecha actual 2016-12-12 00:00:00\n",
      "Fecha actual 2016-12-13 00:00:00\n",
      "Fecha actual 2016-12-14 00:00:00\n",
      "Fecha actual 2016-12-15 00:00:00\n",
      "Fecha actual 2016-12-16 00:00:00\n",
      "Fecha actual 2016-12-19 00:00:00\n",
      "Fecha actual 2016-12-20 00:00:00\n",
      "Fecha actual 2016-12-21 00:00:00\n",
      "Fecha actual 2016-12-22 00:00:00\n",
      "Fecha actual 2016-12-23 00:00:00\n",
      "Fecha actual 2016-12-27 00:00:00\n",
      "Fecha actual 2016-12-28 00:00:00\n",
      "Fecha actual 2016-12-29 00:00:00\n",
      "Fecha actual 2016-12-30 00:00:00\n",
      "Fecha actual 2017-01-03 00:00:00\n",
      "Fecha actual 2017-01-04 00:00:00\n",
      "Fecha actual 2017-01-05 00:00:00\n",
      "Fecha actual 2017-01-06 00:00:00\n",
      "Fecha actual 2017-01-09 00:00:00\n",
      "Fecha actual 2017-01-10 00:00:00\n",
      "Fecha actual 2017-01-11 00:00:00\n",
      "Fecha actual 2017-01-12 00:00:00\n",
      "Fecha actual 2017-01-13 00:00:00\n",
      "Fecha actual 2017-01-17 00:00:00\n",
      "Fecha actual 2017-01-18 00:00:00\n",
      "Fecha actual 2017-01-19 00:00:00\n",
      "Fecha actual 2017-01-20 00:00:00\n",
      "Fecha actual 2017-01-23 00:00:00\n",
      "Fecha actual 2017-01-24 00:00:00\n",
      "Fecha actual 2017-01-25 00:00:00\n",
      "Fecha actual 2017-01-26 00:00:00\n",
      "Fecha actual 2017-01-27 00:00:00\n",
      "Fecha actual 2017-01-30 00:00:00\n",
      "Fecha actual 2017-01-31 00:00:00\n",
      "Fecha actual 2017-02-01 00:00:00\n",
      "Fecha actual 2017-02-02 00:00:00\n",
      "Fecha actual 2017-02-03 00:00:00\n",
      "Fecha actual 2017-02-06 00:00:00\n",
      "Fecha actual 2017-02-07 00:00:00\n",
      "Fecha actual 2017-02-08 00:00:00\n",
      "Fecha actual 2017-02-09 00:00:00\n",
      "Fecha actual 2017-02-10 00:00:00\n",
      "Fecha actual 2017-02-13 00:00:00\n",
      "Fecha actual 2017-02-14 00:00:00\n",
      "Fecha actual 2017-02-15 00:00:00\n",
      "Fecha actual 2017-02-16 00:00:00\n",
      "Fecha actual 2017-02-17 00:00:00\n",
      "Fecha actual 2017-02-21 00:00:00\n",
      "Fecha actual 2017-02-22 00:00:00\n",
      "Fecha actual 2017-02-23 00:00:00\n",
      "Fecha actual 2017-02-24 00:00:00\n",
      "Fecha actual 2017-02-27 00:00:00\n",
      "Fecha actual 2017-02-28 00:00:00\n",
      "Fecha actual 2017-03-01 00:00:00\n",
      "Fecha actual 2017-03-02 00:00:00\n",
      "Fecha actual 2017-03-03 00:00:00\n",
      "Fecha actual 2017-03-06 00:00:00\n",
      "Fecha actual 2017-03-07 00:00:00\n",
      "Fecha actual 2017-03-08 00:00:00\n",
      "Fecha actual 2017-03-09 00:00:00\n",
      "Fecha actual 2017-03-10 00:00:00\n",
      "Fecha actual 2017-03-13 00:00:00\n",
      "Fecha actual 2017-03-14 00:00:00\n",
      "Fecha actual 2017-03-15 00:00:00\n",
      "Fecha actual 2017-03-16 00:00:00\n",
      "Fecha actual 2017-03-17 00:00:00\n",
      "Fecha actual 2017-03-20 00:00:00\n",
      "Fecha actual 2017-03-21 00:00:00\n",
      "Fecha actual 2017-03-22 00:00:00\n",
      "Fecha actual 2017-03-23 00:00:00\n",
      "Fecha actual 2017-03-24 00:00:00\n",
      "Fecha actual 2017-03-27 00:00:00\n",
      "Fecha actual 2017-03-28 00:00:00\n",
      "Fecha actual 2017-03-29 00:00:00\n",
      "Fecha actual 2017-03-30 00:00:00\n",
      "Fecha actual 2017-03-31 00:00:00\n",
      "Fecha actual 2017-04-03 00:00:00\n",
      "Fecha actual 2017-04-04 00:00:00\n",
      "Fecha actual 2017-04-05 00:00:00\n",
      "Fecha actual 2017-04-06 00:00:00\n",
      "Fecha actual 2017-04-07 00:00:00\n",
      "Fecha actual 2017-04-10 00:00:00\n",
      "Fecha actual 2017-04-11 00:00:00\n",
      "Fecha actual 2017-04-12 00:00:00\n",
      "Fecha actual 2017-04-13 00:00:00\n",
      "Fecha actual 2017-04-17 00:00:00\n",
      "Fecha actual 2017-04-18 00:00:00\n",
      "Fecha actual 2017-04-19 00:00:00\n",
      "Fecha actual 2017-04-20 00:00:00\n",
      "Fecha actual 2017-04-21 00:00:00\n",
      "Fecha actual 2017-04-24 00:00:00\n",
      "Fecha actual 2017-04-25 00:00:00\n",
      "Fecha actual 2017-04-26 00:00:00\n",
      "Fecha actual 2017-04-27 00:00:00\n",
      "Fecha actual 2017-04-28 00:00:00\n",
      "Fecha actual 2017-05-01 00:00:00\n",
      "Fecha actual 2017-05-02 00:00:00\n",
      "Fecha actual 2017-05-03 00:00:00\n",
      "Fecha actual 2017-05-04 00:00:00\n",
      "Fecha actual 2017-05-05 00:00:00\n",
      "Fecha actual 2017-05-08 00:00:00\n",
      "Fecha actual 2017-05-09 00:00:00\n",
      "Fecha actual 2017-05-10 00:00:00\n",
      "Fecha actual 2017-05-11 00:00:00\n",
      "Fecha actual 2017-05-12 00:00:00\n",
      "Fecha actual 2017-05-15 00:00:00\n",
      "Fecha actual 2017-05-16 00:00:00\n",
      "Fecha actual 2017-05-17 00:00:00\n",
      "Fecha actual 2017-05-18 00:00:00\n",
      "Fecha actual 2017-05-19 00:00:00\n",
      "Fecha actual 2017-05-22 00:00:00\n",
      "Fecha actual 2017-05-23 00:00:00\n",
      "Fecha actual 2017-05-24 00:00:00\n",
      "Fecha actual 2017-05-25 00:00:00\n",
      "Fecha actual 2017-05-26 00:00:00\n",
      "Fecha actual 2017-05-30 00:00:00\n",
      "Fecha actual 2017-05-31 00:00:00\n",
      "Fecha actual 2017-06-01 00:00:00\n",
      "Fecha actual 2017-06-02 00:00:00\n",
      "Fecha actual 2017-06-05 00:00:00\n",
      "Fecha actual 2017-06-06 00:00:00\n",
      "Fecha actual 2017-06-07 00:00:00\n",
      "Fecha actual 2017-06-08 00:00:00\n",
      "Fecha actual 2017-06-09 00:00:00\n",
      "Fecha actual 2017-06-12 00:00:00\n",
      "Fecha actual 2017-06-13 00:00:00\n",
      "Fecha actual 2017-06-14 00:00:00\n",
      "Fecha actual 2017-06-15 00:00:00\n",
      "Fecha actual 2017-06-16 00:00:00\n",
      "Fecha actual 2017-06-19 00:00:00\n",
      "Fecha actual 2017-06-20 00:00:00\n",
      "Fecha actual 2017-06-21 00:00:00\n",
      "Fecha actual 2017-06-22 00:00:00\n",
      "Fecha actual 2017-06-23 00:00:00\n",
      "Fecha actual 2017-06-26 00:00:00\n",
      "Fecha actual 2017-06-27 00:00:00\n",
      "Fecha actual 2017-06-28 00:00:00\n",
      "Fecha actual 2017-06-29 00:00:00\n",
      "Fecha actual 2017-06-30 00:00:00\n",
      "Fecha actual 2017-07-03 00:00:00\n",
      "Fecha actual 2017-07-05 00:00:00\n",
      "Fecha actual 2017-07-06 00:00:00\n",
      "Fecha actual 2017-07-07 00:00:00\n",
      "Fecha actual 2017-07-10 00:00:00\n",
      "Fecha actual 2017-07-11 00:00:00\n",
      "Fecha actual 2017-07-12 00:00:00\n",
      "Fecha actual 2017-07-13 00:00:00\n",
      "Fecha actual 2017-07-14 00:00:00\n",
      "Fecha actual 2017-07-17 00:00:00\n",
      "Fecha actual 2017-07-18 00:00:00\n",
      "Fecha actual 2017-07-19 00:00:00\n",
      "Fecha actual 2017-07-20 00:00:00\n",
      "Fecha actual 2017-07-21 00:00:00\n",
      "Fecha actual 2017-07-24 00:00:00\n",
      "Fecha actual 2017-07-25 00:00:00\n",
      "Fecha actual 2017-07-26 00:00:00\n",
      "Fecha actual 2017-07-27 00:00:00\n",
      "Fecha actual 2017-07-28 00:00:00\n",
      "Fecha actual 2017-07-31 00:00:00\n",
      "Fecha actual 2017-08-01 00:00:00\n",
      "Fecha actual 2017-08-02 00:00:00\n",
      "Fecha actual 2017-08-03 00:00:00\n",
      "Fecha actual 2017-08-04 00:00:00\n",
      "Fecha actual 2017-08-07 00:00:00\n",
      "Fecha actual 2017-08-08 00:00:00\n",
      "Fecha actual 2017-08-09 00:00:00\n",
      "Fecha actual 2017-08-10 00:00:00\n",
      "Fecha actual 2017-08-11 00:00:00\n",
      "Fecha actual 2017-08-14 00:00:00\n",
      "Fecha actual 2017-08-15 00:00:00\n",
      "Fecha actual 2017-08-16 00:00:00\n",
      "Fecha actual 2017-08-17 00:00:00\n",
      "Fecha actual 2017-08-18 00:00:00\n",
      "Fecha actual 2017-08-21 00:00:00\n",
      "Fecha actual 2017-08-22 00:00:00\n",
      "Fecha actual 2017-08-23 00:00:00\n",
      "Fecha actual 2017-08-24 00:00:00\n",
      "Fecha actual 2017-08-25 00:00:00\n",
      "Fecha actual 2017-08-28 00:00:00\n",
      "Fecha actual 2017-08-29 00:00:00\n",
      "Fecha actual 2017-08-30 00:00:00\n",
      "Fecha actual 2017-08-31 00:00:00\n",
      "Fecha actual 2017-09-01 00:00:00\n",
      "Fecha actual 2017-09-05 00:00:00\n",
      "Fecha actual 2017-09-06 00:00:00\n",
      "Fecha actual 2017-09-07 00:00:00\n",
      "Fecha actual 2017-09-08 00:00:00\n",
      "Fecha actual 2017-09-11 00:00:00\n",
      "Fecha actual 2017-09-12 00:00:00\n",
      "Fecha actual 2017-09-13 00:00:00\n",
      "Fecha actual 2017-09-14 00:00:00\n",
      "Fecha actual 2017-09-15 00:00:00\n",
      "Fecha actual 2017-09-18 00:00:00\n",
      "Fecha actual 2017-09-19 00:00:00\n",
      "Fecha actual 2017-09-20 00:00:00\n",
      "Fecha actual 2017-09-21 00:00:00\n",
      "Fecha actual 2017-09-22 00:00:00\n",
      "Fecha actual 2017-09-25 00:00:00\n",
      "Fecha actual 2017-09-26 00:00:00\n",
      "Fecha actual 2017-09-27 00:00:00\n",
      "Fecha actual 2017-09-28 00:00:00\n",
      "Fecha actual 2017-09-29 00:00:00\n",
      "Fecha actual 2017-10-02 00:00:00\n",
      "Fecha actual 2017-10-03 00:00:00\n",
      "Fecha actual 2017-10-04 00:00:00\n",
      "Fecha actual 2017-10-05 00:00:00\n",
      "Fecha actual 2017-10-06 00:00:00\n",
      "Fecha actual 2017-10-09 00:00:00\n",
      "Fecha actual 2017-10-10 00:00:00\n",
      "Fecha actual 2017-10-11 00:00:00\n",
      "Fecha actual 2017-10-12 00:00:00\n",
      "Fecha actual 2017-10-13 00:00:00\n",
      "Fecha actual 2017-10-16 00:00:00\n",
      "Fecha actual 2017-10-17 00:00:00\n",
      "Fecha actual 2017-10-18 00:00:00\n",
      "Fecha actual 2017-10-19 00:00:00\n",
      "Fecha actual 2017-10-20 00:00:00\n",
      "Fecha actual 2017-10-23 00:00:00\n",
      "Fecha actual 2017-10-24 00:00:00\n",
      "Fecha actual 2017-10-25 00:00:00\n",
      "Fecha actual 2017-10-26 00:00:00\n",
      "Fecha actual 2017-10-27 00:00:00\n",
      "Fecha actual 2017-10-30 00:00:00\n",
      "Fecha actual 2017-10-31 00:00:00\n",
      "Fecha actual 2017-11-01 00:00:00\n",
      "Fecha actual 2017-11-02 00:00:00\n",
      "Fecha actual 2017-11-03 00:00:00\n",
      "Fecha actual 2017-11-06 00:00:00\n",
      "Fecha actual 2017-11-07 00:00:00\n",
      "Fecha actual 2017-11-08 00:00:00\n",
      "Fecha actual 2017-11-09 00:00:00\n",
      "Fecha actual 2017-11-10 00:00:00\n",
      "Fecha actual 2017-11-13 00:00:00\n",
      "Fecha actual 2017-11-14 00:00:00\n",
      "Fecha actual 2017-11-15 00:00:00\n",
      "Fecha actual 2017-11-16 00:00:00\n",
      "Fecha actual 2017-11-17 00:00:00\n",
      "Fecha actual 2017-11-20 00:00:00\n",
      "Fecha actual 2017-11-21 00:00:00\n",
      "Fecha actual 2017-11-22 00:00:00\n",
      "Fecha actual 2017-11-24 00:00:00\n",
      "Fecha actual 2017-11-27 00:00:00\n",
      "Fecha actual 2017-11-28 00:00:00\n",
      "Fecha actual 2017-11-29 00:00:00\n",
      "Fecha actual 2017-11-30 00:00:00\n",
      "Fecha actual 2017-12-01 00:00:00\n",
      "Fecha actual 2017-12-04 00:00:00\n",
      "Fecha actual 2017-12-05 00:00:00\n",
      "Fecha actual 2017-12-06 00:00:00\n",
      "Fecha actual 2017-12-07 00:00:00\n",
      "Fecha actual 2017-12-08 00:00:00\n",
      "Fecha actual 2017-12-11 00:00:00\n",
      "Fecha actual 2017-12-12 00:00:00\n",
      "Fecha actual 2017-12-13 00:00:00\n",
      "Fecha actual 2017-12-14 00:00:00\n",
      "Fecha actual 2017-12-15 00:00:00\n",
      "Fecha actual 2017-12-18 00:00:00\n",
      "Fecha actual 2017-12-19 00:00:00\n",
      "Fecha actual 2017-12-20 00:00:00\n",
      "Fecha actual 2017-12-21 00:00:00\n",
      "Fecha actual 2017-12-22 00:00:00\n",
      "Fecha actual 2017-12-26 00:00:00\n",
      "Fecha actual 2017-12-27 00:00:00\n",
      "Fecha actual 2017-12-28 00:00:00\n",
      "Fecha actual 2017-12-29 00:00:00\n",
      "Fecha actual 2018-01-02 00:00:00\n",
      "Fecha actual 2018-01-03 00:00:00\n",
      "Fecha actual 2018-01-04 00:00:00\n",
      "Fecha actual 2018-01-05 00:00:00\n",
      "Fecha actual 2018-01-08 00:00:00\n",
      "Fecha actual 2018-01-09 00:00:00\n",
      "Fecha actual 2018-01-10 00:00:00\n",
      "Fecha actual 2018-01-11 00:00:00\n",
      "Fecha actual 2018-01-12 00:00:00\n",
      "Fecha actual 2018-01-16 00:00:00\n",
      "Fecha actual 2018-01-17 00:00:00\n",
      "Fecha actual 2018-01-18 00:00:00\n",
      "Fecha actual 2018-01-19 00:00:00\n",
      "Fecha actual 2018-01-22 00:00:00\n",
      "Fecha actual 2018-01-23 00:00:00\n",
      "Fecha actual 2018-01-24 00:00:00\n",
      "Fecha actual 2018-01-25 00:00:00\n",
      "Fecha actual 2018-01-26 00:00:00\n",
      "Fecha actual 2018-01-29 00:00:00\n",
      "Fecha actual 2018-01-30 00:00:00\n",
      "Fecha actual 2018-01-31 00:00:00\n",
      "Fecha actual 2018-02-01 00:00:00\n",
      "Fecha actual 2018-02-02 00:00:00\n",
      "Fecha actual 2018-02-05 00:00:00\n",
      "Fecha actual 2018-02-06 00:00:00\n",
      "Fecha actual 2018-02-07 00:00:00\n",
      "Fecha actual 2018-02-08 00:00:00\n",
      "Fecha actual 2018-02-09 00:00:00\n",
      "Fecha actual 2018-02-12 00:00:00\n",
      "Fecha actual 2018-02-13 00:00:00\n",
      "Fecha actual 2018-02-14 00:00:00\n",
      "Fecha actual 2018-02-15 00:00:00\n",
      "Fecha actual 2018-02-16 00:00:00\n",
      "Fecha actual 2018-02-20 00:00:00\n",
      "Fecha actual 2018-02-21 00:00:00\n",
      "Fecha actual 2018-02-22 00:00:00\n",
      "Fecha actual 2018-02-23 00:00:00\n",
      "Fecha actual 2018-02-26 00:00:00\n",
      "Fecha actual 2018-02-27 00:00:00\n",
      "Fecha actual 2018-02-28 00:00:00\n",
      "Fecha actual 2018-03-01 00:00:00\n",
      "Fecha actual 2018-03-02 00:00:00\n",
      "Fecha actual 2018-03-05 00:00:00\n",
      "Fecha actual 2018-03-06 00:00:00\n",
      "Fecha actual 2018-03-07 00:00:00\n",
      "Fecha actual 2018-03-08 00:00:00\n",
      "Fecha actual 2018-03-09 00:00:00\n",
      "Fecha actual 2018-03-12 00:00:00\n",
      "Fecha actual 2018-03-13 00:00:00\n",
      "Fecha actual 2018-03-14 00:00:00\n",
      "Fecha actual 2018-03-15 00:00:00\n",
      "Fecha actual 2018-03-16 00:00:00\n",
      "Fecha actual 2018-03-19 00:00:00\n",
      "Fecha actual 2018-03-20 00:00:00\n",
      "Fecha actual 2018-03-21 00:00:00\n",
      "Fecha actual 2018-03-22 00:00:00\n",
      "Fecha actual 2018-03-23 00:00:00\n",
      "Fecha actual 2018-03-26 00:00:00\n",
      "Fecha actual 2018-03-27 00:00:00\n",
      "Fecha actual 2018-03-28 00:00:00\n",
      "Fecha actual 2018-03-29 00:00:00\n",
      "Fecha actual 2018-04-02 00:00:00\n",
      "Fecha actual 2018-04-03 00:00:00\n",
      "Fecha actual 2018-04-04 00:00:00\n",
      "Fecha actual 2018-04-05 00:00:00\n",
      "Fecha actual 2018-04-06 00:00:00\n",
      "Fecha actual 2018-04-09 00:00:00\n",
      "Fecha actual 2018-04-10 00:00:00\n",
      "Fecha actual 2018-04-11 00:00:00\n",
      "Fecha actual 2018-04-12 00:00:00\n",
      "Fecha actual 2018-04-13 00:00:00\n",
      "Fecha actual 2018-04-16 00:00:00\n",
      "Fecha actual 2018-04-17 00:00:00\n",
      "Fecha actual 2018-04-18 00:00:00\n",
      "Fecha actual 2018-04-19 00:00:00\n",
      "Fecha actual 2018-04-20 00:00:00\n",
      "Fecha actual 2018-04-23 00:00:00\n",
      "Fecha actual 2018-04-24 00:00:00\n",
      "Fecha actual 2018-04-25 00:00:00\n",
      "Fecha actual 2018-04-26 00:00:00\n",
      "Fecha actual 2018-04-27 00:00:00\n",
      "Fecha actual 2018-04-30 00:00:00\n",
      "Fecha actual 2018-05-01 00:00:00\n",
      "Fecha actual 2018-05-02 00:00:00\n",
      "Fecha actual 2018-05-03 00:00:00\n",
      "Fecha actual 2018-05-04 00:00:00\n",
      "Fecha actual 2018-05-07 00:00:00\n",
      "Fecha actual 2018-05-08 00:00:00\n",
      "Fecha actual 2018-05-09 00:00:00\n",
      "Fecha actual 2018-05-10 00:00:00\n",
      "Fecha actual 2018-05-11 00:00:00\n",
      "Fecha actual 2018-05-14 00:00:00\n",
      "Fecha actual 2018-05-15 00:00:00\n",
      "Fecha actual 2018-05-16 00:00:00\n",
      "Fecha actual 2018-05-17 00:00:00\n",
      "Fecha actual 2018-05-18 00:00:00\n",
      "Fecha actual 2018-05-21 00:00:00\n",
      "Fecha actual 2018-05-22 00:00:00\n",
      "Fecha actual 2018-05-23 00:00:00\n",
      "Fecha actual 2018-05-24 00:00:00\n",
      "Fecha actual 2018-05-25 00:00:00\n",
      "Fecha actual 2018-05-29 00:00:00\n",
      "Fecha actual 2018-05-30 00:00:00\n",
      "Fecha actual 2018-05-31 00:00:00\n",
      "Fecha actual 2018-06-01 00:00:00\n",
      "Fecha actual 2018-06-04 00:00:00\n",
      "Fecha actual 2018-06-05 00:00:00\n",
      "Fecha actual 2018-06-06 00:00:00\n",
      "Fecha actual 2018-06-07 00:00:00\n",
      "Fecha actual 2018-06-08 00:00:00\n",
      "Fecha actual 2018-06-11 00:00:00\n",
      "Fecha actual 2018-06-12 00:00:00\n",
      "Fecha actual 2018-06-13 00:00:00\n",
      "Fecha actual 2018-06-14 00:00:00\n",
      "Fecha actual 2018-06-15 00:00:00\n",
      "Fecha actual 2018-06-18 00:00:00\n",
      "Fecha actual 2018-06-19 00:00:00\n",
      "Fecha actual 2018-06-20 00:00:00\n",
      "Fecha actual 2018-06-21 00:00:00\n",
      "Fecha actual 2018-06-22 00:00:00\n",
      "Fecha actual 2018-06-25 00:00:00\n",
      "Fecha actual 2018-06-26 00:00:00\n",
      "Fecha actual 2018-06-27 00:00:00\n",
      "Fecha actual 2018-06-28 00:00:00\n",
      "Fecha actual 2018-06-29 00:00:00\n",
      "Fecha actual 2018-07-02 00:00:00\n",
      "Fecha actual 2018-07-03 00:00:00\n",
      "Fecha actual 2018-07-05 00:00:00\n",
      "Fecha actual 2018-07-06 00:00:00\n",
      "Fecha actual 2018-07-09 00:00:00\n",
      "Fecha actual 2018-07-10 00:00:00\n",
      "Fecha actual 2018-07-11 00:00:00\n",
      "Fecha actual 2018-07-12 00:00:00\n",
      "Fecha actual 2018-07-13 00:00:00\n",
      "Fecha actual 2018-07-16 00:00:00\n",
      "Fecha actual 2018-07-17 00:00:00\n",
      "Fecha actual 2018-07-18 00:00:00\n",
      "Fecha actual 2018-07-19 00:00:00\n",
      "Fecha actual 2018-07-20 00:00:00\n",
      "Fecha actual 2018-07-23 00:00:00\n",
      "Fecha actual 2018-07-24 00:00:00\n",
      "Fecha actual 2018-07-25 00:00:00\n",
      "Fecha actual 2018-07-26 00:00:00\n",
      "Fecha actual 2018-07-27 00:00:00\n",
      "Fecha actual 2018-07-30 00:00:00\n",
      "Fecha actual 2018-07-31 00:00:00\n",
      "Fecha actual 2018-08-01 00:00:00\n",
      "Fecha actual 2018-08-02 00:00:00\n",
      "Fecha actual 2018-08-03 00:00:00\n",
      "Fecha actual 2018-08-06 00:00:00\n",
      "Fecha actual 2018-08-07 00:00:00\n",
      "Fecha actual 2018-08-08 00:00:00\n",
      "Fecha actual 2018-08-09 00:00:00\n",
      "Fecha actual 2018-08-10 00:00:00\n",
      "Fecha actual 2018-08-13 00:00:00\n",
      "Fecha actual 2018-08-14 00:00:00\n",
      "Fecha actual 2018-08-15 00:00:00\n",
      "Fecha actual 2018-08-16 00:00:00\n",
      "Fecha actual 2018-08-17 00:00:00\n",
      "Fecha actual 2018-08-20 00:00:00\n",
      "Fecha actual 2018-08-21 00:00:00\n",
      "Fecha actual 2018-08-22 00:00:00\n",
      "Fecha actual 2018-08-23 00:00:00\n",
      "Fecha actual 2018-08-24 00:00:00\n",
      "Fecha actual 2018-08-27 00:00:00\n",
      "Fecha actual 2018-08-28 00:00:00\n",
      "Fecha actual 2018-08-29 00:00:00\n",
      "Fecha actual 2018-08-30 00:00:00\n",
      "Fecha actual 2018-08-31 00:00:00\n",
      "Fecha actual 2018-09-04 00:00:00\n",
      "Fecha actual 2018-09-05 00:00:00\n",
      "Fecha actual 2018-09-06 00:00:00\n",
      "Fecha actual 2018-09-07 00:00:00\n",
      "Fecha actual 2018-09-10 00:00:00\n",
      "Fecha actual 2018-09-11 00:00:00\n",
      "Fecha actual 2018-09-12 00:00:00\n",
      "Fecha actual 2018-09-13 00:00:00\n",
      "Fecha actual 2018-09-14 00:00:00\n",
      "Fecha actual 2018-09-17 00:00:00\n",
      "Fecha actual 2018-09-18 00:00:00\n",
      "Fecha actual 2018-09-19 00:00:00\n",
      "Fecha actual 2018-09-20 00:00:00\n",
      "Fecha actual 2018-09-21 00:00:00\n",
      "Fecha actual 2018-09-24 00:00:00\n",
      "Fecha actual 2018-09-25 00:00:00\n",
      "Fecha actual 2018-09-26 00:00:00\n",
      "Fecha actual 2018-09-27 00:00:00\n",
      "Fecha actual 2018-09-28 00:00:00\n",
      "Fecha actual 2018-10-01 00:00:00\n",
      "Fecha actual 2018-10-02 00:00:00\n",
      "Fecha actual 2018-10-03 00:00:00\n",
      "Fecha actual 2018-10-04 00:00:00\n",
      "Fecha actual 2018-10-05 00:00:00\n",
      "Fecha actual 2018-10-08 00:00:00\n",
      "Fecha actual 2018-10-09 00:00:00\n",
      "Fecha actual 2018-10-10 00:00:00\n",
      "Fecha actual 2018-10-11 00:00:00\n",
      "Fecha actual 2018-10-12 00:00:00\n",
      "Fecha actual 2018-10-15 00:00:00\n",
      "Fecha actual 2018-10-16 00:00:00\n",
      "Fecha actual 2018-10-17 00:00:00\n",
      "Fecha actual 2018-10-18 00:00:00\n",
      "Fecha actual 2018-10-19 00:00:00\n",
      "Fecha actual 2018-10-22 00:00:00\n",
      "Fecha actual 2018-10-23 00:00:00\n",
      "Fecha actual 2018-10-24 00:00:00\n",
      "Fecha actual 2018-10-25 00:00:00\n",
      "Fecha actual 2018-10-26 00:00:00\n",
      "Fecha actual 2018-10-29 00:00:00\n",
      "Fecha actual 2018-10-30 00:00:00\n",
      "Fecha actual 2018-10-31 00:00:00\n",
      "Fecha actual 2018-11-01 00:00:00\n",
      "Fecha actual 2018-11-02 00:00:00\n",
      "Fecha actual 2018-11-05 00:00:00\n",
      "Fecha actual 2018-11-06 00:00:00\n",
      "Fecha actual 2018-11-07 00:00:00\n",
      "Fecha actual 2018-11-08 00:00:00\n",
      "Fecha actual 2018-11-09 00:00:00\n",
      "Fecha actual 2018-11-12 00:00:00\n",
      "Fecha actual 2018-11-13 00:00:00\n",
      "Fecha actual 2018-11-14 00:00:00\n",
      "Fecha actual 2018-11-15 00:00:00\n",
      "Fecha actual 2018-11-16 00:00:00\n",
      "Fecha actual 2018-11-19 00:00:00\n",
      "Fecha actual 2018-11-20 00:00:00\n",
      "Fecha actual 2018-11-21 00:00:00\n",
      "Fecha actual 2018-11-23 00:00:00\n",
      "Fecha actual 2018-11-26 00:00:00\n",
      "Fecha actual 2018-11-27 00:00:00\n",
      "Fecha actual 2018-11-28 00:00:00\n",
      "Fecha actual 2018-11-29 00:00:00\n",
      "Fecha actual 2018-11-30 00:00:00\n",
      "Fecha actual 2018-12-03 00:00:00\n",
      "Fecha actual 2018-12-04 00:00:00\n",
      "Fecha actual 2018-12-06 00:00:00\n",
      "Fecha actual 2018-12-07 00:00:00\n",
      "Fecha actual 2018-12-10 00:00:00\n",
      "Fecha actual 2018-12-11 00:00:00\n",
      "Fecha actual 2018-12-12 00:00:00\n",
      "Fecha actual 2018-12-13 00:00:00\n",
      "Fecha actual 2018-12-14 00:00:00\n",
      "Fecha actual 2018-12-17 00:00:00\n",
      "Fecha actual 2018-12-18 00:00:00\n",
      "Fecha actual 2018-12-19 00:00:00\n",
      "Fecha actual 2018-12-20 00:00:00\n",
      "Fecha actual 2018-12-21 00:00:00\n",
      "Fecha actual 2018-12-24 00:00:00\n",
      "Fecha actual 2018-12-26 00:00:00\n",
      "Fecha actual 2018-12-27 00:00:00\n",
      "Fecha actual 2018-12-28 00:00:00\n",
      "Fecha actual 2018-12-31 00:00:00\n",
      "Fecha actual 2019-01-02 00:00:00\n",
      "Fecha actual 2019-01-03 00:00:00\n",
      "Fecha actual 2019-01-04 00:00:00\n",
      "Fecha actual 2019-01-07 00:00:00\n",
      "Fecha actual 2019-01-08 00:00:00\n",
      "Fecha actual 2019-01-09 00:00:00\n",
      "Fecha actual 2019-01-10 00:00:00\n",
      "Fecha actual 2019-01-11 00:00:00\n",
      "Fecha actual 2019-01-14 00:00:00\n",
      "Fecha actual 2019-01-15 00:00:00\n",
      "Fecha actual 2019-01-16 00:00:00\n",
      "Fecha actual 2019-01-17 00:00:00\n",
      "Fecha actual 2019-01-18 00:00:00\n",
      "Fecha actual 2019-01-22 00:00:00\n",
      "Fecha actual 2019-01-23 00:00:00\n",
      "Fecha actual 2019-01-24 00:00:00\n",
      "Fecha actual 2019-01-25 00:00:00\n",
      "Fecha actual 2019-01-28 00:00:00\n",
      "Fecha actual 2019-01-29 00:00:00\n",
      "Fecha actual 2019-01-30 00:00:00\n",
      "Fecha actual 2019-01-31 00:00:00\n",
      "Fecha actual 2019-02-01 00:00:00\n",
      "Fecha actual 2019-02-04 00:00:00\n",
      "Fecha actual 2019-02-05 00:00:00\n",
      "Fecha actual 2019-02-06 00:00:00\n",
      "Fecha actual 2019-02-07 00:00:00\n",
      "Fecha actual 2019-02-08 00:00:00\n",
      "Fecha actual 2019-02-11 00:00:00\n",
      "Fecha actual 2019-02-12 00:00:00\n",
      "Fecha actual 2019-02-13 00:00:00\n",
      "Fecha actual 2019-02-14 00:00:00\n",
      "Fecha actual 2019-02-15 00:00:00\n",
      "Fecha actual 2019-02-19 00:00:00\n",
      "Fecha actual 2019-02-20 00:00:00\n",
      "Fecha actual 2019-02-21 00:00:00\n",
      "Fecha actual 2019-02-22 00:00:00\n",
      "Fecha actual 2019-02-25 00:00:00\n",
      "Fecha actual 2019-02-26 00:00:00\n",
      "Fecha actual 2019-02-27 00:00:00\n",
      "Fecha actual 2019-02-28 00:00:00\n",
      "Fecha actual 2019-03-01 00:00:00\n",
      "Fecha actual 2019-03-04 00:00:00\n",
      "Fecha actual 2019-03-05 00:00:00\n",
      "Fecha actual 2019-03-06 00:00:00\n",
      "Fecha actual 2019-03-07 00:00:00\n",
      "Fecha actual 2019-03-08 00:00:00\n",
      "Fecha actual 2019-03-11 00:00:00\n",
      "Fecha actual 2019-03-12 00:00:00\n",
      "Fecha actual 2019-03-13 00:00:00\n",
      "Fecha actual 2019-03-14 00:00:00\n",
      "Fecha actual 2019-03-15 00:00:00\n",
      "Fecha actual 2019-03-18 00:00:00\n",
      "Fecha actual 2019-03-19 00:00:00\n",
      "Fecha actual 2019-03-20 00:00:00\n",
      "Fecha actual 2019-03-21 00:00:00\n",
      "Fecha actual 2019-03-22 00:00:00\n",
      "Fecha actual 2019-03-25 00:00:00\n",
      "Fecha actual 2019-03-26 00:00:00\n",
      "Fecha actual 2019-03-27 00:00:00\n",
      "Fecha actual 2019-03-28 00:00:00\n",
      "Fecha actual 2019-03-29 00:00:00\n",
      "Fecha actual 2019-04-01 00:00:00\n",
      "Fecha actual 2019-04-02 00:00:00\n",
      "Fecha actual 2019-04-03 00:00:00\n",
      "Fecha actual 2019-04-04 00:00:00\n",
      "Fecha actual 2019-04-05 00:00:00\n",
      "Fecha actual 2019-04-08 00:00:00\n",
      "Fecha actual 2019-04-09 00:00:00\n",
      "Fecha actual 2019-04-10 00:00:00\n",
      "Fecha actual 2019-04-11 00:00:00\n",
      "Fecha actual 2019-04-12 00:00:00\n",
      "Fecha actual 2019-04-15 00:00:00\n",
      "Fecha actual 2019-04-16 00:00:00\n",
      "Fecha actual 2019-04-17 00:00:00\n",
      "Fecha actual 2019-04-18 00:00:00\n",
      "Fecha actual 2019-04-22 00:00:00\n",
      "Fecha actual 2019-04-23 00:00:00\n",
      "Fecha actual 2019-04-24 00:00:00\n",
      "Fecha actual 2019-04-25 00:00:00\n",
      "Fecha actual 2019-04-26 00:00:00\n",
      "Fecha actual 2019-04-29 00:00:00\n",
      "Fecha actual 2019-04-30 00:00:00\n",
      "Fecha actual 2019-05-01 00:00:00\n",
      "Fecha actual 2019-05-02 00:00:00\n",
      "Fecha actual 2019-05-03 00:00:00\n",
      "Fecha actual 2019-05-06 00:00:00\n",
      "Fecha actual 2019-05-07 00:00:00\n",
      "Fecha actual 2019-05-08 00:00:00\n",
      "Fecha actual 2019-05-09 00:00:00\n",
      "Fecha actual 2019-05-10 00:00:00\n",
      "Fecha actual 2019-05-13 00:00:00\n",
      "Fecha actual 2019-05-14 00:00:00\n",
      "Fecha actual 2019-05-15 00:00:00\n",
      "Fecha actual 2019-05-16 00:00:00\n",
      "Fecha actual 2019-05-17 00:00:00\n",
      "Fecha actual 2019-05-20 00:00:00\n",
      "Fecha actual 2019-05-21 00:00:00\n",
      "Fecha actual 2019-05-22 00:00:00\n",
      "Fecha actual 2019-05-23 00:00:00\n",
      "Fecha actual 2019-05-24 00:00:00\n",
      "Fecha actual 2019-05-28 00:00:00\n",
      "Fecha actual 2019-05-29 00:00:00\n",
      "Fecha actual 2019-05-30 00:00:00\n",
      "Fecha actual 2019-05-31 00:00:00\n",
      "Fecha actual 2019-06-03 00:00:00\n",
      "Fecha actual 2019-06-04 00:00:00\n",
      "Fecha actual 2019-06-05 00:00:00\n",
      "Fecha actual 2019-06-06 00:00:00\n",
      "Fecha actual 2019-06-07 00:00:00\n",
      "Fecha actual 2019-06-10 00:00:00\n",
      "Fecha actual 2019-06-11 00:00:00\n",
      "Fecha actual 2019-06-12 00:00:00\n",
      "Fecha actual 2019-06-13 00:00:00\n",
      "Fecha actual 2019-06-14 00:00:00\n",
      "Fecha actual 2019-06-17 00:00:00\n",
      "Fecha actual 2019-06-18 00:00:00\n",
      "Fecha actual 2019-06-19 00:00:00\n",
      "Fecha actual 2019-06-20 00:00:00\n",
      "Fecha actual 2019-06-21 00:00:00\n",
      "Fecha actual 2019-06-24 00:00:00\n",
      "Fecha actual 2019-06-25 00:00:00\n",
      "Fecha actual 2019-06-26 00:00:00\n",
      "Fecha actual 2019-06-27 00:00:00\n",
      "Fecha actual 2019-06-28 00:00:00\n",
      "Fecha actual 2019-07-01 00:00:00\n",
      "Fecha actual 2019-07-02 00:00:00\n",
      "Fecha actual 2019-07-03 00:00:00\n",
      "Fecha actual 2019-07-05 00:00:00\n",
      "Fecha actual 2019-07-08 00:00:00\n",
      "Fecha actual 2019-07-09 00:00:00\n",
      "Fecha actual 2019-07-10 00:00:00\n",
      "Fecha actual 2019-07-11 00:00:00\n",
      "Fecha actual 2019-07-12 00:00:00\n",
      "Fecha actual 2019-07-15 00:00:00\n",
      "Fecha actual 2019-07-16 00:00:00\n",
      "Fecha actual 2019-07-17 00:00:00\n",
      "Fecha actual 2019-07-18 00:00:00\n",
      "Fecha actual 2019-07-19 00:00:00\n",
      "Fecha actual 2019-07-22 00:00:00\n",
      "Fecha actual 2019-07-23 00:00:00\n",
      "Fecha actual 2019-07-24 00:00:00\n",
      "Fecha actual 2019-07-25 00:00:00\n",
      "Fecha actual 2019-07-26 00:00:00\n",
      "Fecha actual 2019-07-29 00:00:00\n",
      "Fecha actual 2019-07-30 00:00:00\n",
      "Fecha actual 2019-07-31 00:00:00\n",
      "Fecha actual 2019-08-01 00:00:00\n",
      "Fecha actual 2019-08-02 00:00:00\n",
      "Fecha actual 2019-08-05 00:00:00\n",
      "Fecha actual 2019-08-06 00:00:00\n",
      "Fecha actual 2019-08-07 00:00:00\n",
      "Fecha actual 2019-08-08 00:00:00\n",
      "Fecha actual 2019-08-09 00:00:00\n",
      "Fecha actual 2019-08-12 00:00:00\n",
      "Fecha actual 2019-08-13 00:00:00\n",
      "Fecha actual 2019-08-14 00:00:00\n",
      "Fecha actual 2019-08-15 00:00:00\n",
      "Fecha actual 2019-08-16 00:00:00\n",
      "Fecha actual 2019-08-19 00:00:00\n",
      "Fecha actual 2019-08-20 00:00:00\n",
      "Fecha actual 2019-08-21 00:00:00\n",
      "Fecha actual 2019-08-22 00:00:00\n",
      "Fecha actual 2019-08-23 00:00:00\n",
      "Fecha actual 2019-08-26 00:00:00\n",
      "Fecha actual 2019-08-27 00:00:00\n",
      "Fecha actual 2019-08-28 00:00:00\n",
      "Fecha actual 2019-08-29 00:00:00\n",
      "Fecha actual 2019-08-30 00:00:00\n",
      "Fecha actual 2019-09-03 00:00:00\n",
      "Fecha actual 2019-09-04 00:00:00\n",
      "Fecha actual 2019-09-05 00:00:00\n",
      "Fecha actual 2019-09-06 00:00:00\n",
      "Fecha actual 2019-09-09 00:00:00\n",
      "Fecha actual 2019-09-10 00:00:00\n",
      "Fecha actual 2019-09-11 00:00:00\n",
      "Fecha actual 2019-09-12 00:00:00\n",
      "Fecha actual 2019-09-13 00:00:00\n",
      "Fecha actual 2019-09-16 00:00:00\n",
      "Fecha actual 2019-09-17 00:00:00\n",
      "Fecha actual 2019-09-18 00:00:00\n",
      "Fecha actual 2019-09-19 00:00:00\n",
      "Fecha actual 2019-09-20 00:00:00\n",
      "Fecha actual 2019-09-23 00:00:00\n",
      "Fecha actual 2019-09-24 00:00:00\n",
      "Fecha actual 2019-09-25 00:00:00\n",
      "Fecha actual 2019-09-26 00:00:00\n",
      "Fecha actual 2019-09-27 00:00:00\n",
      "Fecha actual 2019-09-30 00:00:00\n",
      "Fecha actual 2019-10-01 00:00:00\n",
      "Fecha actual 2019-10-02 00:00:00\n",
      "Fecha actual 2019-10-03 00:00:00\n",
      "Fecha actual 2019-10-04 00:00:00\n",
      "Fecha actual 2019-10-07 00:00:00\n",
      "Fecha actual 2019-10-08 00:00:00\n",
      "Fecha actual 2019-10-09 00:00:00\n",
      "Fecha actual 2019-10-10 00:00:00\n",
      "Fecha actual 2019-10-11 00:00:00\n",
      "Fecha actual 2019-10-14 00:00:00\n",
      "Fecha actual 2019-10-15 00:00:00\n",
      "Fecha actual 2019-10-16 00:00:00\n",
      "Fecha actual 2019-10-17 00:00:00\n",
      "Fecha actual 2019-10-18 00:00:00\n",
      "Fecha actual 2019-10-21 00:00:00\n",
      "Fecha actual 2019-10-22 00:00:00\n",
      "Fecha actual 2019-10-23 00:00:00\n",
      "Fecha actual 2019-10-24 00:00:00\n",
      "Fecha actual 2019-10-25 00:00:00\n",
      "Fecha actual 2019-10-28 00:00:00\n",
      "Fecha actual 2019-10-29 00:00:00\n",
      "Fecha actual 2019-10-30 00:00:00\n",
      "Fecha actual 2019-10-31 00:00:00\n",
      "Fecha actual 2019-11-01 00:00:00\n",
      "Fecha actual 2019-11-04 00:00:00\n",
      "Fecha actual 2019-11-05 00:00:00\n",
      "Fecha actual 2019-11-06 00:00:00\n",
      "Fecha actual 2019-11-07 00:00:00\n",
      "Fecha actual 2019-11-08 00:00:00\n",
      "Fecha actual 2019-11-11 00:00:00\n",
      "Fecha actual 2019-11-12 00:00:00\n",
      "Fecha actual 2019-11-13 00:00:00\n",
      "Fecha actual 2019-11-14 00:00:00\n",
      "Fecha actual 2019-11-15 00:00:00\n",
      "Fecha actual 2019-11-18 00:00:00\n",
      "Fecha actual 2019-11-19 00:00:00\n",
      "Fecha actual 2019-11-20 00:00:00\n",
      "Fecha actual 2019-11-21 00:00:00\n",
      "Fecha actual 2019-11-22 00:00:00\n",
      "Fecha actual 2019-11-25 00:00:00\n",
      "Fecha actual 2019-11-26 00:00:00\n",
      "Fecha actual 2019-11-27 00:00:00\n",
      "Fecha actual 2019-11-29 00:00:00\n",
      "Fecha actual 2019-12-02 00:00:00\n",
      "Fecha actual 2019-12-03 00:00:00\n",
      "Fecha actual 2019-12-04 00:00:00\n",
      "Fecha actual 2019-12-05 00:00:00\n",
      "Fecha actual 2019-12-06 00:00:00\n",
      "Fecha actual 2019-12-09 00:00:00\n",
      "Fecha actual 2019-12-10 00:00:00\n",
      "Fecha actual 2019-12-11 00:00:00\n",
      "Fecha actual 2019-12-12 00:00:00\n",
      "Fecha actual 2019-12-13 00:00:00\n",
      "Fecha actual 2019-12-16 00:00:00\n",
      "Fecha actual 2019-12-17 00:00:00\n",
      "Fecha actual 2019-12-18 00:00:00\n",
      "Fecha actual 2019-12-19 00:00:00\n",
      "Fecha actual 2019-12-20 00:00:00\n",
      "Fecha actual 2019-12-23 00:00:00\n",
      "Fecha actual 2019-12-24 00:00:00\n",
      "Fecha actual 2019-12-26 00:00:00\n",
      "Fecha actual 2019-12-27 00:00:00\n",
      "Fecha actual 2019-12-30 00:00:00\n",
      "Fecha actual 2019-12-31 00:00:00\n",
      "Fecha actual 2020-01-02 00:00:00\n",
      "Fecha actual 2020-01-03 00:00:00\n",
      "Fecha actual 2020-01-06 00:00:00\n",
      "Fecha actual 2020-01-07 00:00:00\n",
      "Fecha actual 2020-01-08 00:00:00\n",
      "Fecha actual 2020-01-09 00:00:00\n",
      "Fecha actual 2020-01-10 00:00:00\n",
      "Fecha actual 2020-01-13 00:00:00\n",
      "Fecha actual 2020-01-14 00:00:00\n",
      "Fecha actual 2020-01-15 00:00:00\n",
      "Fecha actual 2020-01-16 00:00:00\n",
      "Fecha actual 2020-01-17 00:00:00\n",
      "Fecha actual 2020-01-21 00:00:00\n",
      "Fecha actual 2020-01-22 00:00:00\n",
      "Fecha actual 2020-01-23 00:00:00\n",
      "Fecha actual 2020-01-24 00:00:00\n",
      "Fecha actual 2020-01-27 00:00:00\n",
      "Fecha actual 2020-01-28 00:00:00\n",
      "Fecha actual 2020-01-29 00:00:00\n",
      "Fecha actual 2020-01-30 00:00:00\n",
      "Fecha actual 2020-01-31 00:00:00\n",
      "Fecha actual 2020-02-03 00:00:00\n",
      "Fecha actual 2020-02-04 00:00:00\n",
      "Fecha actual 2020-02-05 00:00:00\n",
      "Fecha actual 2020-02-06 00:00:00\n",
      "Fecha actual 2020-02-07 00:00:00\n",
      "Fecha actual 2020-02-10 00:00:00\n",
      "Fecha actual 2020-02-11 00:00:00\n",
      "Fecha actual 2020-02-12 00:00:00\n",
      "Fecha actual 2020-02-13 00:00:00\n",
      "Fecha actual 2020-02-14 00:00:00\n",
      "Fecha actual 2020-02-18 00:00:00\n",
      "Fecha actual 2020-02-19 00:00:00\n",
      "Fecha actual 2020-02-20 00:00:00\n",
      "Fecha actual 2020-02-21 00:00:00\n",
      "Fecha actual 2020-02-24 00:00:00\n",
      "Fecha actual 2020-02-25 00:00:00\n",
      "Fecha actual 2020-02-26 00:00:00\n",
      "Fecha actual 2020-02-27 00:00:00\n",
      "Fecha actual 2020-02-28 00:00:00\n",
      "Fecha actual 2020-03-02 00:00:00\n",
      "Fecha actual 2020-03-03 00:00:00\n",
      "Fecha actual 2020-03-04 00:00:00\n",
      "Fecha actual 2020-03-05 00:00:00\n",
      "Fecha actual 2020-03-06 00:00:00\n",
      "Fecha actual 2020-03-09 00:00:00\n",
      "Fecha actual 2020-03-10 00:00:00\n",
      "Fecha actual 2020-03-11 00:00:00\n",
      "Fecha actual 2020-03-12 00:00:00\n",
      "Fecha actual 2020-03-13 00:00:00\n",
      "Fecha actual 2020-03-16 00:00:00\n",
      "Fecha actual 2020-03-17 00:00:00\n",
      "Fecha actual 2020-03-18 00:00:00\n",
      "Fecha actual 2020-03-19 00:00:00\n",
      "Fecha actual 2020-03-20 00:00:00\n",
      "Fecha actual 2020-03-23 00:00:00\n",
      "Fecha actual 2020-03-24 00:00:00\n",
      "Fecha actual 2020-03-25 00:00:00\n",
      "Fecha actual 2020-03-26 00:00:00\n",
      "Fecha actual 2020-03-27 00:00:00\n",
      "Fecha actual 2020-03-30 00:00:00\n",
      "Fecha actual 2020-03-31 00:00:00\n",
      "Fecha actual 2020-04-01 00:00:00\n",
      "Fecha actual 2020-04-02 00:00:00\n",
      "Fecha actual 2020-04-03 00:00:00\n",
      "Fecha actual 2020-04-06 00:00:00\n",
      "Fecha actual 2020-04-07 00:00:00\n",
      "Fecha actual 2020-04-08 00:00:00\n",
      "Fecha actual 2020-04-09 00:00:00\n",
      "Fecha actual 2020-04-13 00:00:00\n",
      "Fecha actual 2020-04-14 00:00:00\n",
      "Fecha actual 2020-04-15 00:00:00\n",
      "Fecha actual 2020-04-16 00:00:00\n",
      "Fecha actual 2020-04-17 00:00:00\n",
      "Fecha actual 2020-04-20 00:00:00\n",
      "Fecha actual 2020-04-21 00:00:00\n",
      "Fecha actual 2020-04-22 00:00:00\n",
      "Fecha actual 2020-04-23 00:00:00\n",
      "Fecha actual 2020-04-24 00:00:00\n",
      "Fecha actual 2020-04-27 00:00:00\n",
      "Fecha actual 2020-04-28 00:00:00\n",
      "Fecha actual 2020-04-29 00:00:00\n",
      "Fecha actual 2020-04-30 00:00:00\n",
      "Fecha actual 2020-05-01 00:00:00\n",
      "Fecha actual 2020-05-04 00:00:00\n",
      "Fecha actual 2020-05-05 00:00:00\n",
      "Fecha actual 2020-05-06 00:00:00\n",
      "Fecha actual 2020-05-07 00:00:00\n",
      "Fecha actual 2020-05-08 00:00:00\n",
      "Fecha actual 2020-05-11 00:00:00\n",
      "Fecha actual 2020-05-12 00:00:00\n",
      "Fecha actual 2020-05-13 00:00:00\n",
      "Fecha actual 2020-05-14 00:00:00\n",
      "Fecha actual 2020-05-15 00:00:00\n",
      "Fecha actual 2020-05-18 00:00:00\n",
      "Fecha actual 2020-05-19 00:00:00\n",
      "Fecha actual 2020-05-20 00:00:00\n",
      "Fecha actual 2020-05-21 00:00:00\n",
      "Fecha actual 2020-05-22 00:00:00\n",
      "Fecha actual 2020-05-26 00:00:00\n",
      "Fecha actual 2020-05-27 00:00:00\n",
      "Fecha actual 2020-05-28 00:00:00\n",
      "Fecha actual 2020-05-29 00:00:00\n",
      "Fecha actual 2020-06-01 00:00:00\n",
      "Fecha actual 2020-06-02 00:00:00\n",
      "Fecha actual 2020-06-03 00:00:00\n",
      "Fecha actual 2020-06-04 00:00:00\n",
      "Fecha actual 2020-06-05 00:00:00\n",
      "Fecha actual 2020-06-08 00:00:00\n",
      "Fecha actual 2020-06-09 00:00:00\n",
      "Fecha actual 2020-06-10 00:00:00\n",
      "Fecha actual 2020-06-11 00:00:00\n",
      "Fecha actual 2020-06-12 00:00:00\n",
      "Fecha actual 2020-06-15 00:00:00\n",
      "Fecha actual 2020-06-16 00:00:00\n",
      "Fecha actual 2020-06-17 00:00:00\n",
      "Fecha actual 2020-06-18 00:00:00\n",
      "Fecha actual 2020-06-19 00:00:00\n",
      "Fecha actual 2020-06-22 00:00:00\n",
      "Fecha actual 2020-06-23 00:00:00\n",
      "Fecha actual 2020-06-24 00:00:00\n",
      "Fecha actual 2020-06-25 00:00:00\n",
      "Fecha actual 2020-06-26 00:00:00\n",
      "Fecha actual 2020-06-29 00:00:00\n",
      "Fecha actual 2020-06-30 00:00:00\n",
      "Fecha actual 2020-07-01 00:00:00\n",
      "Fecha actual 2020-07-02 00:00:00\n",
      "Fecha actual 2020-07-06 00:00:00\n",
      "Fecha actual 2020-07-07 00:00:00\n",
      "Fecha actual 2020-07-08 00:00:00\n",
      "Fecha actual 2020-07-09 00:00:00\n",
      "Fecha actual 2020-07-10 00:00:00\n",
      "Fecha actual 2020-07-13 00:00:00\n",
      "Fecha actual 2020-07-14 00:00:00\n",
      "Fecha actual 2020-07-15 00:00:00\n",
      "Fecha actual 2020-07-16 00:00:00\n",
      "Fecha actual 2020-07-17 00:00:00\n",
      "Fecha actual 2020-07-20 00:00:00\n",
      "Fecha actual 2020-07-21 00:00:00\n",
      "Fecha actual 2020-07-22 00:00:00\n",
      "Fecha actual 2020-07-23 00:00:00\n",
      "Fecha actual 2020-07-24 00:00:00\n",
      "Fecha actual 2020-07-27 00:00:00\n",
      "Fecha actual 2020-07-28 00:00:00\n",
      "Fecha actual 2020-07-29 00:00:00\n",
      "Fecha actual 2020-07-30 00:00:00\n",
      "Fecha actual 2020-07-31 00:00:00\n",
      "Fecha actual 2020-08-03 00:00:00\n",
      "Fecha actual 2020-08-04 00:00:00\n",
      "Fecha actual 2020-08-05 00:00:00\n",
      "Fecha actual 2020-08-06 00:00:00\n",
      "Fecha actual 2020-08-07 00:00:00\n",
      "Fecha actual 2020-08-10 00:00:00\n",
      "Fecha actual 2020-08-11 00:00:00\n",
      "Fecha actual 2020-08-12 00:00:00\n",
      "Fecha actual 2020-08-13 00:00:00\n",
      "Fecha actual 2020-08-14 00:00:00\n",
      "Fecha actual 2020-08-17 00:00:00\n",
      "Fecha actual 2020-08-18 00:00:00\n",
      "Fecha actual 2020-08-19 00:00:00\n",
      "Fecha actual 2020-08-20 00:00:00\n",
      "Fecha actual 2020-08-21 00:00:00\n",
      "Fecha actual 2020-08-24 00:00:00\n",
      "Fecha actual 2020-08-25 00:00:00\n",
      "Fecha actual 2020-08-26 00:00:00\n",
      "Fecha actual 2020-08-27 00:00:00\n",
      "Fecha actual 2020-08-28 00:00:00\n",
      "Fecha actual 2020-08-31 00:00:00\n",
      "Fecha actual 2020-09-01 00:00:00\n",
      "Fecha actual 2020-09-02 00:00:00\n",
      "Fecha actual 2020-09-03 00:00:00\n",
      "Fecha actual 2020-09-04 00:00:00\n",
      "Fecha actual 2020-09-08 00:00:00\n",
      "Fecha actual 2020-09-09 00:00:00\n",
      "Fecha actual 2020-09-10 00:00:00\n",
      "Fecha actual 2020-09-11 00:00:00\n",
      "Fecha actual 2020-09-14 00:00:00\n",
      "Fecha actual 2020-09-15 00:00:00\n",
      "Fecha actual 2020-09-16 00:00:00\n",
      "Fecha actual 2020-09-17 00:00:00\n",
      "Fecha actual 2020-09-18 00:00:00\n",
      "Fecha actual 2020-09-21 00:00:00\n",
      "Fecha actual 2020-09-22 00:00:00\n",
      "Fecha actual 2020-09-23 00:00:00\n",
      "Fecha actual 2020-09-24 00:00:00\n",
      "Fecha actual 2020-09-25 00:00:00\n",
      "Fecha actual 2020-09-28 00:00:00\n",
      "Fecha actual 2020-09-29 00:00:00\n",
      "Fecha actual 2020-09-30 00:00:00\n",
      "Fecha actual 2020-10-01 00:00:00\n",
      "Fecha actual 2020-10-02 00:00:00\n",
      "Fecha actual 2020-10-05 00:00:00\n",
      "Fecha actual 2020-10-06 00:00:00\n",
      "Fecha actual 2020-10-07 00:00:00\n",
      "Fecha actual 2020-10-08 00:00:00\n",
      "Fecha actual 2020-10-09 00:00:00\n",
      "Fecha actual 2020-10-12 00:00:00\n",
      "Fecha actual 2020-10-13 00:00:00\n",
      "Fecha actual 2020-10-14 00:00:00\n",
      "Fecha actual 2020-10-15 00:00:00\n",
      "Fecha actual 2020-10-16 00:00:00\n",
      "Fecha actual 2020-10-19 00:00:00\n",
      "Fecha actual 2020-10-20 00:00:00\n",
      "Fecha actual 2020-10-21 00:00:00\n",
      "Fecha actual 2020-10-22 00:00:00\n",
      "Fecha actual 2020-10-23 00:00:00\n",
      "Fecha actual 2020-10-26 00:00:00\n",
      "Fecha actual 2020-10-27 00:00:00\n",
      "Fecha actual 2020-10-28 00:00:00\n",
      "Fecha actual 2020-10-29 00:00:00\n",
      "Fecha actual 2020-10-30 00:00:00\n",
      "Fecha actual 2020-11-02 00:00:00\n",
      "Fecha actual 2020-11-03 00:00:00\n",
      "Fecha actual 2020-11-04 00:00:00\n",
      "Fecha actual 2020-11-05 00:00:00\n",
      "Fecha actual 2020-11-06 00:00:00\n",
      "Fecha actual 2020-11-09 00:00:00\n",
      "Fecha actual 2020-11-10 00:00:00\n",
      "Fecha actual 2020-11-11 00:00:00\n",
      "Fecha actual 2020-11-12 00:00:00\n",
      "Fecha actual 2020-11-13 00:00:00\n",
      "Fecha actual 2020-11-16 00:00:00\n",
      "Fecha actual 2020-11-17 00:00:00\n",
      "Fecha actual 2020-11-18 00:00:00\n",
      "Fecha actual 2020-11-19 00:00:00\n",
      "Fecha actual 2020-11-20 00:00:00\n",
      "Fecha actual 2020-11-23 00:00:00\n",
      "Fecha actual 2020-11-24 00:00:00\n",
      "Fecha actual 2020-11-25 00:00:00\n",
      "Fecha actual 2020-11-27 00:00:00\n",
      "Fecha actual 2020-11-30 00:00:00\n",
      "Fecha actual 2020-12-01 00:00:00\n",
      "Fecha actual 2020-12-02 00:00:00\n",
      "Fecha actual 2020-12-03 00:00:00\n",
      "Fecha actual 2020-12-04 00:00:00\n",
      "Fecha actual 2020-12-07 00:00:00\n",
      "Fecha actual 2020-12-08 00:00:00\n",
      "Fecha actual 2020-12-09 00:00:00\n",
      "Fecha actual 2020-12-10 00:00:00\n",
      "Fecha actual 2020-12-11 00:00:00\n",
      "Fecha actual 2020-12-14 00:00:00\n",
      "Fecha actual 2020-12-15 00:00:00\n",
      "Fecha actual 2020-12-16 00:00:00\n",
      "Fecha actual 2020-12-17 00:00:00\n",
      "Fecha actual 2020-12-18 00:00:00\n",
      "Fecha actual 2020-12-21 00:00:00\n",
      "Fecha actual 2020-12-22 00:00:00\n",
      "Fecha actual 2020-12-23 00:00:00\n",
      "Fecha actual 2020-12-24 00:00:00\n",
      "Fecha actual 2020-12-28 00:00:00\n",
      "Fecha actual 2020-12-29 00:00:00\n",
      "Fecha actual 2020-12-30 00:00:00\n",
      "Fecha actual 2020-12-31 00:00:00\n",
      "Fecha actual 2021-01-04 00:00:00\n",
      "Fecha actual 2021-01-05 00:00:00\n",
      "Fecha actual 2021-01-06 00:00:00\n",
      "Fecha actual 2021-01-07 00:00:00\n",
      "Fecha actual 2021-01-08 00:00:00\n",
      "Fecha actual 2021-01-11 00:00:00\n",
      "Fecha actual 2021-01-12 00:00:00\n",
      "Fecha actual 2021-01-13 00:00:00\n",
      "Fecha actual 2021-01-14 00:00:00\n",
      "Fecha actual 2021-01-15 00:00:00\n",
      "Fecha actual 2021-01-19 00:00:00\n",
      "Fecha actual 2021-01-20 00:00:00\n",
      "Fecha actual 2021-01-21 00:00:00\n",
      "Fecha actual 2021-01-22 00:00:00\n",
      "Fecha actual 2021-01-25 00:00:00\n",
      "Fecha actual 2021-01-26 00:00:00\n",
      "Fecha actual 2021-01-27 00:00:00\n",
      "Fecha actual 2021-01-28 00:00:00\n",
      "Fecha actual 2021-01-29 00:00:00\n",
      "Fecha actual 2021-02-01 00:00:00\n",
      "Fecha actual 2021-02-02 00:00:00\n",
      "Fecha actual 2021-02-03 00:00:00\n",
      "Fecha actual 2021-02-04 00:00:00\n",
      "Fecha actual 2021-02-05 00:00:00\n",
      "Fecha actual 2021-02-08 00:00:00\n",
      "Fecha actual 2021-02-09 00:00:00\n",
      "Fecha actual 2021-02-10 00:00:00\n",
      "Fecha actual 2021-02-11 00:00:00\n",
      "Fecha actual 2021-02-12 00:00:00\n",
      "Fecha actual 2021-02-16 00:00:00\n",
      "Fecha actual 2021-02-17 00:00:00\n",
      "Fecha actual 2021-02-18 00:00:00\n",
      "Fecha actual 2021-02-19 00:00:00\n",
      "Fecha actual 2021-02-22 00:00:00\n",
      "Fecha actual 2021-02-23 00:00:00\n",
      "Fecha actual 2021-02-24 00:00:00\n",
      "Fecha actual 2021-02-25 00:00:00\n",
      "Fecha actual 2021-02-26 00:00:00\n",
      "Fecha actual 2021-03-01 00:00:00\n",
      "Fecha actual 2021-03-02 00:00:00\n",
      "Fecha actual 2021-03-03 00:00:00\n",
      "Fecha actual 2021-03-04 00:00:00\n",
      "Fecha actual 2021-03-05 00:00:00\n",
      "Fecha actual 2021-03-08 00:00:00\n",
      "Fecha actual 2021-03-09 00:00:00\n",
      "Fecha actual 2021-03-10 00:00:00\n",
      "Fecha actual 2021-03-11 00:00:00\n",
      "Fecha actual 2021-03-12 00:00:00\n",
      "Fecha actual 2021-03-15 00:00:00\n",
      "Fecha actual 2021-03-16 00:00:00\n",
      "Fecha actual 2021-03-17 00:00:00\n",
      "Fecha actual 2021-03-18 00:00:00\n",
      "Fecha actual 2021-03-19 00:00:00\n",
      "Fecha actual 2021-03-22 00:00:00\n",
      "Fecha actual 2021-03-23 00:00:00\n",
      "Fecha actual 2021-03-24 00:00:00\n",
      "Fecha actual 2021-03-25 00:00:00\n",
      "Fecha actual 2021-03-26 00:00:00\n",
      "Fecha actual 2021-03-29 00:00:00\n",
      "Fecha actual 2021-03-30 00:00:00\n",
      "Fecha actual 2021-03-31 00:00:00\n",
      "Fecha actual 2021-04-01 00:00:00\n",
      "Fecha actual 2021-04-05 00:00:00\n",
      "Fecha actual 2021-04-06 00:00:00\n",
      "Fecha actual 2021-04-07 00:00:00\n",
      "Fecha actual 2021-04-08 00:00:00\n",
      "Fecha actual 2021-04-09 00:00:00\n",
      "Fecha actual 2021-04-12 00:00:00\n",
      "Fecha actual 2021-04-13 00:00:00\n",
      "Fecha actual 2021-04-14 00:00:00\n",
      "Fecha actual 2021-04-15 00:00:00\n",
      "Fecha actual 2021-04-16 00:00:00\n",
      "Fecha actual 2021-04-19 00:00:00\n",
      "Fecha actual 2021-04-20 00:00:00\n",
      "Fecha actual 2021-04-21 00:00:00\n",
      "Fecha actual 2021-04-22 00:00:00\n",
      "Fecha actual 2021-04-23 00:00:00\n",
      "Fecha actual 2021-04-26 00:00:00\n",
      "Fecha actual 2021-04-27 00:00:00\n",
      "Fecha actual 2021-04-28 00:00:00\n",
      "Fecha actual 2021-04-29 00:00:00\n",
      "Fecha actual 2021-04-30 00:00:00\n",
      "Fecha actual 2021-05-03 00:00:00\n",
      "Fecha actual 2021-05-04 00:00:00\n",
      "Fecha actual 2021-05-05 00:00:00\n",
      "Fecha actual 2021-05-06 00:00:00\n",
      "Fecha actual 2021-05-07 00:00:00\n",
      "Fecha actual 2021-05-10 00:00:00\n",
      "Fecha actual 2021-05-11 00:00:00\n",
      "Fecha actual 2021-05-12 00:00:00\n",
      "Fecha actual 2021-05-13 00:00:00\n",
      "Fecha actual 2021-05-14 00:00:00\n",
      "Fecha actual 2021-05-17 00:00:00\n",
      "Fecha actual 2021-05-18 00:00:00\n",
      "Fecha actual 2021-05-19 00:00:00\n",
      "Fecha actual 2021-05-20 00:00:00\n",
      "Fecha actual 2021-05-21 00:00:00\n",
      "Fecha actual 2021-05-24 00:00:00\n",
      "Fecha actual 2021-05-25 00:00:00\n",
      "Fecha actual 2021-05-26 00:00:00\n",
      "Fecha actual 2021-05-27 00:00:00\n",
      "Fecha actual 2021-05-28 00:00:00\n",
      "Fecha actual 2021-06-01 00:00:00\n",
      "Fecha actual 2021-06-02 00:00:00\n",
      "Fecha actual 2021-06-03 00:00:00\n",
      "Fecha actual 2021-06-04 00:00:00\n",
      "Fecha actual 2021-06-07 00:00:00\n",
      "Fecha actual 2021-06-08 00:00:00\n",
      "Fecha actual 2021-06-09 00:00:00\n",
      "Fecha actual 2021-06-10 00:00:00\n",
      "Fecha actual 2021-06-11 00:00:00\n",
      "Fecha actual 2021-06-14 00:00:00\n",
      "Fecha actual 2021-06-15 00:00:00\n",
      "Fecha actual 2021-06-16 00:00:00\n",
      "Fecha actual 2021-06-17 00:00:00\n",
      "Fecha actual 2021-06-18 00:00:00\n",
      "Fecha actual 2021-06-21 00:00:00\n",
      "Fecha actual 2021-06-22 00:00:00\n",
      "Fecha actual 2021-06-23 00:00:00\n",
      "Fecha actual 2021-06-24 00:00:00\n",
      "Fecha actual 2021-06-25 00:00:00\n",
      "Fecha actual 2021-06-28 00:00:00\n",
      "Fecha actual 2021-06-29 00:00:00\n",
      "Fecha actual 2021-06-30 00:00:00\n",
      "Fecha actual 2021-07-01 00:00:00\n",
      "Fecha actual 2021-07-02 00:00:00\n",
      "Fecha actual 2021-07-06 00:00:00\n",
      "Fecha actual 2021-07-07 00:00:00\n",
      "Fecha actual 2021-07-08 00:00:00\n",
      "Fecha actual 2021-07-09 00:00:00\n",
      "Fecha actual 2021-07-12 00:00:00\n",
      "Fecha actual 2021-07-13 00:00:00\n",
      "Fecha actual 2021-07-14 00:00:00\n",
      "Fecha actual 2021-07-15 00:00:00\n",
      "Fecha actual 2021-07-16 00:00:00\n",
      "Fecha actual 2021-07-19 00:00:00\n",
      "Fecha actual 2021-07-20 00:00:00\n",
      "Fecha actual 2021-07-21 00:00:00\n",
      "Fecha actual 2021-07-22 00:00:00\n",
      "Fecha actual 2021-07-23 00:00:00\n",
      "Fecha actual 2021-07-26 00:00:00\n",
      "Fecha actual 2021-07-27 00:00:00\n",
      "Fecha actual 2021-07-28 00:00:00\n",
      "Fecha actual 2021-07-29 00:00:00\n",
      "Fecha actual 2021-07-30 00:00:00\n",
      "Fecha actual 2021-08-02 00:00:00\n",
      "Fecha actual 2021-08-03 00:00:00\n",
      "Fecha actual 2021-08-04 00:00:00\n",
      "Fecha actual 2021-08-05 00:00:00\n",
      "Fecha actual 2021-08-06 00:00:00\n",
      "Fecha actual 2021-08-09 00:00:00\n",
      "Fecha actual 2021-08-10 00:00:00\n",
      "Fecha actual 2021-08-11 00:00:00\n",
      "Fecha actual 2021-08-12 00:00:00\n",
      "Fecha actual 2021-08-13 00:00:00\n",
      "Fecha actual 2021-08-16 00:00:00\n",
      "Fecha actual 2021-08-17 00:00:00\n",
      "Fecha actual 2021-08-18 00:00:00\n",
      "Fecha actual 2021-08-19 00:00:00\n",
      "Fecha actual 2021-08-20 00:00:00\n",
      "Fecha actual 2021-08-23 00:00:00\n",
      "Fecha actual 2021-08-24 00:00:00\n",
      "Fecha actual 2021-08-25 00:00:00\n",
      "Fecha actual 2021-08-26 00:00:00\n",
      "Fecha actual 2021-08-27 00:00:00\n",
      "Fecha actual 2021-08-30 00:00:00\n",
      "Fecha actual 2021-08-31 00:00:00\n",
      "Fecha actual 2021-09-01 00:00:00\n",
      "Fecha actual 2021-09-02 00:00:00\n",
      "Fecha actual 2021-09-03 00:00:00\n",
      "Fecha actual 2021-09-07 00:00:00\n",
      "Fecha actual 2021-09-08 00:00:00\n",
      "Fecha actual 2021-09-09 00:00:00\n",
      "Fecha actual 2021-09-10 00:00:00\n",
      "Fecha actual 2021-09-13 00:00:00\n",
      "Fecha actual 2021-09-14 00:00:00\n",
      "Fecha actual 2021-09-15 00:00:00\n",
      "Fecha actual 2021-09-16 00:00:00\n",
      "Fecha actual 2021-09-17 00:00:00\n",
      "Fecha actual 2021-09-20 00:00:00\n",
      "Fecha actual 2021-09-21 00:00:00\n",
      "Fecha actual 2021-09-22 00:00:00\n",
      "Fecha actual 2021-09-23 00:00:00\n",
      "Fecha actual 2021-09-24 00:00:00\n",
      "Fecha actual 2021-09-27 00:00:00\n",
      "Fecha actual 2021-09-28 00:00:00\n",
      "Fecha actual 2021-09-29 00:00:00\n",
      "Fecha actual 2021-09-30 00:00:00\n",
      "Fecha actual 2021-10-01 00:00:00\n",
      "Fecha actual 2021-10-04 00:00:00\n",
      "Fecha actual 2021-10-05 00:00:00\n",
      "Fecha actual 2021-10-06 00:00:00\n",
      "Fecha actual 2021-10-07 00:00:00\n",
      "Fecha actual 2021-10-08 00:00:00\n",
      "Fecha actual 2021-10-11 00:00:00\n",
      "Fecha actual 2021-10-12 00:00:00\n",
      "Fecha actual 2021-10-13 00:00:00\n",
      "Fecha actual 2021-10-14 00:00:00\n",
      "Fecha actual 2021-10-15 00:00:00\n",
      "Fecha actual 2021-10-18 00:00:00\n",
      "Fecha actual 2021-10-19 00:00:00\n",
      "Fecha actual 2021-10-20 00:00:00\n",
      "Fecha actual 2021-10-21 00:00:00\n",
      "Fecha actual 2021-10-22 00:00:00\n",
      "Fecha actual 2021-10-25 00:00:00\n",
      "Fecha actual 2021-10-26 00:00:00\n",
      "Fecha actual 2021-10-27 00:00:00\n",
      "Fecha actual 2021-10-28 00:00:00\n",
      "Fecha actual 2021-10-29 00:00:00\n",
      "Fecha actual 2021-11-01 00:00:00\n",
      "Fecha actual 2021-11-02 00:00:00\n",
      "Fecha actual 2021-11-03 00:00:00\n",
      "Fecha actual 2021-11-04 00:00:00\n",
      "Fecha actual 2021-11-05 00:00:00\n",
      "Fecha actual 2021-11-08 00:00:00\n",
      "Fecha actual 2021-11-09 00:00:00\n",
      "Fecha actual 2021-11-10 00:00:00\n",
      "Fecha actual 2021-11-11 00:00:00\n",
      "Fecha actual 2021-11-12 00:00:00\n",
      "Fecha actual 2021-11-15 00:00:00\n",
      "Fecha actual 2021-11-16 00:00:00\n",
      "Fecha actual 2021-11-17 00:00:00\n",
      "Fecha actual 2021-11-18 00:00:00\n",
      "Fecha actual 2021-11-19 00:00:00\n",
      "Fecha actual 2021-11-22 00:00:00\n",
      "Fecha actual 2021-11-23 00:00:00\n",
      "Fecha actual 2021-11-24 00:00:00\n",
      "Fecha actual 2021-11-26 00:00:00\n",
      "Fecha actual 2021-11-29 00:00:00\n",
      "Fecha actual 2021-11-30 00:00:00\n",
      "Fecha actual 2021-12-01 00:00:00\n",
      "Fecha actual 2021-12-02 00:00:00\n",
      "Fecha actual 2021-12-03 00:00:00\n",
      "Fecha actual 2021-12-06 00:00:00\n",
      "Fecha actual 2021-12-07 00:00:00\n",
      "Fecha actual 2021-12-08 00:00:00\n",
      "Fecha actual 2021-12-09 00:00:00\n",
      "Fecha actual 2021-12-10 00:00:00\n",
      "Fecha actual 2021-12-13 00:00:00\n",
      "Fecha actual 2021-12-14 00:00:00\n",
      "Fecha actual 2021-12-15 00:00:00\n",
      "Fecha actual 2021-12-16 00:00:00\n",
      "Fecha actual 2021-12-17 00:00:00\n",
      "Fecha actual 2021-12-20 00:00:00\n",
      "Fecha actual 2021-12-21 00:00:00\n",
      "Fecha actual 2021-12-22 00:00:00\n",
      "Fecha actual 2021-12-23 00:00:00\n",
      "Fecha actual 2021-12-27 00:00:00\n",
      "Fecha actual 2021-12-28 00:00:00\n",
      "Fecha actual 2021-12-29 00:00:00\n",
      "Fecha actual 2021-12-30 00:00:00\n",
      "Fecha actual 2021-12-31 00:00:00\n",
      "Fecha actual 2022-01-03 00:00:00\n",
      "Fecha actual 2022-01-04 00:00:00\n",
      "Fecha actual 2022-01-05 00:00:00\n",
      "Fecha actual 2022-01-06 00:00:00\n",
      "Fecha actual 2022-01-07 00:00:00\n",
      "Fecha actual 2022-01-10 00:00:00\n",
      "Fecha actual 2022-01-11 00:00:00\n",
      "Fecha actual 2022-01-12 00:00:00\n",
      "Fecha actual 2022-01-13 00:00:00\n",
      "Fecha actual 2022-01-14 00:00:00\n",
      "Fecha actual 2022-01-18 00:00:00\n",
      "Fecha actual 2022-01-19 00:00:00\n",
      "Fecha actual 2022-01-20 00:00:00\n",
      "Fecha actual 2022-01-21 00:00:00\n",
      "Fecha actual 2022-01-24 00:00:00\n",
      "Fecha actual 2022-01-25 00:00:00\n",
      "Fecha actual 2022-01-26 00:00:00\n",
      "Fecha actual 2022-01-27 00:00:00\n",
      "Fecha actual 2022-01-28 00:00:00\n",
      "Fecha actual 2022-01-31 00:00:00\n",
      "Fecha actual 2022-02-01 00:00:00\n",
      "Fecha actual 2022-02-02 00:00:00\n",
      "Fecha actual 2022-02-03 00:00:00\n",
      "Fecha actual 2022-02-04 00:00:00\n",
      "Fecha actual 2022-02-07 00:00:00\n",
      "Fecha actual 2022-02-08 00:00:00\n",
      "Fecha actual 2022-02-09 00:00:00\n",
      "Fecha actual 2022-02-10 00:00:00\n",
      "Fecha actual 2022-02-11 00:00:00\n",
      "Fecha actual 2022-02-14 00:00:00\n",
      "Fecha actual 2022-02-15 00:00:00\n",
      "Fecha actual 2022-02-16 00:00:00\n",
      "Fecha actual 2022-02-17 00:00:00\n",
      "Fecha actual 2022-02-18 00:00:00\n",
      "Fecha actual 2022-02-22 00:00:00\n",
      "Fecha actual 2022-02-23 00:00:00\n",
      "Fecha actual 2022-02-24 00:00:00\n",
      "Fecha actual 2022-02-25 00:00:00\n",
      "Fecha actual 2022-02-28 00:00:00\n",
      "Fecha actual 2022-03-01 00:00:00\n",
      "Fecha actual 2022-03-02 00:00:00\n",
      "Fecha actual 2022-03-03 00:00:00\n",
      "Fecha actual 2022-03-04 00:00:00\n",
      "Fecha actual 2022-03-07 00:00:00\n",
      "Fecha actual 2022-03-08 00:00:00\n",
      "Fecha actual 2022-03-09 00:00:00\n",
      "Fecha actual 2022-03-10 00:00:00\n",
      "Fecha actual 2022-03-11 00:00:00\n",
      "Fecha actual 2022-03-14 00:00:00\n",
      "Fecha actual 2022-03-15 00:00:00\n",
      "Fecha actual 2022-03-16 00:00:00\n",
      "Fecha actual 2022-03-17 00:00:00\n",
      "Fecha actual 2022-03-18 00:00:00\n",
      "Fecha actual 2022-03-21 00:00:00\n",
      "Fecha actual 2022-03-22 00:00:00\n",
      "Fecha actual 2022-03-23 00:00:00\n",
      "Fecha actual 2022-03-24 00:00:00\n",
      "Fecha actual 2022-03-25 00:00:00\n",
      "Fecha actual 2022-03-28 00:00:00\n",
      "Fecha actual 2022-03-29 00:00:00\n",
      "Fecha actual 2022-03-30 00:00:00\n",
      "Fecha actual 2022-03-31 00:00:00\n",
      "Fecha actual 2022-04-01 00:00:00\n",
      "Fecha actual 2022-04-04 00:00:00\n",
      "Fecha actual 2022-04-05 00:00:00\n",
      "Fecha actual 2022-04-06 00:00:00\n",
      "Fecha actual 2022-04-07 00:00:00\n",
      "Fecha actual 2022-04-08 00:00:00\n",
      "Fecha actual 2022-04-11 00:00:00\n",
      "Fecha actual 2022-04-12 00:00:00\n",
      "Fecha actual 2022-04-13 00:00:00\n",
      "Fecha actual 2022-04-14 00:00:00\n",
      "Fecha actual 2022-04-18 00:00:00\n",
      "Fecha actual 2022-04-19 00:00:00\n",
      "Fecha actual 2022-04-20 00:00:00\n",
      "Fecha actual 2022-04-21 00:00:00\n",
      "Fecha actual 2022-04-22 00:00:00\n",
      "Fecha actual 2022-04-25 00:00:00\n",
      "Fecha actual 2022-04-26 00:00:00\n",
      "Fecha actual 2022-04-27 00:00:00\n",
      "Fecha actual 2022-04-28 00:00:00\n",
      "Fecha actual 2022-04-29 00:00:00\n",
      "Fecha actual 2022-05-02 00:00:00\n",
      "Fecha actual 2022-05-03 00:00:00\n",
      "Fecha actual 2022-05-04 00:00:00\n",
      "Fecha actual 2022-05-05 00:00:00\n",
      "Fecha actual 2022-05-06 00:00:00\n",
      "Fecha actual 2022-05-09 00:00:00\n",
      "Fecha actual 2022-05-10 00:00:00\n",
      "Fecha actual 2022-05-11 00:00:00\n",
      "Fecha actual 2022-05-12 00:00:00\n",
      "Fecha actual 2022-05-13 00:00:00\n",
      "Fecha actual 2022-05-16 00:00:00\n",
      "Fecha actual 2022-05-17 00:00:00\n",
      "Fecha actual 2022-05-18 00:00:00\n",
      "Fecha actual 2022-05-19 00:00:00\n",
      "Fecha actual 2022-05-20 00:00:00\n",
      "Fecha actual 2022-05-23 00:00:00\n",
      "Fecha actual 2022-05-24 00:00:00\n",
      "Fecha actual 2022-05-25 00:00:00\n",
      "Fecha actual 2022-05-26 00:00:00\n",
      "Fecha actual 2022-05-27 00:00:00\n",
      "Fecha actual 2022-05-31 00:00:00\n",
      "Fecha actual 2022-06-01 00:00:00\n",
      "Fecha actual 2022-06-02 00:00:00\n",
      "Fecha actual 2022-06-03 00:00:00\n",
      "Fecha actual 2022-06-06 00:00:00\n",
      "Fecha actual 2022-06-07 00:00:00\n",
      "Fecha actual 2022-06-08 00:00:00\n",
      "Fecha actual 2022-06-09 00:00:00\n",
      "Fecha actual 2022-06-10 00:00:00\n",
      "Fecha actual 2022-06-13 00:00:00\n",
      "Fecha actual 2022-06-14 00:00:00\n",
      "Fecha actual 2022-06-15 00:00:00\n",
      "Fecha actual 2022-06-16 00:00:00\n",
      "Fecha actual 2022-06-17 00:00:00\n",
      "Fecha actual 2022-06-21 00:00:00\n",
      "Fecha actual 2022-06-22 00:00:00\n",
      "Fecha actual 2022-06-23 00:00:00\n",
      "Fecha actual 2022-06-24 00:00:00\n",
      "Fecha actual 2022-06-27 00:00:00\n",
      "Fecha actual 2022-06-28 00:00:00\n",
      "Fecha actual 2022-06-29 00:00:00\n",
      "Fecha actual 2022-06-30 00:00:00\n",
      "Fecha actual 2022-07-01 00:00:00\n",
      "Fecha actual 2022-07-05 00:00:00\n",
      "Fecha actual 2022-07-06 00:00:00\n",
      "Fecha actual 2022-07-07 00:00:00\n",
      "Fecha actual 2022-07-08 00:00:00\n",
      "Fecha actual 2022-07-11 00:00:00\n",
      "Fecha actual 2022-07-12 00:00:00\n",
      "Fecha actual 2022-07-13 00:00:00\n",
      "Fecha actual 2022-07-14 00:00:00\n",
      "Fecha actual 2022-07-15 00:00:00\n",
      "Fecha actual 2022-07-18 00:00:00\n",
      "Fecha actual 2022-07-19 00:00:00\n",
      "Fecha actual 2022-07-20 00:00:00\n",
      "Fecha actual 2022-07-21 00:00:00\n",
      "Fecha actual 2022-07-22 00:00:00\n",
      "Fecha actual 2022-07-25 00:00:00\n",
      "Fecha actual 2022-07-26 00:00:00\n",
      "Fecha actual 2022-07-27 00:00:00\n",
      "Fecha actual 2022-07-28 00:00:00\n",
      "Fecha actual 2022-07-29 00:00:00\n",
      "Fecha actual 2022-08-01 00:00:00\n",
      "Fecha actual 2022-08-02 00:00:00\n",
      "Fecha actual 2022-08-03 00:00:00\n",
      "Fecha actual 2022-08-04 00:00:00\n",
      "Fecha actual 2022-08-05 00:00:00\n",
      "Fecha actual 2022-08-08 00:00:00\n",
      "Fecha actual 2022-08-09 00:00:00\n",
      "Fecha actual 2022-08-10 00:00:00\n",
      "Fecha actual 2022-08-11 00:00:00\n",
      "Fecha actual 2022-08-12 00:00:00\n",
      "Fecha actual 2022-08-15 00:00:00\n",
      "Fecha actual 2022-08-16 00:00:00\n",
      "Fecha actual 2022-08-17 00:00:00\n",
      "Fecha actual 2022-08-18 00:00:00\n",
      "Fecha actual 2022-08-19 00:00:00\n",
      "Fecha actual 2022-08-22 00:00:00\n",
      "Fecha actual 2022-08-23 00:00:00\n",
      "Fecha actual 2022-08-24 00:00:00\n",
      "Fecha actual 2022-08-25 00:00:00\n",
      "Fecha actual 2022-08-26 00:00:00\n",
      "Fecha actual 2022-08-29 00:00:00\n",
      "Fecha actual 2022-08-30 00:00:00\n",
      "Fecha actual 2022-08-31 00:00:00\n",
      "Fecha actual 2022-09-01 00:00:00\n",
      "Fecha actual 2022-09-02 00:00:00\n",
      "Fecha actual 2022-09-06 00:00:00\n",
      "Fecha actual 2022-09-07 00:00:00\n",
      "Fecha actual 2022-09-08 00:00:00\n",
      "Fecha actual 2022-09-09 00:00:00\n",
      "Fecha actual 2022-09-12 00:00:00\n",
      "Fecha actual 2022-09-13 00:00:00\n",
      "Fecha actual 2022-09-14 00:00:00\n",
      "Fecha actual 2022-09-15 00:00:00\n",
      "Fecha actual 2022-09-16 00:00:00\n",
      "Fecha actual 2022-09-19 00:00:00\n",
      "Fecha actual 2022-09-20 00:00:00\n",
      "Fecha actual 2022-09-21 00:00:00\n",
      "Fecha actual 2022-09-22 00:00:00\n",
      "Fecha actual 2022-09-23 00:00:00\n",
      "Fecha actual 2022-09-26 00:00:00\n",
      "Fecha actual 2022-09-27 00:00:00\n",
      "Fecha actual 2022-09-28 00:00:00\n",
      "Fecha actual 2022-09-29 00:00:00\n",
      "Fecha actual 2022-09-30 00:00:00\n",
      "Fecha actual 2022-10-03 00:00:00\n",
      "Fecha actual 2022-10-04 00:00:00\n",
      "Fecha actual 2022-10-05 00:00:00\n",
      "Fecha actual 2022-10-06 00:00:00\n",
      "Fecha actual 2022-10-07 00:00:00\n",
      "Fecha actual 2022-10-10 00:00:00\n",
      "Fecha actual 2022-10-11 00:00:00\n",
      "Fecha actual 2022-10-12 00:00:00\n",
      "Fecha actual 2022-10-13 00:00:00\n",
      "Fecha actual 2022-10-14 00:00:00\n",
      "Fecha actual 2022-10-17 00:00:00\n",
      "Fecha actual 2022-10-18 00:00:00\n",
      "Fecha actual 2022-10-19 00:00:00\n",
      "Fecha actual 2022-10-20 00:00:00\n",
      "Fecha actual 2022-10-21 00:00:00\n",
      "Fecha actual 2022-10-24 00:00:00\n",
      "Fecha actual 2022-10-25 00:00:00\n",
      "Fecha actual 2022-10-26 00:00:00\n",
      "Fecha actual 2022-10-27 00:00:00\n",
      "Fecha actual 2022-10-28 00:00:00\n",
      "Fecha actual 2022-10-31 00:00:00\n",
      "Fecha actual 2022-11-01 00:00:00\n",
      "Fecha actual 2022-11-02 00:00:00\n",
      "Fecha actual 2022-11-03 00:00:00\n",
      "Fecha actual 2022-11-04 00:00:00\n",
      "Fecha actual 2022-11-07 00:00:00\n",
      "Fecha actual 2022-11-08 00:00:00\n",
      "Fecha actual 2022-11-09 00:00:00\n",
      "Fecha actual 2022-11-10 00:00:00\n",
      "Fecha actual 2022-11-11 00:00:00\n",
      "Fecha actual 2022-11-14 00:00:00\n",
      "Fecha actual 2022-11-15 00:00:00\n",
      "Fecha actual 2022-11-16 00:00:00\n",
      "Fecha actual 2022-11-17 00:00:00\n",
      "Fecha actual 2022-11-18 00:00:00\n",
      "Fecha actual 2022-11-21 00:00:00\n",
      "Fecha actual 2022-11-22 00:00:00\n",
      "Fecha actual 2022-11-23 00:00:00\n",
      "Fecha actual 2022-11-25 00:00:00\n",
      "Fecha actual 2022-11-28 00:00:00\n",
      "Fecha actual 2022-11-29 00:00:00\n",
      "Fecha actual 2022-11-30 00:00:00\n",
      "Fecha actual 2022-12-01 00:00:00\n",
      "Fecha actual 2022-12-02 00:00:00\n",
      "Fecha actual 2022-12-05 00:00:00\n",
      "Fecha actual 2022-12-06 00:00:00\n",
      "Fecha actual 2022-12-07 00:00:00\n",
      "Fecha actual 2022-12-08 00:00:00\n",
      "Fecha actual 2022-12-09 00:00:00\n",
      "Fecha actual 2022-12-12 00:00:00\n",
      "Fecha actual 2022-12-13 00:00:00\n",
      "Fecha actual 2022-12-14 00:00:00\n",
      "Fecha actual 2022-12-15 00:00:00\n",
      "Fecha actual 2022-12-16 00:00:00\n",
      "Fecha actual 2022-12-19 00:00:00\n",
      "Fecha actual 2022-12-20 00:00:00\n",
      "Fecha actual 2022-12-21 00:00:00\n",
      "Fecha actual 2022-12-22 00:00:00\n",
      "Fecha actual 2022-12-23 00:00:00\n",
      "Fecha actual 2022-12-27 00:00:00\n",
      "Fecha actual 2022-12-28 00:00:00\n",
      "Fecha actual 2022-12-29 00:00:00\n",
      "Fecha actual 2022-12-30 00:00:00\n",
      "Fecha actual 2023-01-03 00:00:00\n",
      "Fecha actual 2023-01-04 00:00:00\n",
      "Fecha actual 2023-01-05 00:00:00\n",
      "Fecha actual 2023-01-06 00:00:00\n",
      "Fecha actual 2023-01-09 00:00:00\n",
      "Fecha actual 2023-01-10 00:00:00\n",
      "Fecha actual 2023-01-11 00:00:00\n",
      "Fecha actual 2023-01-12 00:00:00\n",
      "Fecha actual 2023-01-13 00:00:00\n",
      "Fecha actual 2023-01-17 00:00:00\n",
      "Fecha actual 2023-01-18 00:00:00\n",
      "Fecha actual 2023-01-19 00:00:00\n",
      "Fecha actual 2023-01-20 00:00:00\n",
      "Fecha actual 2023-01-23 00:00:00\n",
      "Fecha actual 2023-01-24 00:00:00\n",
      "Fecha actual 2023-01-25 00:00:00\n",
      "Fecha actual 2023-01-26 00:00:00\n",
      "Fecha actual 2023-01-27 00:00:00\n",
      "Fecha actual 2023-01-30 00:00:00\n",
      "Fecha actual 2023-01-31 00:00:00\n",
      "Fecha actual 2023-02-01 00:00:00\n",
      "Fecha actual 2023-02-02 00:00:00\n",
      "Fecha actual 2023-02-03 00:00:00\n",
      "Fecha actual 2023-02-06 00:00:00\n",
      "Fecha actual 2023-02-07 00:00:00\n",
      "Fecha actual 2023-02-08 00:00:00\n",
      "Fecha actual 2023-02-09 00:00:00\n",
      "Fecha actual 2023-02-10 00:00:00\n",
      "Fecha actual 2023-02-13 00:00:00\n",
      "Fecha actual 2023-02-14 00:00:00\n",
      "Fecha actual 2023-02-15 00:00:00\n",
      "Fecha actual 2023-02-16 00:00:00\n",
      "Fecha actual 2023-02-17 00:00:00\n",
      "Fecha actual 2023-02-21 00:00:00\n",
      "Fecha actual 2023-02-22 00:00:00\n",
      "Fecha actual 2023-02-23 00:00:00\n",
      "Fecha actual 2023-02-24 00:00:00\n",
      "Fecha actual 2023-02-27 00:00:00\n",
      "Fecha actual 2023-02-28 00:00:00\n",
      "Fecha actual 2023-03-01 00:00:00\n",
      "Fecha actual 2023-03-02 00:00:00\n",
      "Fecha actual 2023-03-03 00:00:00\n",
      "Fecha actual 2023-03-06 00:00:00\n",
      "Fecha actual 2023-03-07 00:00:00\n",
      "Fecha actual 2023-03-08 00:00:00\n",
      "Fecha actual 2023-03-09 00:00:00\n",
      "Fecha actual 2023-03-10 00:00:00\n",
      "Fecha actual 2023-03-13 00:00:00\n",
      "Fecha actual 2023-03-14 00:00:00\n",
      "Fecha actual 2023-03-15 00:00:00\n",
      "Fecha actual 2023-03-16 00:00:00\n",
      "Fecha actual 2023-03-17 00:00:00\n",
      "Fecha actual 2023-03-20 00:00:00\n",
      "Fecha actual 2023-03-21 00:00:00\n",
      "Fecha actual 2023-03-22 00:00:00\n",
      "Fecha actual 2023-03-23 00:00:00\n",
      "Fecha actual 2023-03-24 00:00:00\n",
      "Fecha actual 2023-03-27 00:00:00\n",
      "Fecha actual 2023-03-28 00:00:00\n",
      "Fecha actual 2023-03-29 00:00:00\n",
      "Fecha actual 2023-03-30 00:00:00\n",
      "Fecha actual 2023-03-31 00:00:00\n",
      "Fecha actual 2023-04-03 00:00:00\n",
      "Fecha actual 2023-04-04 00:00:00\n",
      "Fecha actual 2023-04-05 00:00:00\n",
      "Fecha actual 2023-04-06 00:00:00\n",
      "Fecha actual 2023-04-10 00:00:00\n",
      "Fecha actual 2023-04-11 00:00:00\n",
      "Fecha actual 2023-04-12 00:00:00\n",
      "Fecha actual 2023-04-13 00:00:00\n",
      "Fecha actual 2023-04-14 00:00:00\n",
      "Fecha actual 2023-04-17 00:00:00\n",
      "Fecha actual 2023-04-18 00:00:00\n",
      "Fecha actual 2023-04-19 00:00:00\n",
      "Fecha actual 2023-04-20 00:00:00\n",
      "Fecha actual 2023-04-21 00:00:00\n",
      "Fecha actual 2023-04-24 00:00:00\n",
      "Fecha actual 2023-04-25 00:00:00\n",
      "Fecha actual 2023-04-26 00:00:00\n",
      "Fecha actual 2023-04-27 00:00:00\n",
      "Fecha actual 2023-04-28 00:00:00\n",
      "Fecha actual 2023-05-01 00:00:00\n",
      "Fecha actual 2023-05-02 00:00:00\n",
      "Fecha actual 2023-05-03 00:00:00\n",
      "Fecha actual 2023-05-04 00:00:00\n",
      "Fecha actual 2023-05-05 00:00:00\n",
      "Fecha actual 2023-05-08 00:00:00\n",
      "Fecha actual 2023-05-09 00:00:00\n",
      "Fecha actual 2023-05-10 00:00:00\n",
      "Fecha actual 2023-05-11 00:00:00\n",
      "Fecha actual 2023-05-12 00:00:00\n",
      "Fecha actual 2023-05-15 00:00:00\n",
      "Fecha actual 2023-05-16 00:00:00\n",
      "Fecha actual 2023-05-17 00:00:00\n",
      "Fecha actual 2023-05-18 00:00:00\n",
      "Fecha actual 2023-05-19 00:00:00\n",
      "Fecha actual 2023-05-22 00:00:00\n",
      "Fecha actual 2023-05-23 00:00:00\n",
      "Fecha actual 2023-05-24 00:00:00\n",
      "Fecha actual 2023-05-25 00:00:00\n",
      "Fecha actual 2023-05-26 00:00:00\n",
      "Fecha actual 2023-05-30 00:00:00\n",
      "Fecha actual 2023-05-31 00:00:00\n",
      "Fecha actual 2023-06-01 00:00:00\n",
      "Fecha actual 2023-06-02 00:00:00\n",
      "Fecha actual 2023-06-05 00:00:00\n",
      "Fecha actual 2023-06-06 00:00:00\n",
      "Fecha actual 2023-06-07 00:00:00\n",
      "Fecha actual 2023-06-08 00:00:00\n",
      "Fecha actual 2023-06-09 00:00:00\n",
      "Fecha actual 2023-06-12 00:00:00\n",
      "Fecha actual 2023-06-13 00:00:00\n",
      "Fecha actual 2023-06-14 00:00:00\n",
      "Fecha actual 2023-06-15 00:00:00\n",
      "Fecha actual 2023-06-16 00:00:00\n",
      "Fecha actual 2023-06-20 00:00:00\n",
      "Fecha actual 2023-06-21 00:00:00\n",
      "Fecha actual 2023-06-22 00:00:00\n",
      "Fecha actual 2023-06-23 00:00:00\n",
      "Fecha actual 2023-06-26 00:00:00\n",
      "Fecha actual 2023-06-27 00:00:00\n",
      "Fecha actual 2023-06-28 00:00:00\n",
      "Fecha actual 2023-06-29 00:00:00\n",
      "Fecha actual 2023-06-30 00:00:00\n",
      "Fecha actual 2023-07-03 00:00:00\n",
      "Fecha actual 2023-07-05 00:00:00\n",
      "Fecha actual 2023-07-06 00:00:00\n",
      "Fecha actual 2023-07-07 00:00:00\n",
      "Fecha actual 2023-07-10 00:00:00\n",
      "Fecha actual 2023-07-11 00:00:00\n",
      "Fecha actual 2023-07-12 00:00:00\n",
      "Fecha actual 2023-07-13 00:00:00\n",
      "Fecha actual 2023-07-14 00:00:00\n",
      "Fecha actual 2023-07-17 00:00:00\n",
      "Fecha actual 2023-07-18 00:00:00\n",
      "Fecha actual 2023-07-19 00:00:00\n",
      "Fecha actual 2023-07-20 00:00:00\n",
      "Fecha actual 2023-07-21 00:00:00\n",
      "Fecha actual 2023-07-24 00:00:00\n",
      "Fecha actual 2023-07-25 00:00:00\n",
      "Fecha actual 2023-07-26 00:00:00\n",
      "Fecha actual 2023-07-27 00:00:00\n",
      "Fecha actual 2023-07-28 00:00:00\n",
      "Fecha actual 2023-07-31 00:00:00\n",
      "Fecha actual 2023-08-01 00:00:00\n",
      "Fecha actual 2023-08-02 00:00:00\n",
      "Fecha actual 2023-08-03 00:00:00\n",
      "Fecha actual 2023-08-04 00:00:00\n",
      "Fecha actual 2023-08-07 00:00:00\n",
      "Fecha actual 2023-08-08 00:00:00\n",
      "Fecha actual 2023-08-09 00:00:00\n",
      "Fecha actual 2023-08-10 00:00:00\n",
      "Fecha actual 2023-08-11 00:00:00\n",
      "Fecha actual 2023-08-14 00:00:00\n",
      "Fecha actual 2023-08-15 00:00:00\n",
      "Fecha actual 2023-08-16 00:00:00\n",
      "Fecha actual 2023-08-17 00:00:00\n",
      "Fecha actual 2023-08-18 00:00:00\n",
      "Fecha actual 2023-08-21 00:00:00\n",
      "Fecha actual 2023-08-22 00:00:00\n",
      "Fecha actual 2023-08-23 00:00:00\n",
      "Fecha actual 2023-08-24 00:00:00\n",
      "Fecha actual 2023-08-25 00:00:00\n",
      "Fecha actual 2023-08-28 00:00:00\n",
      "Fecha actual 2023-08-29 00:00:00\n",
      "Fecha actual 2023-08-30 00:00:00\n",
      "Fecha actual 2023-08-31 00:00:00\n",
      "Fecha actual 2023-09-01 00:00:00\n",
      "Fecha actual 2023-09-05 00:00:00\n",
      "Fecha actual 2023-09-06 00:00:00\n",
      "Fecha actual 2023-09-07 00:00:00\n",
      "Fecha actual 2023-09-08 00:00:00\n",
      "Fecha actual 2023-09-11 00:00:00\n",
      "Fecha actual 2023-09-12 00:00:00\n",
      "Fecha actual 2023-09-13 00:00:00\n",
      "Fecha actual 2023-09-14 00:00:00\n",
      "Fecha actual 2023-09-15 00:00:00\n",
      "Fecha actual 2023-09-18 00:00:00\n",
      "Fecha actual 2023-09-19 00:00:00\n",
      "Fecha actual 2023-09-20 00:00:00\n",
      "Fecha actual 2023-09-21 00:00:00\n",
      "Fecha actual 2023-09-22 00:00:00\n",
      "Fecha actual 2023-09-25 00:00:00\n",
      "Fecha actual 2023-09-26 00:00:00\n",
      "Fecha actual 2023-09-27 00:00:00\n",
      "Fecha actual 2023-09-28 00:00:00\n",
      "Fecha actual 2023-09-29 00:00:00\n",
      "Fecha actual 2023-10-02 00:00:00\n",
      "Fecha actual 2023-10-03 00:00:00\n",
      "Fecha actual 2023-10-04 00:00:00\n",
      "Fecha actual 2023-10-05 00:00:00\n",
      "Fecha actual 2023-10-06 00:00:00\n",
      "Fecha actual 2023-10-09 00:00:00\n",
      "Fecha actual 2023-10-10 00:00:00\n",
      "Fecha actual 2023-10-11 00:00:00\n",
      "Fecha actual 2023-10-12 00:00:00\n",
      "Fecha actual 2023-10-13 00:00:00\n",
      "Fecha actual 2023-10-16 00:00:00\n",
      "Fecha actual 2023-10-17 00:00:00\n",
      "Fecha actual 2023-10-18 00:00:00\n",
      "Fecha actual 2023-10-19 00:00:00\n",
      "Fecha actual 2023-10-20 00:00:00\n",
      "Fecha actual 2023-10-23 00:00:00\n",
      "Fecha actual 2023-10-24 00:00:00\n",
      "Fecha actual 2023-10-25 00:00:00\n",
      "Fecha actual 2023-10-26 00:00:00\n",
      "Fecha actual 2023-10-27 00:00:00\n",
      "Fecha actual 2023-10-30 00:00:00\n",
      "Fecha actual 2023-10-31 00:00:00\n",
      "Fecha actual 2023-11-01 00:00:00\n",
      "Fecha actual 2023-11-02 00:00:00\n",
      "Fecha actual 2023-11-03 00:00:00\n",
      "Fecha actual 2023-11-06 00:00:00\n",
      "Fecha actual 2023-11-07 00:00:00\n",
      "Fecha actual 2023-11-08 00:00:00\n",
      "Fecha actual 2023-11-09 00:00:00\n",
      "Fecha actual 2023-11-10 00:00:00\n",
      "Fecha actual 2023-11-13 00:00:00\n",
      "Fecha actual 2023-11-14 00:00:00\n",
      "Fecha actual 2023-11-15 00:00:00\n",
      "Fecha actual 2023-11-16 00:00:00\n",
      "Fecha actual 2023-11-17 00:00:00\n",
      "Fecha actual 2023-11-20 00:00:00\n",
      "Fecha actual 2023-11-21 00:00:00\n",
      "Fecha actual 2023-11-22 00:00:00\n",
      "Fecha actual 2023-11-24 00:00:00\n",
      "Fecha actual 2023-11-27 00:00:00\n",
      "Fecha actual 2023-11-28 00:00:00\n",
      "Fecha actual 2023-11-29 00:00:00\n",
      "Fecha actual 2023-11-30 00:00:00\n",
      "Fecha actual 2023-12-01 00:00:00\n",
      "Fecha actual 2023-12-04 00:00:00\n",
      "Fecha actual 2023-12-05 00:00:00\n",
      "Fecha actual 2023-12-06 00:00:00\n",
      "Fecha actual 2023-12-07 00:00:00\n",
      "Fecha actual 2023-12-08 00:00:00\n",
      "Fecha actual 2023-12-11 00:00:00\n",
      "Fecha actual 2023-12-12 00:00:00\n",
      "Fecha actual 2023-12-13 00:00:00\n",
      "Fecha actual 2023-12-14 00:00:00\n",
      "Fecha actual 2023-12-15 00:00:00\n",
      "Fecha actual 2023-12-18 00:00:00\n",
      "Fecha actual 2023-12-19 00:00:00\n",
      "Fecha actual 2023-12-20 00:00:00\n",
      "Fecha actual 2023-12-21 00:00:00\n",
      "Fecha actual 2023-12-22 00:00:00\n",
      "Fecha actual 2023-12-26 00:00:00\n",
      "Fecha actual 2023-12-27 00:00:00\n",
      "Fecha actual 2023-12-28 00:00:00\n",
      "Fecha actual 2023-12-29 00:00:00\n",
      "Num Empresas 50\n",
      "Num Fechas  20\n",
      "Num Características Empresa 19\n",
      "Num Características Macro 31\n",
      "Fecha actual 2024-01-02 00:00:00\n",
      "Fecha actual 2024-01-03 00:00:00\n",
      "Fecha actual 2024-01-04 00:00:00\n",
      "Fecha actual 2024-01-05 00:00:00\n",
      "Fecha actual 2024-01-08 00:00:00\n",
      "Fecha actual 2024-01-09 00:00:00\n",
      "Fecha actual 2024-01-10 00:00:00\n",
      "Fecha actual 2024-01-11 00:00:00\n",
      "Fecha actual 2024-01-12 00:00:00\n",
      "Fecha actual 2024-01-16 00:00:00\n",
      "Fecha actual 2024-01-17 00:00:00\n",
      "Fecha actual 2024-01-18 00:00:00\n",
      "Fecha actual 2024-01-19 00:00:00\n",
      "Fecha actual 2024-01-22 00:00:00\n",
      "Fecha actual 2024-01-23 00:00:00\n",
      "Fecha actual 2024-01-24 00:00:00\n",
      "Fecha actual 2024-01-25 00:00:00\n",
      "Fecha actual 2024-01-26 00:00:00\n",
      "Fecha actual 2024-01-29 00:00:00\n",
      "Fecha actual 2024-01-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Listado de empresas y columnas macroeconómicas\n",
    "empresas = sorted(set(col.split('_')[-1] for col in train.columns if '_' in col))\n",
    "empresas=empresas[:-1]\n",
    "columnas_macro = [\n",
    "    'GDP',\n",
    "    'UNRATE',\n",
    "    'CPIAUCSL',\n",
    "    'PAYEMS',\n",
    "    'FEDFUNDS',\n",
    "    'DGS10',\n",
    "    'M1SL',\n",
    "    'M2SL',\n",
    "    '^GSPC',\n",
    "    'INDPRO',\n",
    "    'RSAFS',\n",
    "    'EXCAUS',\n",
    "    'BOPGSTB',\n",
    "    'GFDEBTN',\n",
    "    'FGEXPND',\n",
    "    'PCEPI',\n",
    "    'PPIACO',\n",
    "    '^IXIC',\n",
    "    '^RUT',\n",
    "    '^STOXX50E',\n",
    "    '^FTSE',\n",
    "    'CL=F',\n",
    "    'SI=F',\n",
    "    'GC=F',\n",
    "    '^HSI',\n",
    "    'NG=F',\n",
    "    'ZC=F',\n",
    "    'EURUSD=X',\n",
    "    'BTC-USD',\n",
    "    'HO=F',\n",
    "    'ZC=F'\n",
    "]\n",
    "\n",
    "# Transformar conjuntos de entrenamiento y prueba\n",
    "tensor_train = transformar_a_tensor_3d(train, empresas, columnas_macro)\n",
    "tensor_test = transformar_a_tensor_3d(test, empresas, columnas_macro)\n",
    "\n",
    "# # Guardar los tensores\n",
    "# np.save('tensor_train.npy', tensor_train)\n",
    "# np.save('tensor_test.npy', tensor_test)\n",
    "\n",
    "# print(\"Tensores guardados exitosamente.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ALEX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3616, 500, 50)\n",
      "Tensores cargados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# # Cargar los tensores\n",
    "# tensor_train_cargado = np.load('tensor_train.npy')\n",
    "# tensor_test_cargado = np.load('tensor_test.npy')\n",
    "# print(tensor_train_cargado.shape)\n",
    "# print(\"Tensores cargados exitosamente.\")\n",
    "# tensor_train=tensor_train_cargado\n",
    "# tensor_test=tensor_test_cargado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Aplanar los datos de entrenamiento para ajustar el escalador\n",
    "# train_2d = tensor_train.reshape(-1, tensor_train.shape[-1])\n",
    "# scaler = StandardScaler()\n",
    "# train_2d_scaled = scaler.fit_transform(train_2d)\n",
    "\n",
    "# # Escalar el conjunto de prueba utilizando el escalador ajustado\n",
    "# test_2d = tensor_test.reshape(-1, tensor_test.shape[-1])\n",
    "# test_2d_scaled = scaler.transform(test_2d)\n",
    "\n",
    "# # Volver a la forma 3D\n",
    "# tensor_train_scaled = train_2d_scaled.reshape(tensor_train.shape)\n",
    "# tensor_test_scaled = test_2d_scaled.reshape(tensor_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating temporal windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (3470, 30, 50, 50)\n",
      "Forma de y_train: (3470, 50)\n",
      "Forma de X_test: (14, 5, 50, 50)\n",
      "Forma de y_test: (14, 50)\n",
      "Forma de X_train: (3470, 30, 50, 50)\n",
      "Forma de y_train: (3470, 50)\n",
      "Forma de X_test: (14, 5, 50, 50)\n",
      "Forma de y_test: (14, 50)\n"
     ]
    }
   ],
   "source": [
    "ventana = 30  # Días de entrada\n",
    "horizonte = 22  # Días a predecir\n",
    "\n",
    "# Crear ventanas para entrenamiento y prueba\n",
    "X_train, y_train = crear_ventanas_temporales(tensor_train, ventana=ventana, horizonte=horizonte)\n",
    "X_test, y_test = crear_ventanas_temporales(tensor_test, ventana=5, horizonte=1)\n",
    "\n",
    "print(\"Forma de X_train:\", X_train.shape)\n",
    "print(\"Forma de y_train:\", y_train.shape)\n",
    "print(\"Forma de X_test:\", X_test.shape)\n",
    "print(\"Forma de y_test:\", y_test.shape)\n",
    "\n",
    "\n",
    "# Aplanar X_train para ajustar el escalador\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])  # Aplanar: (n_samples * ventana * n_empresas, n_features)\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled_2d = scaler_X.fit_transform(X_train_2d)\n",
    "\n",
    "# Volver a la forma original\n",
    "X_train = X_train_scaled_2d.reshape(X_train.shape)\n",
    "\n",
    "# Escalar X_test con el mismo escalador\n",
    "X_test_2d = X_test.reshape(-1, X_test.shape[-1])  # Aplanar\n",
    "X_test_scaled_2d = scaler_X.transform(X_test_2d)\n",
    "X_test = X_test_scaled_2d.reshape(X_test.shape)\n",
    "\n",
    "# Ajustar escalador para y_train\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "\n",
    "# Escalar y_test con el mismo escalador\n",
    "y_test= scaler_y.transform(y_test)\n",
    "\n",
    "print(\"Forma de X_train:\", X_train.shape)\n",
    "print(\"Forma de y_train:\", y_train.shape)\n",
    "print(\"Forma de X_test:\", X_test.shape)\n",
    "print(\"Forma de y_test:\", y_test.shape)\n",
    "\n",
    "\n",
    "# # Supongamos que las salidas son las primeras 2 columnas del tensor\n",
    "# n_output_features = y_train.shape[1]  # Número de salidas\n",
    "# output_columns = slice(0, n_output_features)  # Índice de las columnas correspondientes a y\n",
    "\n",
    "# # Extraer media y escala específicas para las salidas\n",
    "# output_mean = scaler.mean_[output_columns]\n",
    "# output_scale = scaler.scale_[output_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Learning Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: torch.Size([3470, 30, 50, 50])\n",
      "Forma de y_train: torch.Size([3470, 50])\n",
      "Forma de X_test: torch.Size([14, 5, 50, 50])\n",
      "Forma de y_test: torch.Size([14, 50])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convertir datos a tensores\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)  # (3472, 30, 100)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)  # (3472, 2)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)  # (16, 3, 100)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)  # (16, 2)\n",
    "\n",
    "print(\"Forma de X_train:\", X_train_tensor.shape)\n",
    "print(\"Forma de y_train:\", y_train_tensor.shape)\n",
    "print(\"Forma de X_test:\", X_test_tensor.shape)\n",
    "print(\"Forma de y_test:\", y_test_tensor.shape)\n",
    "\n",
    "X_train_tensor = X_train_tensor.view(\n",
    "    X_train_tensor.shape[0],  # batch_size\n",
    "    X_train_tensor.shape[1],  # seq_length\n",
    "    -1  # Flatten (n_companies * features_per_company)\n",
    ")\n",
    "\n",
    "X_test_tensor = X_test_tensor.view(\n",
    "    X_test_tensor.shape[0], \n",
    "    X_test_tensor.shape[1], \n",
    "    -1\n",
    ")\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Crear dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTSM original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo LSTM con configuración 30_22_128_4_0.65_50_0.0001_0.0001_adam_MSE\n",
      "Epoch 1/50, Train Loss: 0.9591, Valid Loss: 2.9084, Train RMSE: 0.9748, Valid RMSE: 1.7054, Train R²: 0.0497, Valid R²: -729.6248Train MPE: 0.3924, Valid MPE: 0.0752, Train MAPE: 2.0782, Valid MAPE: 0.9153\n",
      "Epoch 2/50, Train Loss: 0.6422, Valid Loss: 1.7037, Train RMSE: 0.7988, Valid RMSE: 1.3053, Train R²: 0.3619, Valid R²: -424.1813Train MPE: 0.3151, Valid MPE: 0.1200, Train MAPE: 2.9297, Valid MAPE: 0.8090\n",
      "Epoch 3/50, Train Loss: 0.5072, Valid Loss: 1.6298, Train RMSE: 0.7099, Valid RMSE: 1.2766, Train R²: 0.4961, Valid R²: -396.1164Train MPE: 0.2691, Valid MPE: 0.1129, Train MAPE: 5.8109, Valid MAPE: 0.7681\n",
      "Epoch 4/50, Train Loss: 0.4140, Valid Loss: 1.5707, Train RMSE: 0.6411, Valid RMSE: 1.2533, Train R²: 0.5890, Valid R²: -381.6360Train MPE: 0.2597, Valid MPE: 0.1170, Train MAPE: 3.4261, Valid MAPE: 0.7567\n",
      "Epoch 5/50, Train Loss: 0.3674, Valid Loss: 1.3297, Train RMSE: 0.6039, Valid RMSE: 1.1531, Train R²: 0.6353, Valid R²: -328.0719Train MPE: 0.2320, Valid MPE: 0.1447, Train MAPE: 3.2589, Valid MAPE: 0.7628\n",
      "Epoch 6/50, Train Loss: 0.3399, Valid Loss: 1.4244, Train RMSE: 0.5805, Valid RMSE: 1.1935, Train R²: 0.6630, Valid R²: -347.8833Train MPE: 0.2174, Valid MPE: 0.1339, Train MAPE: 3.4923, Valid MAPE: 0.7612\n",
      "Epoch 7/50, Train Loss: 0.3209, Valid Loss: 1.4352, Train RMSE: 0.5638, Valid RMSE: 1.1980, Train R²: 0.6822, Valid R²: -350.5711Train MPE: 0.1917, Valid MPE: 0.1289, Train MAPE: 2.7643, Valid MAPE: 0.7526\n",
      "Epoch 8/50, Train Loss: 0.3031, Valid Loss: 1.4286, Train RMSE: 0.5481, Valid RMSE: 1.1952, Train R²: 0.6996, Valid R²: -349.5450Train MPE: 0.2109, Valid MPE: 0.1329, Train MAPE: 2.1434, Valid MAPE: 0.7616\n",
      "Epoch 9/50, Train Loss: 0.2890, Valid Loss: 1.4197, Train RMSE: 0.5349, Valid RMSE: 1.1915, Train R²: 0.7139, Valid R²: -348.9899Train MPE: 0.1754, Valid MPE: 0.1287, Train MAPE: 3.3428, Valid MAPE: 0.7512\n",
      "Epoch 10/50, Train Loss: 0.2893, Valid Loss: 1.3714, Train RMSE: 0.5352, Valid RMSE: 1.1711, Train R²: 0.7136, Valid R²: -337.1179Train MPE: 0.2071, Valid MPE: 0.1377, Train MAPE: 2.1141, Valid MAPE: 0.7610\n",
      "Epoch 11/50, Train Loss: 0.2751, Valid Loss: 1.4557, Train RMSE: 0.5220, Valid RMSE: 1.2065, Train R²: 0.7275, Valid R²: -356.3759Train MPE: 0.1714, Valid MPE: 0.1239, Train MAPE: 3.9413, Valid MAPE: 0.7485\n",
      "Epoch 12/50, Train Loss: 0.2806, Valid Loss: 1.4693, Train RMSE: 0.5273, Valid RMSE: 1.2121, Train R²: 0.7219, Valid R²: -358.7538Train MPE: 0.2004, Valid MPE: 0.1233, Train MAPE: 1.9689, Valid MAPE: 0.7504\n",
      "Epoch 13/50, Train Loss: 0.2660, Valid Loss: 1.3213, Train RMSE: 0.5135, Valid RMSE: 1.1495, Train R²: 0.7363, Valid R²: -327.0201Train MPE: 0.1676, Valid MPE: 0.1370, Train MAPE: 3.2657, Valid MAPE: 0.7469\n",
      "Epoch 14/50, Train Loss: 0.2590, Valid Loss: 1.3171, Train RMSE: 0.5065, Valid RMSE: 1.1476, Train R²: 0.7435, Valid R²: -325.2633Train MPE: 0.2005, Valid MPE: 0.1360, Train MAPE: 3.3675, Valid MAPE: 0.7414\n",
      "Epoch 15/50, Train Loss: 0.2569, Valid Loss: 1.4832, Train RMSE: 0.5044, Valid RMSE: 1.2179, Train R²: 0.7456, Valid R²: -363.2659Train MPE: 0.1459, Valid MPE: 0.1150, Train MAPE: 3.6119, Valid MAPE: 0.7329\n",
      "Epoch 16/50, Train Loss: 0.2616, Valid Loss: 1.4543, Train RMSE: 0.5088, Valid RMSE: 1.2059, Train R²: 0.7411, Valid R²: -355.4906Train MPE: 0.2143, Valid MPE: 0.1185, Train MAPE: 2.1261, Valid MAPE: 0.7333\n",
      "Epoch 17/50, Train Loss: 0.2419, Valid Loss: 1.4852, Train RMSE: 0.4889, Valid RMSE: 1.2187, Train R²: 0.7609, Valid R²: -363.9320Train MPE: 0.1504, Valid MPE: 0.1063, Train MAPE: 3.1931, Valid MAPE: 0.7132\n",
      "Epoch 18/50, Train Loss: 0.2422, Valid Loss: 1.5842, Train RMSE: 0.4896, Valid RMSE: 1.2586, Train R²: 0.7602, Valid R²: -383.0775Train MPE: 0.1963, Valid MPE: 0.1072, Train MAPE: 3.1037, Valid MAPE: 0.7332\n",
      "Epoch 19/50, Train Loss: 0.2326, Valid Loss: 1.1161, Train RMSE: 0.4797, Valid RMSE: 1.0565, Train R²: 0.7699, Valid R²: -282.4072Train MPE: 0.1461, Valid MPE: 0.1452, Train MAPE: 2.3803, Valid MAPE: 0.7154\n",
      "Epoch 20/50, Train Loss: 0.2421, Valid Loss: 1.1885, Train RMSE: 0.4897, Valid RMSE: 1.0902, Train R²: 0.7602, Valid R²: -294.9539Train MPE: 0.1928, Valid MPE: 0.1367, Train MAPE: 2.2899, Valid MAPE: 0.7142\n",
      "Epoch 21/50, Train Loss: 0.2472, Valid Loss: 1.3254, Train RMSE: 0.4949, Valid RMSE: 1.1513, Train R²: 0.7551, Valid R²: -327.6217Train MPE: 0.1358, Valid MPE: 0.1218, Train MAPE: 2.8097, Valid MAPE: 0.7124\n",
      "Epoch 22/50, Train Loss: 0.2492, Valid Loss: 1.1384, Train RMSE: 0.4962, Valid RMSE: 1.0670, Train R²: 0.7537, Valid R²: -280.8456Train MPE: 0.2105, Valid MPE: 0.1277, Train MAPE: 2.2757, Valid MAPE: 0.6899\n",
      "Epoch 23/50, Train Loss: 0.2306, Valid Loss: 1.0716, Train RMSE: 0.4782, Valid RMSE: 1.0352, Train R²: 0.7714, Valid R²: -271.7013Train MPE: 0.1413, Valid MPE: 0.1514, Train MAPE: 2.2419, Valid MAPE: 0.7215\n",
      "Epoch 24/50, Train Loss: 0.2232, Valid Loss: 1.0934, Train RMSE: 0.4705, Valid RMSE: 1.0456, Train R²: 0.7786, Valid R²: -273.4089Train MPE: 0.1979, Valid MPE: 0.1420, Train MAPE: 2.4720, Valid MAPE: 0.7107\n",
      "Epoch 25/50, Train Loss: 0.2058, Valid Loss: 1.0092, Train RMSE: 0.4512, Valid RMSE: 1.0046, Train R²: 0.7965, Valid R²: -255.5592Train MPE: 0.1353, Valid MPE: 0.1477, Train MAPE: 2.4761, Valid MAPE: 0.7010\n",
      "Epoch 26/50, Train Loss: 0.2111, Valid Loss: 1.2059, Train RMSE: 0.4573, Valid RMSE: 1.0981, Train R²: 0.7909, Valid R²: -295.3258Train MPE: 0.1930, Valid MPE: 0.1285, Train MAPE: 3.1734, Valid MAPE: 0.7068\n",
      "Epoch 27/50, Train Loss: 0.2352, Valid Loss: 0.9126, Train RMSE: 0.4829, Valid RMSE: 0.9553, Train R²: 0.7668, Valid R²: -233.6907Train MPE: 0.1214, Valid MPE: 0.1495, Train MAPE: 2.8922, Valid MAPE: 0.6867\n",
      "Epoch 28/50, Train Loss: 0.2321, Valid Loss: 1.1776, Train RMSE: 0.4798, Valid RMSE: 1.0852, Train R²: 0.7698, Valid R²: -291.5289Train MPE: 0.2129, Valid MPE: 0.1408, Train MAPE: 3.1639, Valid MAPE: 0.7249\n",
      "Epoch 29/50, Train Loss: 0.2457, Valid Loss: 1.2037, Train RMSE: 0.4934, Valid RMSE: 1.0972, Train R²: 0.7565, Valid R²: -298.8368Train MPE: 0.1259, Valid MPE: 0.1305, Train MAPE: 4.1259, Valid MAPE: 0.7095\n",
      "Epoch 30/50, Train Loss: 0.2218, Valid Loss: 1.1029, Train RMSE: 0.4692, Valid RMSE: 1.0502, Train R²: 0.7799, Valid R²: -276.7014Train MPE: 0.2134, Valid MPE: 0.1519, Train MAPE: 2.7391, Valid MAPE: 0.7283\n",
      "Epoch 31/50, Train Loss: 0.2270, Valid Loss: 1.1583, Train RMSE: 0.4749, Valid RMSE: 1.0762, Train R²: 0.7745, Valid R²: -286.9447Train MPE: 0.1239, Valid MPE: 0.1239, Train MAPE: 3.1864, Valid MAPE: 0.6954\n",
      "Epoch 32/50, Train Loss: 0.2202, Valid Loss: 1.1923, Train RMSE: 0.4669, Valid RMSE: 1.0919, Train R²: 0.7820, Valid R²: -293.3223Train MPE: 0.2147, Valid MPE: 0.1334, Train MAPE: 2.3320, Valid MAPE: 0.7157\n",
      "Epoch 33/50, Train Loss: 0.1933, Valid Loss: 1.0727, Train RMSE: 0.4377, Valid RMSE: 1.0357, Train R²: 0.8084, Valid R²: -271.9982Train MPE: 0.1320, Valid MPE: 0.1537, Train MAPE: 3.1028, Valid MAPE: 0.7310\n",
      "Epoch 34/50, Train Loss: 0.1920, Valid Loss: 1.1668, Train RMSE: 0.4355, Valid RMSE: 1.0802, Train R²: 0.8103, Valid R²: -287.3469Train MPE: 0.1835, Valid MPE: 0.1359, Train MAPE: 3.1058, Valid MAPE: 0.7165\n",
      "Epoch 35/50, Train Loss: 0.1906, Valid Loss: 1.1388, Train RMSE: 0.4346, Valid RMSE: 1.0671, Train R²: 0.8111, Valid R²: -282.3199Train MPE: 0.1260, Valid MPE: 0.1317, Train MAPE: 2.3704, Valid MAPE: 0.7052\n",
      "Epoch 36/50, Train Loss: 0.1881, Valid Loss: 1.1693, Train RMSE: 0.4318, Valid RMSE: 1.0813, Train R²: 0.8135, Valid R²: -287.7956Train MPE: 0.1868, Valid MPE: 0.1366, Train MAPE: 2.4172, Valid MAPE: 0.7164\n",
      "Epoch 37/50, Train Loss: 0.1876, Valid Loss: 1.1447, Train RMSE: 0.4309, Valid RMSE: 1.0699, Train R²: 0.8144, Valid R²: -282.1144Train MPE: 0.1245, Valid MPE: 0.1328, Train MAPE: 2.3802, Valid MAPE: 0.7064\n",
      "Early stopping triggered at epoch 37\n",
      "Base path: C:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\Proyect2_LABIACD\\plots\n",
      "Filename: metrics\\LSTM\\50\\30_22_128_4_0.65_50_0.0001_0.0001_adam_MSE.png\n",
      "Save path: C:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\Proyect2_LABIACD\\plots\\metrics\\LSTM\\50\\30_22_128_4_0.65_50_0.0001_0.0001_adam_MSE.png\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'train_mae'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 54\u001b[0m\n\u001b[0;32m     46\u001b[0m trained_model, preds, actuals,metrics \u001b[38;5;241m=\u001b[39m train_and_evaluate_model(\n\u001b[0;32m     47\u001b[0m     model, train_loader, test_loader, num_epochs, learning_rate,weight_decay,optimizer_choice,critierion,patience\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# # Guardar predicciones con Pickle\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# with open('lstm_preds.pkl', 'wb') as f:\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#     pickle.dump(lstm_preds, f)\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[43mplot_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompanies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mguardar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m plot_prices_comparision(actuals,preds,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m,scaler_y,\u001b[38;5;28mlen\u001b[39m(companies),companies,guardar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,file_name\u001b[38;5;241m=\u001b[39mfile_name)\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive - Universidad de Oviedo\\Escritorio\\UNI\\3ºAÑO\\LAB_IACD\\Proyecto_2_Lab_IACD\\Proyect2_LABIACD\\visualizations.py:209\u001b[0m, in \u001b[0;36mplot_metrics\u001b[1;34m(metrics, modelo, num_empresas, guardar, file_name)\u001b[0m\n\u001b[0;32m    206\u001b[0m axes[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mgrid()\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# MAE\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m axes[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(epochs, \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_mae\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain MAE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m axes[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(epochs, metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_mae\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation MAE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m axes[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'train_mae'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/QAAANECAYAAAAe72QXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADzBElEQVR4nOzdd3hT5dvA8W+S7s1uWWXvvfeQvYegspcgCCjievmpCC5UxIngQKmoKEPBgUDL3nvIll0oLRu6aJs25/3jIYHS3SZNm96f68qV5OQ559xPGCf3eZZO0zQNIYQQQgghhBBC5Ct6ewcghBBCCCGEEEKIrJOEXgghhBBCCCGEyIckoRdCCCGEEEIIIfIhSeiFEEIIIYQQQoh8SBJ6IYQQQgghhBAiH5KEXgghhBBCCCGEyIckoRdCCCGEEEIIIfIhSeiFEEIIIYQQQoh8SBJ6IYQQQgghhBAiH5KEXgghHjFy5Ei8vLzsHYYQQgghclFQUBA6nY59+/bZOxQhMk0SeiGsRC4CmTdy5Eh0Ol2qDzc3N3uHJ4QQQliu6+aHk5MTpUqVYuTIkYSFhaUo365dO3Q6HZUrV071eCEhIZZjLV++PNlnR44cYcCAAQQGBuLm5kapUqXo1KkTX3zxRbJy5cqVS/P62bVrV+tV3kYe/U4ffezatcveIQqR7zjZOwAhRMHk6urKggULUmw3GAx2iEYIIYRI3VtvvUX58uWJi4tj165dBAUFsW3bNo4ePZriJrSbmxtnzpxhz549NGnSJNlnP//8M25ubsTFxSXbvmPHDtq3b0/ZsmUZO3Ys/v7+XLp0iV27dvHZZ58xefLkZOXr1avHiy++mCLOkiVLWqnGtmf+Th9VqVIlO0QjRP4mCb0Qwuo0TSMuLg53d/c0yzg5OTF06NBcjEoIIYTIum7dutGoUSMAnn76aYoWLcoHH3zAn3/+yRNPPJGsbMWKFUlMTOSXX35JltDHxcWxYsUKevTowW+//ZZsn3fffRdfX1/27t2Ln59fss+uXbuWIp5SpUrl6etnTEwMnp6e6ZZ5+DsVQuSMdLkXIpcdPHiQbt264ePjg5eXFx06dEjRxcxoNDJz5kwqV66Mm5sbRYoUoVWrVoSEhFjKREREMGrUKEqXLo2rqysBAQH06dOHCxcupHt+8/jwc+fO0aVLFzw9PSlZsiRvvfUWmqYlK2symfj000+pWbMmbm5ulChRgmeeeYbbt28nK1euXDl69uzJ2rVradSoEe7u7nz99dc5+6J40DVvy5YtPPPMMxQpUgQfHx+GDx+eIgaAefPmUbNmTVxdXSlZsiQTJ07kzp07Kcrt3r2b7t27U6hQITw9PalTpw6fffZZinJhYWH07dsXLy8vihUrxksvvURSUlKO6yWEECL/at26NQBnz55N9fNBgwaxZMkSTCaTZdtff/1FbGxsihsA5uPUrFkzRTIPULx4cesETdavqatXr6Z169Z4enri7e1Njx49OHbsWLIy5t8UZ8+epXv37nh7ezNkyJAcx3rhwgV0Oh0fffQRn3zyCYGBgbi7u9O2bVuOHj2aovyGDRsssfr5+dGnTx9OnDiRolxYWBhjxoyhZMmSuLq6Ur58eSZMmEBCQkKycvHx8UydOpVixYrh6elJv379uH79eo7rJYQtSAu9ELno2LFjtG7dGh8fH1555RWcnZ35+uuvadeuHZs3b6Zp06YAzJgxg1mzZvH000/TpEkTIiMj2bdvHwcOHKBTp04APP744xw7dozJkydTrlw5rl27RkhICKGhoZQrVy7dOJKSkujatSvNmjXjww8/ZM2aNbz55pskJiby1ltvWco988wzBAUFMWrUKJ577jnOnz/P3LlzOXjwINu3b8fZ2dlS9tSpUwwaNIhnnnmGsWPHUrVq1Qy/jxs3bqTY5uLigo+PT7JtkyZNws/PjxkzZnDq1Cnmz5/PxYsX2bRpEzqdzvKdzZw5k44dOzJhwgRLub179yaLNSQkhJ49exIQEMDzzz+Pv78/J06c4O+//+b5559P9h116dKFpk2b8tFHH7Fu3TrmzJlDxYoVmTBhQoZ1E0II4ZjMN84LFSqU6ueDBw9mxowZbNq0icceewyAxYsX06FDh1QT9MDAQHbu3MnRo0epVatWhuc3Go2pXj89PT3T7Rlnlplr6o8//siIESPo0qULH3zwAbGxscyfP59WrVpx8ODBZL8zEhMT6dKlC61ateKjjz7Cw8Mjwxju3r2bog46nY4iRYok27Zo0SKioqKYOHEicXFxfPbZZzz22GMcOXKEEiVKALBu3Tq6detGhQoVmDFjBvfu3eOLL76gZcuWHDhwwBLrlStXaNKkCXfu3GHcuHFUq1aNsLAwli9fTmxsLC4uLpbzTp48mUKFCvHmm29y4cIFPv30UyZNmsSSJUsyrJsQuU4TQljFwoULNUDbu3dvmmX69u2rubi4aGfPnrVsu3Lliubt7a21adPGsq1u3bpajx490jzO7du3NUCbPXt2luMcMWKEBmiTJ0+2bDOZTFqPHj00FxcX7fr165qmadrWrVs1QPv555+T7b9mzZoU2wMDAzVAW7NmTZZiSO3RpUsXSznzd9qwYUMtISHBsv3DDz/UAO2PP/7QNE3Trl27prm4uGidO3fWkpKSLOXmzp2rAdr333+vaZqmJSYmauXLl9cCAwO127dvJ4vJZDKliO+tt95KVqZ+/fpaw4YNM1VHIYQQ+Zv5GrRu3Trt+vXr2qVLl7Tly5drxYoV01xdXbVLly4lK9+2bVutZs2amqZpWqNGjbQxY8Zomqau2S4uLtoPP/ygbdy4UQO0ZcuWWfYLDg7WDAaDZjAYtObNm2uvvPKKtnbt2mTXPTPz9Ta1x6xZszJVn4yuqVFRUZqfn582duzYZPtHRERovr6+ybabr5f/93//l5mv1BJDag9XV1dLufPnz2uA5u7url2+fNmyfffu3RqgvfDCC5Zt9erV04oXL67dvHnTsu3w4cOaXq/Xhg8fbtk2fPhwTa/Xp/o7zfwbwBxfx44dk/0ueOGFFzSDwaDduXMnU/UUIjdJl3shcklSUhLBwcH07duXChUqWLYHBAQwePBgtm3bRmRkJAB+fn4cO3aM06dPp3osd3d3XFxc2LRpU6rd5DJj0qRJltc6nY5JkyaRkJDAunXrAFi2bBm+vr506tSJGzduWB4NGzbEy8uLjRs3Jjte+fLl6dKlS6bP7+bmRkhISIrH+++/n6LsuHHjkvUGmDBhAk5OTvzzzz+AujufkJDAlClT0Osf/Lc2duxYfHx8WLVqFaCGO5w/f54pU6ak6NpobpV42Pjx45O9b926NefOnct0HYUQQuR/HTt2pFixYpQpU4YBAwbg6enJn3/+SenSpdPcZ/Dgwfz+++8kJCSwfPlyDAYD/fr1S7Vsp06d2LlzJ7179+bw4cN8+OGHdOnShVKlSvHnn3+mKN+0adNUr5+DBg3KVH0yuqaGhIRw584dBg0alOz6bzAYaNq0aYrrv/kYWfHll1+miH/16tUpyvXt25dSpUpZ3jdp0oSmTZtaYg0PD+fQoUOMHDmSwoULW8rVqVOHTp06WcqZTCZWrlxJr169Uh27/+hvgHHjxiXb1rp1a5KSkrh48WKW6ilEbpAu90LkkuvXrxMbG5tqV/Tq1atjMpm4dOkSNWvW5K233qJPnz5UqVKFWrVq0bVrV4YNG0adOnUANUP8Bx98wIsvvkiJEiVo1qwZPXv2ZPjw4fj7+2cYi16vT3ZTAaBKlSrAg66Ep0+f5u7du2mO33t0op7UZqtNj8FgoGPHjpkq++gSQF5eXgQEBFhiNV9gH/1uXVxcqFChguVz83jHzHRpdHNzo1ixYsm2FSpUKNs3UIQQQuRPX375JVWqVOHu3bt8//33bNmyBVdX13T3eeqpp3jppZdYvXo1P//8Mz179sTb2zvN8o0bN7bcADh8+DArVqzgk08+YcCAARw6dIgaNWpYyhYtWjTT18/UZHRNNTcmmIcLPOrRYXFOTk7p3txITZMmTTI1KV5qSwBWqVKFpUuXAmlf/0H9tlq7di0xMTFER0cTGRmZqes/QNmyZZO9Nw+vkN8AIi+ShF6IPKhNmzacPXuWP/74g+DgYBYsWMAnn3zCV199xdNPPw3AlClT6NWrFytXrmTt2rW88cYbzJo1iw0bNlC/fv0cx2AymShevDg///xzqp8/muxmZtxefiLL5wkhhIDkyWffvn1p1aoVgwcP5tSpU3h5eaW6T0BAAO3atWPOnDls3749xcz2aXFxcaFx48Y0btyYKlWqMGrUKJYtW8abb75ptfpkxDyZ348//phqI4GTU/L0wdXVNVnvOEeQ1m8A7ZHJg4XICxzrX58QeVixYsXw8PDg1KlTKT47efIker2eMmXKWLYVLlyYUaNG8csvv3Dp0iXq1KnDjBkzku1XsWJFXnzxRYKDgzl69CgJCQnMmTMnw1hMJlOKruP//fcfgGXymIoVK3Lz5k1atmxJx44dUzzq1q2bxW8g+x4dehAdHU14eLgl1sDAQIAU321CQgLnz5+3fF6xYkWAVGfIFUIIITJiMBiYNWsWV65cYe7cuemWHTx4MFu3bsXHx4fu3btn+Vzmmwjh4eHZijUtGV1TzdfK4sWLp3r9b9eunVXjyUqsoH6vZHT9B/XbqmjRonh6elKsWDF8fHzk+i8ckiT0QuQSg8FA586d+eOPP5ItLXf16lUWL15Mq1atLN3Ybt68mWxfLy8vKlWqRHx8PACxsbHExcUlK1OxYkW8vb0tZTLy8A8RTdOYO3cuzs7OdOjQAYAnnniCpKQk3n777RT7JiYmprocnK188803GI1Gy/v58+eTmJhIt27dADW+0cXFhc8//zzZ3fPvvvuOu3fv0qNHDwAaNGhA+fLl+fTTT1PEL3fdhRBCZEa7du1o0qQJn376aYpr8cMGDBjAm2++ybx585LNoP6ojRs3pnoNMo//zsyqMVmR0TW1S5cu+Pj48N577yUrZ5aby7etXLmSsLAwy/s9e/awe/duS6wBAQHUq1ePH374Idl1/ejRowQHB1tupOj1evr27ctff/3Fvn37UpxHfgOI/Ey63AthZd9//z1r1qxJsf3555/nnXfeISQkhFatWvHss8/i5OTE119/TXx8PB9++KGlbI0aNWjXrh0NGzakcOHC7Nu3j+XLl1smsvvvv//o0KEDTzzxBDVq1MDJyYkVK1Zw9epVnnrqqQxjdHNzY82aNYwYMYKmTZuyevVqVq1axf/+9z9LV/q2bdvyzDPPMGvWLA4dOkTnzp1xdnbm9OnTLFu2jM8++4wBAwZk+3tKTEzkp59+SvWzfv364enpaXmfkJBgqe+pU6eYN28erVq1onfv3oDq/TBt2jRmzpxJ165d6d27t6Vc48aNGTp0KKAu6PPnz6dXr17Uq1ePUaNGERAQwMmTJzl27Bhr167Ndn2EEEIUHC+//DIDBw4kKCgoxQSqZr6+vil61qVm8uTJxMbG0q9fP6pVq0ZCQgI7duxgyZIllCtXjlGjRiUrHxYWlur108vLi759+2Z4voyuqT4+PsyfP59hw4bRoEEDnnrqKYoVK0ZoaCirVq2iZcuWGfZOyMjq1as5efJkiu0tWrRINsdPpUqVaNWqFRMmTCA+Pp5PP/2UIkWK8Morr1jKzJ49m27dutG8eXPGjBljWbbu0e//vffeIzg4mLZt2zJu3DiqV69OeHg4y5YtY9u2bSkmyxUi37DnFPtCOJL0lmIBLMvbHDhwQOvSpYvm5eWleXh4aO3bt9d27NiR7FjvvPOO1qRJE83Pz09zd3fXqlWrpr377ruWZWZu3LihTZw4UatWrZrm6emp+fr6ak2bNtWWLl2aYZwjRozQPD09tbNnz2qdO3fWPDw8tBIlSmhvvvlmsiXfzL755hutYcOGmru7u+bt7a3Vrl1be+WVV7QrV65YygQGBqa7zF5qMaT3XZ0/fz7Zd7p582Zt3LhxWqFChTQvLy9tyJAhyZanMZs7d65WrVo1zdnZWStRooQ2YcKEFMvTaZqmbdu2TevUqZPm7e2teXp6anXq1NG++OKLFN/Ro958801N/tsUQoiCIb3laJOSkrSKFStqFStW1BITEzVNS75sXVpSW7Zu9erV2ujRo7Vq1appXl5emouLi1apUiVt8uTJ2tWrV5Ptn96ydYGBgZmqT2avqRs3btS6dOmi+fr6am5ublrFihW1kSNHavv27bOUSet6mVEMaT0WLlyoadqDZetmz56tzZkzRytTpozm6uqqtW7dWjt8+HCK465bt05r2bKl5u7urvn4+Gi9evXSjh8/nqLcxYsXteHDh1uWHqxQoYI2ceJELT4+Pll8j/6Zm//cNm7cmOm6CpFbdJomfUyEKEhGjhzJ8uXLiY6OtncoGQoKCmLUqFHs3bs3U7PhCiGEECJ1+emaeuHCBcqXL8/s2bN56aWX7B2OEHmajKEXQgghhBBCCCHyIUnohRBCCCGEEEKIfEgSeiGEEEIIIYQQIh+SMfRCCCGEEEIIIUQ+JC30QgghhBBCCCFEPiQJvRBCCCGEEEIIkQ852TuA3GYymbhy5Qre3t7odDp7hyOEEEKgaRpRUVGULFkSvV7utVuDXO+FEELkJba61he4hP7KlSuUKVPG3mEIIYQQKVy6dInSpUvbOwyHINd7IYQQeZG1r/V2Tejnz5/P/PnzuXDhAgA1a9Zk+vTpdOvWLc19li1bxhtvvMGFCxeoXLkyH3zwAd27d8/0Ob29vQH1Rfr4+OQofqPRSHBwMJ07d8bZ2TlHx8pLHLFejlgncMx6SZ3yD0esl73qFBkZSZkyZSzXKJFzcr1PnyPWCRyzXlKn/MMR6yV1sh5bXevtmtCXLl2a999/n8qVK6NpGj/88AN9+vTh4MGD1KxZM0X5HTt2MGjQIGbNmkXPnj1ZvHgxffv25cCBA9SqVStT5zR3u/Px8bHKBd7DwwMfHx+H+QsOjlkvR6wTOGa9pE75hyPWy951kq7h1iPX+/Q5Yp3AMesldco/HLFeUifrs/a13q4D9Xr16kX37t2pXLkyVapU4d1338XLy4tdu3alWv6zzz6ja9euvPzyy1SvXp23336bBg0aMHfu3FyOXAghhBBCCCGEsK88M4Y+KSmJZcuWERMTQ/PmzVMts3PnTqZOnZpsW5cuXVi5cmWax42Pjyc+Pt7yPjIyElB3ZoxGY45iNu+f0+PkNY5YL0esEzhmvaRO+Ycj1stedXKk71AIIYQQucfuCf2RI0do3rw5cXFxeHl5sWLFCmrUqJFq2YiICEqUKJFsW4kSJYiIiEjz+LNmzWLmzJkptgcHB+Ph4ZGz4O8LCQmxynHyGkeslyPWCRyzXlKn/MMR65XbdYqNjc3V8wkhhBDCMdg9oa9atSqHDh3i7t27LF++nBEjRrB58+Y0k/qsmjZtWrJWffNkBJ07d7bKmLqQkBA6derkMGNKwDHr5Yh1Asesl73rZDKZMBqNaJpmtWMmJiayY8cOWrRogZOT3f/btRpHrJct6qTT6XBycsJgMKRZxtx7TAghRM4lJSXZpeeT0WjEycmJuLg4kpKScv38tiB1yhoXF5dcX37W7r/AXFxcqFSpEgANGzZk7969fPbZZ3z99dcpyvr7+3P16tVk265evYq/v3+ax3d1dcXV1TXFdmdnZ6slC9Y8Vl7iiPVyxDqBY9bLHnVKSEjgwoULmEwmqx5X0zT8/f0JDw93qEnPHLFetqyTn58f/v7+qR7X0f79CiGEPWiaRkREBHfu3LHb+f39/bl06ZLDXRelTpmj1+spX748Li4uVj1ueuye0D/KZDIlG/P+sObNm7N+/XqmTJli2RYSEpLmmHshhMgsTdMIDw/HYDBQpkwZq95dNZlMREdH4+Xllet3bW3JEetlizppmkZsbCzXrl0DICAgwCrHFUIIkZw5mS9evDgeHh65noDKdTF/sFWdTCYTV65cITw8nLJly+ba3z+7JvTTpk2jW7dulC1blqioKBYvXsymTZtYu3YtAMOHD6dUqVLMmjULgOeff562bdsyZ84cevTowa+//sq+ffv45ptv7FkNIYQDSExMJDY2lpIlS1ptfg0zk8lEQkICbm5uDnMxBMesl63q5O7uDsC1a9coXrx4ut3vhRBCZF1SUpIlmS9SpIhdYpDrYv5gyzoVK1aMK1eukJiYmGu97+ya0F+7do3hw4cTHh6Or68vderUYe3atXTq1AmA0NDQZF9yixYtWLx4Ma+//jr/+9//qFy5MitXrsz0GvRCCJEW8xiq3OwiJQoW840io9EoCb0QQliZecy8tW/KC5EV5t+RSUlJBSOh/+6779L9fNOmTSm2DRw4kIEDB9ooIiFEQeco48NE3iN/t4QQwvbk/1phT/b4++cY/SaEEEIIIYQQQogCRhJ6IYQQyZQrV45PP/3U3mEIIYQQIhvkOl6wSEIvhBD5lE6nS/cxY8aMbB137969jBs3LkextWvXLtmKJEIIIYRILq9fx3U6HQaDAX9/f6pVq8asWbPQNM1S5sKFC5YyYWFhyfYPDw/HyckJnU7HhQsXLNtXrFhBs2bN8PX1xdvbm5o1ayb7vRAUFJTqd+Hm5paj+jiyPLdsXX6i3/YxHY8tQF/4PLSeYu9whBAFTHh4uOX1kiVLmD59OqdOnbJs8/LysrzWNI2kpCScnDL+b79YsWLWDVSI/OzsRpxWv0IToyfQ3d7RCCEcSF6/jo8dO5YZM2Zw48YN9u7dy/jx4/Hz82PChAnJypUqVYpFixYxbdo0y7YffviBUqVKERoaatm2fv16nnzySd5991169+6NTqfj+PHjhISEJDuej49Psu8BZG6E9EgLfU4kROOZcA0ir9g7EiFEAeTv7295+Pr6otPpLO9PnjyJt7c3q1evpmHDhri6urJt2zbOnj1Lnz59KFGiBF5eXjRu3Jh169YlO+6jXfV0Oh0LFiygX79+eHh4ULlyZf78888cxf7bb79Rs2ZNXF1dKVeuHHPmzEn2+bx586hcuTJubm6UKFGCAQMGWD5bvnw5tWvXxt3dnSJFitCxY0diYmJyFI8QadIb0N34D++48IzLCiFEFuT167iHhwf+/v6ULVuWUaNGUadOnRTJN8CIESNYuHBhsm0LFy5kxIgRybb99ddftGzZkpdffpmqVatSpUoV+vbty5dffpms3MPfg/lRokSJDOMtqCShzwn3QgDo7t2ycyBCCGvTNI3YhESrPe4lJGW67MPd2XLq//7v/3j//fc5ceIEderUITo6mu7du7N+/XoOHjxI165d6dWrV7I76KmZOXMmTzzxBP/++y/du3dnyJAh3LqVvf/79u/fzxNPPMFTTz3FkSNHmDFjBm+88QZBQUEA7Nu3j+eee4633nqLU6dOsWbNGtq0aQOo1oxBgwYxevRoTpw4waZNm+jfv79VvzMhkvEuCYCb8TbI3zMh8hVrX8sze713tOu4pmls3bqVkydPprq8b+/evbl9+zbbtm0DYNu2bdy+fZtevXolK+fv78+xY8c4evRoJmsvMkO63OeA5lFEvbh3276BCCGs7p4xiRrT19rl3Mff6oKHi3X+e37rrbfo1KmT5X3hwoWpW7eu5f3bb7/NihUr+PPPP5k0aVKaxxk5ciSDBg0C4L333uPzzz9nz549tGjRIssxffzxx3To0IE33ngDgCpVqnD8+HFmz57NyJEjCQ0NxdPTk549e+Lt7U1gYCD169cHVEKfmJhI//79CQwMBKB27dpZjkGITPMJAMDJFI8xPhJcito5ICFEZtnrWn50Rie8DAarHMvW1/GuXbumuc+8efNYsGABCQkJGI1G3NzceO6551KUc3Z2ZujQoXz//fe0atWK77//nqFDh6ZYh33y5Mls3bqV2rVrExgYSLNmzejcuTNDhgzB1dXVUu7u3bvJhhsAtG7dmtWrV6cZa0EmLfQ5cb+Fntib9o1DCCHS0KhRo2Tvo6Ojeemll6hevTp+fn54eXlx4sSJDO/s16lTx/La09MTHx8frl27lq2YTpw4QcuWLZNta9myJadPnyYpKYlOnToRGBhIhQoVGDZsGD///DOxsbEA1K1blw4dOlC7dm0GDhzIt99+y+3bclNV2JCLJ5qbr3odJd3uhRC5y57X8SFDhnDgwAHWrFlD165dee2119K8kT969GiWLVtGREQEy5YtY/To0SnKeHp6smrVKs6cOcPrr7+Ol5cXL774Ik2aNLFc5wG8vb05dOhQsseCBQvSjbUgkxb6nHAvDIBOWuiFcDjuzgaOv9XFKscymUxERUbh7eONXp/xfVR3Z+vc1Qd18XzYSy+9REhICB999BGVKlXC3d2dAQMGkJCQkO5xHr3LrtPpMJlMVovzYd7e3hw4cIBNmzYRHBzM9OnTmTFjBnv37sXPz4+QkBB27NhBcHAwX3zxBa+99hq7d++mfPnyNolHCLxLQtxddFHhgPQIESK/sOa1PDPM13tHuY77+vpSqVIlihcvzpIlS6hSpQrNmjWjY8eOKcrWrl2batWqMWjQIKpXr06tWrU4dOhQqsetWLEiFStW5Omnn+a1116jSpUqLFmyhFGjRgGg1+upVKlSurGJByShzwHNQyX03JMWeiEcjU6ns1q3d5PJRKKLAQ8Xp0wl9La0fft2Ro4cSb9+/QB1p//h5WRyQ/Xq1dm+fXuKuKpUqYLhfhdFJycnOnbsSMeOHXnzzTfx8/Njw4YN9O/fH51OR8uWLWnZsiXTp08nMDCQFStWMHXq1Fythyg4NO8AdNdPyCS4QuQz1ryWZ4b5em/LGdntdR338vLi+eef56WXXuLgwYOp1nH06NE8++yzzJ8/P9PHLVeuHB4eHjK5bQ5IQp8T5hb6+ChIMoLBOYMdhBDCvipXrszvv/9Or1690Ol0vPHGGzZrab9+/XqKu/MBAQG8+OKLNG7cmLfffpsnn3ySnTt3MnfuXObNmwfA33//zblz52jTpg2FChXin3/+wWQyUbVqVXbv3s369evp3LkzxYsXZ/fu3Vy/fp3q1avbpA5CAOCtxtHrpMu9EMLOcvM6/qhnnnmGt99+m99++y3Z6jNmY8eOZeDAgfj5+aW6/4wZM4iNjaV79+4EBgZy584dPv/8c4xGY7J5AjRNIyIiIsX+xYsXt3vDSF4k30hOuPmhcf/ulHS7F0LkAx9//DGFChWiRYsW9OrViy5dutCgQQObnGvx4sXUr18/2ePbb7+lQYMGLF26lF9//ZVatWoxffp03nrrLUaOHAmAn58fv//+O4899hjVq1fnq6++4pdffqFmzZr4+PiwZcsWunfvTpUqVXj99deZM2cO3bp1s0kdhADVQg/IGHohhN3l5nX8UYULF2b48OHMmDEj1ZsITk5OFC1aFCen1NuM27Zty7lz5xg+fDjVqlWjW7duREREEBwcTNWqVS3lIiMjCQgISPHI7tw9jk5a6HNCb8Bo8MAlKUZNjOdV3N4RCSEKqJEjR1oSYoB27dqlumxOuXLl2LBhQ7JtEydOTPb+0a57qR3nzp07mEwmIiMjU41n06ZN6cb7+OOP8/jjj6f6WatWrdLcv3r16qxZsybdYwthbZqlhV663AshbMMe1/H0mK/DjybuX331VbJY0luir169esk+b9++Pe3bt0/3vI9+DyJj0kKfQwlO95dUiJW16IUQQgiHZEnoU3YBFUIIIexJEvocSnDyVi/uSUIvhBBCOCLNp6R6IV3uhRBC5DGS0OdQgsHcQi8z3QshhBAOydxCH3sDEuPtHIwQQgjxgCT0OWRpoZcu90IIIYRjci9Mku7+SjbSSi+EECIPkYQ+hx6MoZcWeiGEEMIh6XTEORdSryMloRdCCJF3SEKfQ5Yu97JsnRBCCOGw7lkS+jD7BiKEEEI8RBL6HJJZ7oUQQgjHF+dyP6GXLvdCCCHyEEnoc0i63AshhBCO70ELvaxFL4QQIu+QhD6HZNk6IYQQwvHFSUIvhBAiD5KEPodk2TohRH7Xrl07pkyZYnlfrlw5Pv3003T30el0rFy5MsfnttZxhLC1e86F1Qvpci+EyGPkOl6wSUKfQ5Yu9/fugCnJrrEIIQqWXr160bVr11Q/27p1Kzqdjn///TfLx927dy/jxo3LaXjJzJgxg3r16qXYHh4eTrdu3ax6rkcFBQXh5+dn03MIx2cZQy8t9EIIK8lP1/GZM2fa9Tqu0+nQ6XTo9XoCAgJ48sknCQ0NTVauXbt26HQ63n///RTH6NGjBzqdjhkzZli2nT9/nsGDB1OyZEnc3NwoXbo0ffr04eTJk5Yy5vM++vj1119tVt+skoQ+hywJPRrE3bVrLEKIgmXMmDGEhIRw+fLlFJ8tXLiQRo0aUadOnSwft1ixYnh4eFgjxAz5+/vj6uqaK+cSIicsXe6jwsFksm8wQgiHINfxzPPx8SE8PJywsDB+++03Tp06xcCBA1OUK1OmDEFBQcm2hYWFsX79egICAizbjEYjXbp04e7du/z++++cOnWKJUuWULt2be7cuZNs/4ULFxIeHp7s0bdvXxvUMnskoc8hTeeE5np/HL10uxdC5KKePXtSrFixFBeu6Oholi1bxpgxY7h58yaDBg2iVKlSeHh4ULt2bX755Zd0j/toV73Tp0/Tpk0b3NzcqFGjBiEhISn2efXVV6lSpQoeHh5UqFCBN954A6PRCKg76zNnzuTw4cOWO9vmmB/tqnfkyBEee+wx3N3dKVKkCOPGjSM6Otry+ciRI+nbty8fffQRAQEBFClShIkTJ1rOlR2hoaH06dMHLy8v/Pz8GDVqFFevXrV8fvjwYdq3b4+3tzc+Pj40bNiQffv2AXDx4kV69epFoUKF8PT0pGbNmvzzzz/ZjkXkXXHOfmjowJQIsTfsHY4QwgHkl+v44sWLeeutt+x6HdfpdPj7+xMQEECLFi0YM2YMe/bsITIyMsV3euPGDbZv327Z9sMPP9C5c2eKFy9u2Xby5EnOnj3LvHnzaNasGYGBgbRs2ZJ33nmHZs2aJTumn58f/v7+yR5ubm7pxpubJKG3Bvci6lmWrhPCcWgaJMRY72GMzXxZTctUiE5OTgwfPpygoCC0h/ZZtmwZSUlJDBo0iLi4OBo2bMiqVas4evQo48aNY9iwYezZsydT5zCZTPTv3x8XFxd2797NV199xauvvpqinLe3N0FBQRw/fpzPPvuMb7/9lk8++QSAJ598khdffJGaNWta7mw/+eSTKY4RExNDly5dKFSoEHv37mXZsmWsW7eOSZMmJSu3ceNGzp49y8aNG/nhhx8ICgpK8WMos0wmE3369OHWrVts3ryZtWvXcuHCBQYNGmQpM2TIEEqXLs3evXvZv38///d//4ezszMAEydOJD4+ni1btnDkyBE++OADvLy80jpdgbRlyxZ69epFyZIlMz3WMj4+ntdee43AwEBcXV0pV64c33//ve2DTYemcwLPYuqNrEUvRP5g7Wt5Zq/3DnYd79evH1OnTs0z1/Fr166xYsUKDAYDBoMh2WcuLi4MGTKEhQsXWrYFBQUxevToZOWKFCmCXq9n+fLlJCXl72HTTvYOwBFo7oXQ3bkgLfRCOBJjLLxX0iqH0gN+Wdnhf1fAxTNTRUePHs3s2bPZvHkz7dq1A1TXsMcffxxfX198fX156aWXLOUnT57M2rVrWbp0KU2aNMnw+OvWrePkyZOsXbuWkiXV9/Hee++lGC/3+uuvW16XK1eOl156iV9//ZVXXnkFd3d3vLy8cHJywt/fP81zLV68mLi4OBYtWoSnp6r/3Llz6dWrFx988AElSpQAoFChQsydOxeDwUC1atXo0aMH69evZ+zYsZn6zh62fv16jhw5wvnz5ylTpgwmk4n58+fTvHlz9u7dS+PGjQkNDeXll1+mWrVqAFSuXNmyf2hoKI8//ji1a9cGoEKFClmOwdHFxMRQt25dRo8eTf/+/TO1zxNPPMHVq1f57rvvqFSpEuHh4ZjyQDd3zTsAXcw1iAyHkvXtHY4QIiNWvJZnhvl6b/q/y2DwztQ+ef06/tJLL+WJ6/jdu3fx8vJC0zRiY2MBeO655yznedjo0aNp3bo1n332Gfv37+fu3bv07Nkz2fj5kiVL8tlnn/Hqq68yc+ZMGjVqRPv27RkyZEiKa/mgQYNS3Dg4fvw4ZcuWTTPe3CQJvTW435/5VpauE0LksmrVqtGiRQu+//572rVrx5kzZ9i6dStvvfUWAElJSbz33nssXbqUsLAwEhISiI+Pz/TYuhMnTlCmTBnLjwCA5s2bpyi3ZMkSPv/8c86ePUt0dDSJiYn4+PhkqS4nTpygbt26yS7OLVu2xGQycerUKcsPgZo1aya7sAYEBHDkyJEsnevhc5YpU4YyZcpYtlWrVg0/Pz9OnDhB48aNmTp1Kk8//TQ//vgjHTt2ZODAgVSsWBFQPyYmTJhAcHAwHTt25PHHH8/WeEdH1q1btyxNmLRmzRo2b97MuXPnKFxYXV/LlStno+iyyKckRByGKJkYTwhhHXIdz9x13NvbmwMHDmA0Glm9ejU///wz7777bqpl69atS+XKlVm+fDkbN25k2LBhODmlTHufffZZRowYwaZNm9i1axfLli3jvffe488//6RTp06Wcp988gkdO3ZMtu/D36e9SUJvDR73E3rpci+E43D2UC3lVmAymYiMisLH2xu9PhMjnZyzNpHNmDFjmDx5Ml9++SULFy6kYsWKtG3bFoDZs2fz2Wef8emnn1K7dm08PT2ZMmUKCQkJ2alKqnbu3MmQIUOYOXMmXbp0wdfXl19//ZU5c+ZY7RwPM3d3N9PpdDZtvZ0xYwaDBw9m1apVrF69mjfffJNff/2Vfv368fTTT9OlSxdWrVpFcHAws2bNYs6cOUyePNlm8Ti6P//8k0aNGvHhhx/y448/4unpSe/evXn77bdxd3e3a2ya9/0JlWSmeyHyByteyzPDcr2X63i6snMd1+v1VKpUCYDq1atz9uxZJkyYwI8//phq+dGjR/Pll19y/PjxdIcneHt706tXL3r16sU777xDly5deOedd5Il9P7+/pZz50WS0FuBZm6hly73QjgOnS7T3d4zZDKBc5I6XmYS+ix64okneP7551m8eDGLFi1iwoQJ6HQ6ALZv306fPn0YOnTo/VBM/Pfff9SoUSNTx65evTqXLl0iPDzcMjvsrl27kpXZuXMngYGBvPbaa5ZtFy9eTFbGxcUlwzFq1atXJygoiJiYGMvd/e3bt6PX66latWqm4s0qc/0uXbpkaaU/efIkd+7cSfYdValShSpVqvDCCy8waNAgFi5cSL9+/QA1o+748eMZP34806ZN49tvv5WEPgfOnTvHtm3bcHNzY8WKFdy4cYNnn32WmzdvJhsT+aj4+Hji4+Mt780TJRmNxhxNmmg+BkCSZwkMgOluGEk5PKa9meuU0+8mr3HEekmdMn9MTdMwmUzJk0On3LsRqGkaOCehQZZuNA8YMIDnn3+en376iUWLFjF+/Hg0TUPTNLZt20bv3r0ZPHgw8OA6Xr169WTnMNf90fdVq1bl0qVLhIWFWa7jO3bssBzLZDKxfft2AgMDmTZtmmX/CxcuPKgTKglPSkpKtV7m41StWpWgoCCioqIs1/GtW7ei1+upXLkyJpPJUq9HYzUfJzXm7Q9//sorr1C5cmWef/55GjRokKLeTz31FC+99BJ169alWrVqln3N50/tOwOoWrUqO3fuTLY9xd+pdJjraDQaU3TTt9W/YUnorcFDutwLIezHy8uLJ598kmnTphEZGcnIkSMtn5m7nO3YsYNChQrx8ccfc/Xq1Uwn9B07dqRKlSqMGDGC2bNnExkZmSxxB6hUqRKhoaH8+uuvNG7cmFWrVrFixYpkZcqVK8f58+c5dOgQpUuXxtvbO8UyN0OGDOHNN99kxIgRzJgxg+vXrzN58mSGDRtm6aaXXUlJSRw6dCjZNldXVzp27Ejt2rUZMmQIn376KQkJCUyYMIG2bdvSqFEj7t27x8svv8yAAQMoX748ly9fZu/evTz++OMATJkyhW7dulGlShVu377Nxo0bqV69eo5iLehMJhM6nY6ff/4ZX19fAD7++GMGDBjAvHnz0mylnzVrFjNnzkyxPTg42GrLNx29cJMGwI3zR9jpIKsZpDbbtSNwxHpJndJnHt8dHR1t1dbr7IiKisryPv369eN///sfUVFR9O/f33JTMjAwkD/++IOQkBD8/PyYN28eERERVK5c2VImMTGRhIQEy3uTyURcXByRkZE0adKESpUqMWzYMGbOnElUVJTlOn7v3j0iIyMpWbIkoaGhLFy4kAYNGhAcHMyKFSvQNM1SlxIlSnD+/Hm2b99OyZIl8fLyslzHzcfp1asXM2bMYOjQobz66qvcvHmT5557jieffBJ3d3ciIyMxGo0kJiYmm50+ISEhxbaHxcXFoWlass99fX3p2bMnr732GkuWLEnxPRgMBk6ePImTk5Nlv6SkJOLj44mKiuLIkSPMmjWLJ598kqpVq+Li4sL27dtZuHAhzz//fLJzRUREcPr06WQxeXl5pTp+PyEhgXv37rFlyxYSExOTfWYe+29tktBbg/v9tWmly70Qwk7GjBnDd999R/fu3ZON63r99dc5d+4cXbp0wcPDg3HjxtG3b1/u3r2bqePq9XpWrFjBmDFjaNKkCeXKlePzzz+na9euljK9e/fmhRdeYNKkScTHx9OjRw/eeOONZJPPPP744/z++++0b9+eO3fusHDhwmQ3HgA8PDxYu3Ytzz//PI0bN8bDw4PHH3+cjz/+OEffDaglgOrXTz6JWcWKFTlz5gx//PEHkydPpk2bNuj1ejp06MC8efMAMBgM3Lx5k+HDh3P16lWKFi1K//79LYljUlISEydO5PLly/j4+NC1a1fLrMAiewICAihVqpQlmQfVk0LTNC5fvpxsUsKHTZs2jalTp1reR0ZGUqZMGTp37pzlcaCPMhqNhISEUKN5Rwj9hmIuCXTv3j1Hx7Q3c506deqUovtrfuaI9ZI6ZU5cXByXLl3Cy8vLbkuKmRNgb29vS0+5zHrmmWf48ccf6datW7JeaTNnzuTy5csMGDAADw8Pxo4da7mOm/9vc3JywsXFxfJer9fj5uZmeb9ixQrGjh1Lx44dLUvade/eHXd3d3x8fHjqqac4ePAgr776KvHx8XTv3p033niDmTNn4u3tTVRUFEOGDGHNmjX07t2bO3fu8N1331mu4+bj+Pj4sGbNGl544QU6dOiAh4cH/fv3Z86cOZYVYJydnXFyckr2/7KLi0uKbQ9zc3NDp9Ol+Pyll16iZcuWnDx5kiZNmqT4Hh4tbzAYcHV1xdvbm5IlS1KpUiU++ugjLly4gE6no1y5csyYMYMpU6YkGyI5ceLEFDG99957qa4WEBcXh7u7u2WZwIeldcMip3Salsl1FRxEZGQkvr6+yf4RZJfRaOSff/6hR7kEnFY8DWVbwOjVVorUfsz16t69u0NdOBytTuCY9bJXneLi4jh//jzly5e3+g8Bk8lEZGQkPj4+mRtDn084Yr1sWaf0/o5Z89qUl+l0OlasWEHfvn3TLPPNN98wZcoUrl27ZvkB+Mcff9C/f3+io6MzPY7eFtf77k0r4/xVc3Dxhv9dztEx7c0Rrx/gmPWSOmWOLa/jmSXXxfzB0a71jvGnYm/S5V4IIYRIVXR0NIcOHbIMeTAPvQgNDQVUy/rw4cMt5QcPHkyRIkUYNWoUx48fZ8uWLbz88suMHj3a7pPiYZ4ULyEK4mzT0iKEEEJkhST0ViCT4gkhhBCp27dvH/Xr17cMeZg6dSr169dn+vTpAISHh1uSe1DjEkNCQrhz5w6NGjViyJAh9OrVi88//9wu8Sfj4gWu91tVosLtG4sQQgiBjKG3Dvci6jn2Fmiamh1bCCGEELRr1470RvcFBQWl2FatWrW8OwGYT0m4HqmWritmm9UXhBBCiMySFnpr8Lg/KZ6WBHGZm2hKCCGEEPmQrEUvhBAiD5GE3hqc3MD5/pI4Mo5eCCGEcFw+91eRiJKEXgghhP1JQm8tHuZu97ftG4cQIkcK2MIfIheZTCZ7hyCswZzQR8oYeiHyIvm/VtiTPX5Hyhh6a3EvBHcvycR4QuRTzs7O6HQ6rl+/TrFixbK8fmx6TCYTCQkJxMXFOcySL+CY9bJFnTRNIyEhgevXr6PX63FxcbHKcYWdSJd7IfIkFxcX9Ho9V65coVixYri4uFj1Wp4Zcl3MH2xVJ03TuH79OjqdLleXmJSE3lrMLfTS5V6IfMlgMFC6dGkuX77MhQsXrHpsTdO4d+8e7u7uuf7jwpYcsV62rJOHhwdly5Z1mB9EBZZPKfUsXe6FyFP0ej3ly5cnPDycK1fs8+9Trov5gy3rpNPpKF26NAaDwarHTY8k9NbiIUvXCZHfeXl5UblyZYxGo1WPazQa2bJlC23atMnVO7a25oj1slWdDAYDTk5ODvNjqEDzMbfQS5d7IfIaFxcXypYtS2JiIklJSbl+frku5g+2rJOzs3OuJvMgCb31WNailxZ6IfIzg8Fg9f+IDQYDiYmJuLm5OczFEByzXo5YJ2Fl3vfH0Mdcg8QEcJIhFELkJebuzvb4P9wRryFSp7xP+v1Zi3S5F0IIIRyfRxEw3E/ioyPsG4sQQogCTxJ6a5Eu90IIIYTj0+vB21+9lm73Qggh7EwSemuxLFsnLfRCCCGEQzN3u48Ms28cQgghCjxJ6K3FvZB6loReCCGEcGzmteijpIVeCCGEfdk1oZ81axaNGzfG29ub4sWL07dvX06dOpXuPkFBQeh0umQPNze3XIo4HeYu9zKGXgghhHBs5oRe1qIXQghhZ3ZN6Ddv3szEiRPZtWsXISEhGI1GOnfuTExMTLr7+fj4EB4ebnlcvHgxlyJOx8Nd7jXNvrEIIYQQwnYkoRdCCJFH2HXZujVr1iR7HxQURPHixdm/fz9t2rRJcz+dToe/v7+tw8sa87J1SfGQEAOuXvaNRwghhBC24X1/LXrpci+EEMLO8tQ69Hfv3gWgcOHC6ZaLjo4mMDAQk8lEgwYNeO+996hZs2aqZePj44mPj7e8j4yMBMBoNGI0GnMUr3l/o9EITi44GVzRJcVjjLoGetccHduektXLQThincAx6yV1yj8csV72qpMjfYcFgo9MiieEECJvyDMJvclkYsqUKbRs2ZJatWqlWa5q1ap8//331KlTh7t37/LRRx/RokULjh07RunSpVOUnzVrFjNnzkyxPTg4GA8PD6vEHhISAkBnvQfuSfFsD/mTux7lrXJsezLXy5E4Yp3AMesldco/HLFeuV2n2NjYXD2fyCHLpHgRapidTmffeIQQQhRYeSahnzhxIkePHmXbtm3plmvevDnNmze3vG/RogXVq1fn66+/5u23305Rftq0aUydOtXyPjIykjJlytC5c2d8fHxyFLPRaCQkJIROnTrh7OyMU9j7cO02rRrUQKvQPkfHtqdH6+UIHLFO4Jj1kjrlH45YL3vVydx7TOQTXveH/SUlQOxN8Cxq33iEEEIUWHkioZ80aRJ///03W7ZsSbWVPT3Ozs7Ur1+fM2fOpPq5q6srrq4pu787Oztb7cea5Vj3J8ZzSogEB/hxa83vKK9wxDqBY9ZL6pR/OGK9crtOjvb9OTwnF/AsBjHXVbd7SeiFEELYiV1nudc0jUmTJrFixQo2bNhA+fJZ76aelJTEkSNHCAgIsEGEWWReui72pn3jEEIIIYRtWcbRy8R4Qggh7MeuCf3EiRP56aefWLx4Md7e3kRERBAREcG9e/csZYYPH860adMs79966y2Cg4M5d+4cBw4cYOjQoVy8eJGnn37aHlVI7uGl64QQQgjhuLzN4+hl6TohhBD2Y9cu9/PnzwegXbt2ybYvXLiQkSNHAhAaGope/+C+w+3btxk7diwREREUKlSIhg0bsmPHDmrUqJFbYafNXVrohRBCiALB537PQFmLXgghhB3ZNaHXNC3DMps2bUr2/pNPPuGTTz6xUUQ5ZO5yf09a6IUQQgiHJl3uhRBC5AF27XLvcKTLvRBCCFEwSJd7IYQQeYAk9NYkXe6FEEKIgsHSQi8JvRBCCPuRhN6azC30927bNw4hhBBC2JZ0uRdCCJEHSEJvTR6F1LN0uRdCCCEcm/f9SfHi70J8tH1jEUIIUWBJQm9N5i73xhgwxtk3FiGEEELYjpsPuHir11HSSi+EEMI+JKG3Jjdf0BnUa5npXgghhHBssnSdEEIIO5OE3pp0ugdL18nEeEIIIYRj85aEXgghhH1JQm9tsnSdEEIIUTD4lFLPsnSdEEIIO5GE3trM4+ily70QQgjh2Cxd7mUMvRBCCPuQhN7apMu9EEIIUTBIl3shhBB2Jgm9tVkSelmLXgghhHBo0uVeCCGEnUlCb23u0kIvhBBCFAjS5V4IIYSdSUJvbeZJ8WQMvRBCCOHYzC300VchyWjfWIQQQhRIktBbm6XLvST0QgghhEPzKAp6Z0BTSb0QQgiRyyShtzbpci+EEEIUDHo9ePur1zIxnhBCCDuQhN7apMu9EEIIUXD4lFTPktALIYSwA0norU263AshhBAFh3npuiiZGE8IIUTuk4Te2swt9PGRMkGOEEKIAm/Lli306tWLkiVLotPpWLlyZab33b59O05OTtSrV89m8eWYpYU+zL5xCCGEKJAkobc2N19Ap17fk7XohRBCFGwxMTHUrVuXL7/8Mkv73blzh+HDh9OhQwcbRWYlloReWuiFEELkPid7B+Bw9AZw91PJfOxN8Cpu74iEEEIIu+nWrRvdunXL8n7jx49n8ODBGAyGLLXq5zrpci+EEMKOJKG3BY8i9xN6GUcvhBBCZNXChQs5d+4cP/30E++8806m9omPjyc+Pt7yPjIyEgCj0YjRmLMhcOb9UzuOzqM4ToB2N4zEHJ4nN6VXp/zMEesldco/HLFeUifrn9faJKG3BVm6TgghhMiW06dP83//939s3boVJ6fM/0yZNWsWM2fOTLE9ODgYDw8Pq8QWEhKSYptH/HU6Aaa7YfyzahXodFY5V25JrU6OwBHrJXXKPxyxXlKnnIuNjbXJcSWhtwVZuk4IIYTIsqSkJAYPHszMmTOpUqVKlvadNm0aU6dOtbyPjIykTJkydO7cGR8fnxzFZTQaCQkJoVOnTjg7Oyf/MDEejr+IQTPSvX3zB6vd5HHp1ikfc8R6SZ3yD0esl9TJesw9x6xNEnpbkKXrhBBCiCyLiopi3759HDx4kEmTJgFgMpnQNA0nJyeCg4N57LHHUt3X1dUVV1fXFNudnZ2t9oMt1WM5O4NHUYi9gfO9a+Bbwirnyi3W/H7yEkesl9Qp/3DEekmdrHM+W5CE3hbcC6ln6XIvhBBCZJqPjw9HjhxJtm3evHls2LCB5cuXU758eTtFlgGfAIi9oWa6969t72iEEEIUIJLQ24Kly70sWyeEEKJgi46O5syZM5b358+f59ChQxQuXJiyZcsybdo0wsLCWLRoEXq9nlq1aiXbv3jx4ri5uaXYnqd4l4SIIxB1xd6RCCGEKGAkobcFD5kUTwghhADYt28f7du3t7w3j3MfMWIEQUFBhIeHExoaaq/wrMOyFr0k9EIIIXKXJPS2YG6hlzH0QgghCrh27dqhaVqanwcFBaW7/4wZM5gxY4Z1g7I2SeiFEELYid7eATgk87J1Msu9EEII4fi8A9SzJPRCCCFymST0tiBd7oUQQoiCw9xCHxVu3ziEEEIUOJLQ24JlUrw7YEqyayhCCCGEsDHpci+EEMJOJKG3BfOydWgqqRdCCCGE4zIn9HF3ICHWrqEIIYQoWCShtwWDM7j6qtcyjl4IIYRwbK4+4OypXku3eyGEELlIEnpb8bjfSi8z3QshhBCOTacDH5kYTwghRO6ThN5W3GViPCGEEKLAkHH0Qggh7EASeluxTIwnLfRCCCGEw/M2z3QvCb0QQojcIwm9rcjSdUIIIUTBYelyL2PohRBC5B5J6G3F3EIvY+iFEEIIx+dTSj1Hhtk3DiGEEAWKJPS2Yh5DL13uhRBCCMfnfb+FXma5F0IIkYskobcVmeVeCCGEKDiky70QQgg7kITeVqTLvRBCCFFwmLvcR0dAUqJ9YxFCCFFgSEJvK7JsnRBCCFFweBYDnQE0E8Rcs3c0QgghCghJ6G1Flq0TQgghCg694cE4elmLXgghRC6RhN5WLMvW3QJNs28sQgghhLA9H0nohRBC5C5J6G3F3OVeS4K4u/aNRQghhBC2Z26hP7NOxtELIYTIFZLQ24qzGzh7qtfS7V4IIYRwfBUfU88HfoDvO8P1U/aNRwghhMOThN6WHu52L4QQQgjH1nAk9JkHrr4Qth++ag3bPwNTkr0jE0II4aDsmtDPmjWLxo0b4+3tTfHixenbty+nTmV8N3vZsmVUq1YNNzc3ateuzT///JML0WaDu6xFL4QQQhQYOh3UHwLP7oRKHSEpHkKmw/dd4MZpe0cnhBDCAdk1od+8eTMTJ05k165dhISEYDQa6dy5MzExMWnus2PHDgYNGsSYMWM4ePAgffv2pW/fvhw9ejQXI88kmeleCCGEKHh8S8GQ5dB7Lrj6wOW98FUr2PGFtNYLIYSwKrsm9GvWrGHkyJHUrFmTunXrEhQURGhoKPv3709zn88++4yuXbvy8ssvU716dd5++20aNGjA3LlzczHyTPKQteiFEEKIAkmngwbDVGt9xccgMQ6CX4eF3eDGGXtHJ4QQwkE42TuAh929q2aDL1y4cJpldu7cydSpU5Nt69KlCytXrky1fHx8PPHx8Zb3kZGRABiNRoxGY47iNe+f1nH0boUwAEnRNzDl8Fy5KaN65UeOWCdwzHpJnfIPR6yXverkSN+heIRvaRj6OxxYBGtfg0u74auW0GE6NB2v1q8XQgghsinPJPQmk4kpU6bQsmVLatWqlWa5iIgISpQokWxbiRIliIiISLX8rFmzmDlzZortwcHBeHh45Czo+0JCQlLdXjX8BtWA0JMH+Tc2j47zT0da9crPHLFO4Jj1kjrlH45Yr9yuU2xsbK6eT+QynQ4ajlAt9X9OgnObYO3/4Pgf0GUWlG5o7wiFEELkU3kmoZ84cSJHjx5l27ZtVj3utGnTkrXoR0ZGUqZMGTp37oyPj0+Ojm00GgkJCaFTp044Ozun+Fy/NwwiVhJYzJvS3bvn6Fy5KaN65UeOWCdwzHpJnfIPR6yXvepk7j0mHJxfGRi2EvYvhOA3VGv9gsegZn/o8AYUrmDvCIUQQuQzeSKhnzRpEn///TdbtmyhdOnS6Zb19/fn6tWrybZdvXoVf3//VMu7urri6uqaYruzs7PVfqyleSzv4gDo426jz4c/dq35HeUVjlgncMx6SZ3yD0esV27XydG+P5EOnQ4ajYbKnWHje3BoMRz7HU78BY3HQJtXwLOIvaMUQgiRT9h1UjxN05g0aRIrVqxgw4YNlC9fPsN9mjdvzvr165NtCwkJoXnz5rYKM/tk2TohhBBCpMa3NPSdB+O3qSXuTEbY/RV8Xg+2zoEEGYYhhBAiY3ZN6CdOnMhPP/3E4sWL8fb2JiIigoiICO7du2cpM3z4cKZNm2Z5//zzz7NmzRrmzJnDyZMnmTFjBvv27WPSpEn2qEL6ZNk6IYQQQqTHvxYM/U11xfevA/GRsP4t+KIhHPxJlrkTthF7C8IO2DsKIYQV2DWhnz9/Pnfv3qVdu3YEBARYHkuWLLGUCQ0NJTw83PK+RYsWLF68mG+++Ya6deuyfPlyVq5cme5EenZjWbbuFmiafWMRQgghRN5VsT2M2wz9F4BvWYi6An9MVOvX/xcsvyOE9Vw7CfNbwLftISztpaKFEPmDXcfQa5m4OG3atCnFtoEDBzJw4EAbRGRl7vcT+qR4SIgBVy/7xiOEEEKIvEuvhzoDoXov2LsAtsyGa8dh8UBoMg66z7Z3hCK/C9sPPw140Hv03GYoJassCJGf2bWF3uG5eILh/oR80u1eCCGEEJnh7AYtJsHzh6DFZLVt7wK4fdGuYYl87vxW+KG3+k3qfH/pZmmhFyLfk4TelnS6h7rd37RvLEIIIYTIX9wLQed3oEI70EwqqRciO06thp8eh4RoKN8GBv6gtss4eiHyPUnobc08MZ7MdC+EEEKI7GjyjHo+sEhmvxdZd3gJ/DpEDQGt2gMGL4NyLUGnV3M1RF6xd4RCiByQhN7WzEvX3btt3ziEEEIIkT9V6QJ+gRB3B44stXc0Ij/Z8y2sGAdaEtQdBE8sUkM6XDyheA1VRlrphcjXJKG3NelyL4QQQoic0BugyVj1evc3MuO9yJimqUkV/3lJvW/yDPSZB4aH5sMu1UA9yzh6IfI1SehtTbrcCyGEECKn6g9VE5ldOwYXt9s7GpGXaRoEvw4b3lHv274K3T5Qqyg8zDy7vST0QuRrktDbmru00AshhBAih9wLQZ0n1evdX9k3FpF3mZLgz8mwc65632UWtP+fmqj5UeaE/spBMJlyL0YhhFVJQm9r5hZ6WbZOCCGEEDnRZJx6PrkK7lyybywiZ5KM1p9fKfo6LB8NB39UE971+RKaP5t2+WLVVa+P+Ei4edq6sQghco1TxkVEjljG0EtCL4QQQogcKFEDyrWGC1th33fQcYa9IxLZYTLBzwPh3EaVVFfqoB5lW6gJ6zJL0+DqUTi1Bv5bc7/rvAZ6ZxjwHdTok/7+BicIqAehO9S+xarmpFZCCDuRFnpbky73QgghCrAtW7bQq1cvSpYsiU6nY+XKlemW//333+nUqRPFihXDx8eH5s2bs3bt2twJNj9oOl497/8BjPfsG0t+YTLB3TC4sB0O/gwb34Pfx8FfU+yzDODhX1QyD3D9hOoe/2M/+KAc/DQAdn0FN06nPvmh8R78Fwx/vwCf1ISvWsHGdyBsH6CpBH3obxkn82YyMZ4Q+Z600Nuapcu9LFsnhBCi4ImJiaFu3bqMHj2a/v37Z1h+y5YtdOrUiffeew8/Pz8WLlxIr1692L17N/Xr18+FiPO4qt3AtyzcDYUjy6HBMHtHlHckGdGdXkvFq6vRr9mkvqPbF+BOKCQlpL6PwQW6f5h7McbegpA31Ou2r0KxanBmPZxdD1HhcCZEPUD9OVfqgK5cWwJvbMaw9Gc4vwUSH7qR4+QOFdurpQ0rdwafklmLRybGEyLfk4Te1jzur0MvLfRCCCEKoG7dutGtW7dMl//000+TvX/vvff4448/+OuvvyShB7WEXeMxsO5N2PO1mv0+tQnPCpqYG7BkGE6hO6gFcOWRz/VO4FsGCgVCoXLg6g07vlDfYbXuUKFd7sS5/i31m7BYNWjzMhicoVZ/1Rp/7TicWacS/NCd6obE/oU47V9IvYeP4VMKqnRVj/Ktwdk9+/GYE/qIo2CMy1qXfyFEniAJva2ZW+iNsfIfpRBCCJFFJpOJqKgoChcunG65+Ph44uPjLe8jIyMBMBqNGI3GHMVg3j+nx7Ga2oNw2jQLXcQREs9vQyvTLMuHyHSd4qMgOkJNsubmB26+KjnOS64dx2npEHR3L6G5eBPmUZMS1ZqgK1IB/Mqi+ZVTLdePxK2Pi8JwIAht5UQSx24BNx+bhqkL249hfxA6ILHrh2gmwPTQ91+4CjSpAk2ehYQYdBe3ozu3Ad35Ldy5l4RXwwHoqnaD4jWT38TJyd9LzwCcPIqii71BYtghNHOCb2N57t+UlThivaRO1j+vteWx/5EdkKuPuoCYEtVM985Z7AolhBBCFGAfffQR0dHRPPHEE+mWmzVrFjNnzkyxPTg4GA8PD6vEEhISYpXjWENd36aUu7mZq3++xb7yk7J+AE3DLfEOe1d+jZvxNu7G27gZb+NmvIV7gvn1bZxNcSl2TdS7kWDwwGjwxOjkgdH82uBBgsGLBCdv4p19iHd68Eg0eNikJ4H/3QM0vPAVOlMc0a4l2F1hCtFupSAe1Up/JRo4ev+RnCGpBe1d/sEz8jJXFo7gUOBYq8dnoZloe+pN/NC4VKglB47egaP/ZGLHNlC2jXoZDewPBUKtGlpTp1L4c4MTIT9yrvhVqx47I3np35Q1OWK9pE45Fxtrmzk7JKG3NZ1OrR0bc111scrq2CYhhBCigFq8eDEzZ87kjz/+oHjx4umWnTZtGlOnTrW8j4yMpEyZMnTu3Bkfn5y1vBqNRkJCQujUqRPOzs45OpbVXA2EBW0peXc/3VvVy9rvi9hb6JcNxXB5T6aKay5egIYuIQYAJ1McTqY4MGZ+BR9N7wyeRcGjKJpncfAsilakEqZ6w9T2rNI09Ds/R3/wM3RomMq1xrX/9zR38srSn5WubgDaj70JvLWVUh2eQavSNeuxZIJ+7wIMhy6iufrgP/xbunul//f5Ybb++6ffehy2HKZmoTiqde9u9eOnJk/+m7ICR6yX1Ml6zD3HrE0S+tzgUeR+Qi9L1wkhhBCZ8euvv/L000+zbNkyOnbsmGF5V1dXXF1dU2x3dna22g82ax4rx0rXg8CW6C5ux/nQIujwRub2i4qAn/rC9RNo6MCrBDrfUuAdoG4KpPKsc/VS+yYlQtxdiLujHvfuPPT+rnp/77ZqwIi5fv9xA+Ij0ZmMatK3qHAebqc3bPsYGo6EFpPAt3Tm6mCMg78mw5Gl6n3jp9F3fR+9wdnS/TzTf1YV26hz7/gCp3+mQrkW4Fkkc3FkVtRV2DwLAF2H6TgXKpWtw9js71/ZxgDorxxEn8t/v/PUvykrcsR6SZ2scz5bkIQ+N8jSdUIIIUSm/fLLL4wePZpff/2VHj162DucvKvpM3BxO+wPUhOsZTRPz+2LsKgP3D6P5uXPhjLP06b/2Mz/yDQ4qWQ3qwmvMQ5ib0D0NZXgx1yHmGtw/E+4cgB2z4e9C6DuU9DqBShSMe1jRUXAr0PUMm06A3T7AJrksKt8+9fhdAhcPwmrpsLAIOsODwh5A+LvqiXlGo223nGtpeT9petunVWNTx7pz1chhMhbZB363GD+j/GetNALIYQoWKKjozl06BCHDh0C4Pz58xw6dIjQUDUOeNq0aQwfPtxSfvHixQwfPpw5c+bQtGlTIiIiiIiI4O7du/YIP2+r2gN8Sqtk+djv6Ze9/h983xVunwe/QBJHrFJjzXODs5tqfS/VAKp0hvpDVOI+dgMMWwnlWqvJ4Q7+CHMbwfLRatb1R105BN8+ppJ5Nz8YtiLnybw5vn5fqTmPjq+Eo7/l/Jhm57fCv0sAHfT8WK1SkNd4FIbCFdTrKwftG4sQIsskoc8N5oQ+VtaiF0IIUbDs27eP+vXrW5acmzp1KvXr12f69OkAhIeHW5J7gG+++YbExEQmTpxIQECA5fH888/bJf48zeAEje+3+O7+Wi19lprww7CwG0RdUculjV4DfoG5F2dadDq1hvrIv2F0MFTuAppJJdRftYTFT8GlvarssRXqhkRkGBStom4GVGhrvVhK1le9HABWvQiR4Tk/ZmKCOhaolvlcmkE+Wyzr0R+wbxxCiCyTLve5QbrcCyGEKKDatWuHllaiCQQFBSV7v2nTJtsG5GgajIRNH0D4Ibi8F8o0Sf556C74+YkHXb6H/q66zOe1JajKNoUhSyH8X9j2MRxbCf+tVg//OhDxrypXsQMM+B7c/awfQ+sX4dRq9V3+ORmGLMtZ1/tdX8KNU+BRNPNzHNhLqYZwZBmE7bd3JEKILJIW+txgXoteutwLIYQQwpo8i0Dtger17q+Tf3Z2A/zYTyXzZVvAiD+tP+GbtQXUUWPYJ+2DekNVN3hzMt9sIgxeaptkHsDgDP2+BoMrnAmBAz9k/1h3QmHzh+p153fUikd5maWFfl/aPT2EEHmSJPS5wUNa6IUQQghhI03HqefjKx90FT/xFyx+EoyxUKkjDP0N3HztFmKWFa0Efb+E5w5B65dgwELo+p4aZmBLxas9aE1f+xrcvpC946yZpr77si3UZH95nX8ddfMk5jrcvWTvaIQQWSAJfW4wt9DLsnVCCCGEsLaAulC2OZgSYf9COLwElo6ApASo3hue+gVcPOwdZfb4lVEJdq3+uXfOZs+qRDwhGlZOBJMpa/ufWgMn/1YJco851p0x31ac3aBELfVaut0Lka9IQp8b3GWWeyGEEELYUJP7rfQ75sKKcaAlQb0hqmXbycW+seU3egP0nQfOnnBxm1pWL7MSYmH1/cn1mj0LJWrYJkZbsHS7l4ReiPxEEvrcYOlyLwm9EEIIIWygei/wLgnGGPW+6XjoPdf2XdQdVeHy0OUd9XrdTIg4AqakjPfb9rEaP+9TCtq+atsYrS03Z7q/fpLAGxsz950KIdIl/8vnBnOX+/hISDKqSVeEEEIIIazF4Aytp8LqV9SY8/b/yx9dvfOyhqPg5Co4sw6+aqW2ObmBswe4eN5/9lAt+S4e6v2pf1S5rrPA1ct+sWeHOaG/chCSEm13Myj6Ok4/96NezHWSDlSD5uNtcx4hCghJ6HODmy+gAzTVSu9dwt4RCSGEEMLRNBkL9Yep8dAi53Q66P0F/DQArh1T2xLj1CO9YZSVOqq5C/KbopXBxRsSotRyeyVqWv8cmgZ/PYcu5joA+u2fQKOR8ndWiByQhD436A1quZJ7tyD8MHh3tndEQgghhHBEkhhZl09JmLBdJfEJMephjFVj5Y0xjzzHgmaCOk/mz94RegOUrAcXtsLlfbZJ6A8sglP/oBlciNe54xYdAfuDoJm00guRXZLQ55bAFmrG0yVDoMfH0GCYvSMSQgghhBAZ0enA2V09PIvaOxrbKt1IJfRh+6HhCOse++ZZtZwfYGr3P07+d5F6lxaqeQcaDM+/KzEIYWcyKV5u6fe1mrAmKQH+nAT/vKzG0wshhBBCCJEX2GpivKRE+H2c6s1QrjWmps8SWrg1mm9ZiL4K+7637vmEKEAkoc8trl4wcBG0f0293/MN/NgPYm7YNy4hhBDiEdeuXUv388TERPbs2ZNL0Qghco05ob92XA0vsJatcyBsH7j6Qt/5oNOj6Z1IavWi+nzbJ9Y9nxAFiCT0uUmvh7avwFOLwcVLdWn6pj2E/2vvyIQQQgiLgICAZEl97dq1uXTpkuX9zZs3ad68uT1CE0LYkk9J8A4ALcl6v08v74fNH6jXPT4CvzKWj7TaT0Ch8hB7A/Z8a53zCVHASEJvD9V6wNProHAFuBsK33WGo7/ZOyohhBACAE3Tkr2/cOECRqMx3TJCCAdh6Xa/P+fHSoiB38eqGwS1HofaA5N/bnCGtq+q19s/g/ionJ9TiAJGEnp7KV4dxm6Aih0g8R4sHw3rZoIpyd6RCSGEEBnS5cdZvIUQGSvVQD1bI6EPfh1unQWfUtBjTuqz/9ceCEUqqdWgdn+d83MKUcBkK6G/dOkSly9ftrzfs2cPU6ZM4ZtvvrFaYAWCeyEYsgxaPKfeb/sYfnkK4u7aNy4hhBBCCFEwWVro9+XsOKfWPJjsru889bs3NQYnaPt/6vWOL+R3sBBZlK2EfvDgwWzcuBGAiIgIOnXqxJ49e3jttdd46623rBqgw9MboPPb0H8BOLnB6WD49jG4fsrekQkhhCigdDodUVFRREZGcvfuXXQ6HdHR0URGRloeQggHVbI+oIM7oRB9PXvHiL6uVnUCaDYRKrRLv3yt/lC0KsTdgV1fZe+cQhRQ2Urojx49SpMmTQBYunQptWrVYseOHfz8888EBQVZM76Co85AGL0GfErDzTPwdRs1ligp0d6RCSGEKGA0TaNKlSoUKlSIwoULEx0dTf369SlUqBCFChWiatWq9g5RCGErbr5QtIp6fSUby9dpGvz1HMRch+I1oMP0jPfRG6Dd/Vb6nV/CvTtZP68QBZRTdnYyGo24uroCsG7dOnr37g1AtWrVCA8Pt150BU3J+jBuk5o85NxGCJkOR3+HPnPBv7a9oxNCCFFAmHvhCSEKqFIN4cYpNY6+Spes7XtgEZz6Bwwu0P8bcHbL3H41+kLx2WrJvF3zoP3/shy23d08g95kzLicEFaUrYS+Zs2afPXVV/To0YOQkBDefvttAK5cuUKRIkWsGmCB41UMhq2AQz/D2v9B+CH4ph20nAJtXs78f4pCCCFENrVt29beIQgh7KlUAzi8OOsT4908C2umqdePvZG1Bim9HtpNg6XDYOc8aDoePApn7fz2dGQ5zr+NoWbRjkCf3DmnpqlhumfWwdn1cGkPNBwJnd9JfQJC4ZCyldB/8MEH9OvXj9mzZzNixAjq1q0LwJ9//mnpii9yQKeD+kOhUif45yU48Sds/Ug99/4Cyjazd4RCCCEcWGJiIklJSZbeeABXr17lq6++IiYmht69e9OqVSs7RiiEsKmHl67TtMwlh0mJ8Ps4MMZAudbQfFLWz1utJ5SoDVePwM65meuunxeYTLBlNgClb+8EUyLgbJtz3bsD5zfDmfXqEXk5+ec754Kr94MhDLnBeE/1yvgvWA0jrtQx984tspfQt2vXjhs3bhAZGUmhQg9mrBw3bhweHh5WC67A8y4BT/4Ix/9Uif2N/+D7rtBkrPoPztXb3hEKIYRwQGPHjsXFxYWvv1ZLSEVFRdG4cWPi4uIICAjgk08+4Y8//qB79+52jlQIYRMlaqku8/duw+3zULhC+uXj7sKK8WpmfFdf6DtftbhnlV4P7afBr4PV5HjNJoJnPuj9eyYErp8EwCUphsTLe6FiG+sc22RSPXbP3k/gL+0B7aFlrg2uUK6lSqKN92DD27BpFngWg8ZjrBNDWnFd2gWHf4FjKyH+/mSpJ/6EsRuheDXbnVskk62E/t69e2iaZknmL168yIoVK6hevTpdumRxnI3IWI3eUL61Wsvz4E+w5xs4tRp6fgqV5Q6YsKKoq/DLk+BZHAYvke5aQhRQ27dvZ+7cuZb3ixYtIikpidOnT+Pr68urr77K7NmzJaEXwlE5uYB/HZWgX96ffkJ/7QT8OkStN29wVUvU+ZXJ/rmrdoeAuhB+GHZ8Bp3ywQpa2z8HQNM7ozMZ0Z1ea52EPikRvm0PEf8m316kskrgK3WEwBbg8lCDamI8bPkQVr0IHkWgZt+cx/Gwm2fh3yVw+Fe4c/HBdt+y4OYDV4/C0uEwdgO4eln33NakmewdgdVka5b7Pn36sGjRIgDu3LlD06ZNmTNnDn379mX+/PlWDVDc514I+nwJw1aCXyDcvQQ/Pw6/PwPxUfaOTjiCe3fgp8fhykE4vRYiw+wdkRDCTsLCwqhcubLl/fr163n88cfx9fUFYMSIERw7dsxe4QkhckPpRuo5vXH0x1bAtx1UMu9TWq3YVL1nzs6r00H719TrPd9C9LWcHc/WwvbDxW2gd8LU/nUA9GeCrXPsC1tVMm9wgao9oMfH8PxhmLwPur2vGvZcHukd3f5/0HAUoN2faHtzjsNwToxBfyAIvusMXzSAzR+oZN7FWw0THrlKxTVsJXgHqAkV/35BDdfIiaREWPuaukFw8KfsL6Nodv0UbPoAp29a0/vQKJy+e0xNQn52o+rdkE9lq4X+wIEDfPLJJwAsX76cEiVKcPDgQX777TemT5/OhAkTrBqkeEjF9vDsTtjwrpoB9N9f1d2w7rPtHZnIzxJi4Zen1Jg1s/DD4FvafjEJIezGzc2Ne/ce/LjZtWsXs2fPTvZ5dHS0PUITQuSWh8fRPyopEdbPgB1fqPfl28KA78GzqHXOXbmzOn/YfrWMc5d3rXNcW7jfOk/tgZjqDUW3/i30N/6DW+cyHqqQkeN/qOe6g6D355nbR6eDHnMg9qbq/v7rYJVwl6yX9fNHhmMImU6Xo79jOHJ/KW2dHio+pmKq2j35DQWvYjBgIQT1gCNLIbA5NBqd9fOC6tL/5yTVpR/ufxc6KNNEnbdqdyhaOePepNdOwvGValjA9ROqCubPIv5Vj+2fqd4lZZupXKtCO9VDRW/IXuy5LFst9LGxsXh7q/HbwcHB9O/fH71eT7Nmzbh48WIGe4scc/GEru/BE6qXBP8uAWOcfWMS+VeSEZaPgtCdatxbmfuTLoYftm9cQgi7qVevHj/++CMAW7du5erVqzz22GOWz8+ePUvJkiXtFZ4QIjeYE/rww+q3glnMDfix74NkvuXzMPR36yXzcL+V/v6ydXsXQFSE9Y5tTbfOq6QZoMVkcPPlplcV9f6/tTk7dlIinPhLvc5qt3m9Afp/qyYnTIiGnweorvJZOffOeTC3MfojSzFoiWjFa0Lnd2HqCRj6G9QekLJ3AKgkvuOb6vXqV+HKoazFDqplf9VUlczrDNB4rBqGgQaXdsO6N+HLxvBFQ9WCf3GHitns2gnYOAu+bArzmqo5Ba6fAL0zVO5MYs8vWFf9QxL7zIe6g1WvgqR4NdnguhlqhbHZFWHpCNi3UP0552HZaqGvVKkSK1eupF+/fqxdu5YXXngBgGvXruHj42PVAEU6qvUE3zKq+/3Jv9U/LCGywmSCPybCf2vAyU2Nm4/4V01yIgm9EAXW9OnT6datG0uXLiU8PJyRI0cSEBBg+XzFihW0bNnSjhEKIWyucAVw81UT3l09plp4w/bDkuFqZnVnT+j7JdTsZ5vzV+wAZZqqBG7rnLzZG3Xnl2osdqWOUKImGI1E+NanWPQJNd9Vsxz0Wg7dAbE31LDbcq2zvr+zGzy1GIK6Q8QR+LEfjAkGb/8MzrtbJdNXjwJgKtWIrZ49aDFwMs7OmZy5v8VzELpLzXy/bASM2wzufpnbV9PU0t37FwI66P/Ngxzn7mX1vZ5aDee3qKEeO+eqh3thqNAWrh5XXf7N9M6qR0HNvlC1G7gXQjMaiQn7B61Wd6g/WJ3zxn9wbpN6nN+qJoQ8vlI9AJ5e/2AYSh6TrYR++vTpDB48mBdeeIHHHnuM5s2bA6q1vn79+lYNUKRDr1fdXbZ8qNatl4ReZIWmwdppqoeHzqB6fAQ2f9B1SRJ6IQqstm3bsn//foKDg/H392fgwIHJPq9Xr54sUyuEo9PpVCv92Q0qkY/4V020lpQARSrBkz/bdiZzcyv9oj5qQmiATm+rRDUviLmpxnWDSmDvu+pTj9phi+HidnUzxM03e8c3d7ev1gMM2VwCz80HhvwG33eG2xfgpwEwalXqMcXcVC3fB1XvLNwLQceZJNV+ijur12TtvDqdmhzx6zbqvH9MhCd/ytxkyxveUcOKAfrMTZ7f+JZWq301GQtxkerv5qnVau6ne7fUnA6g5hyo+BjU6Hs/iffLON5iVdWj6TOqR0rYgfsJ/ka1gkFA3ax9B7koWwn9gAEDaNWqFeHh4ZY16AE6dOhAv36Zv0u3ZcsWZs+ezf79+wkPD2fFihX07ds3zfKbNm2iffv2KbaHh4fj75/B3SZHVe9+Qn92o7prJWOeRWZt+Qh2f6Ve950PVe6vUFGiFqCDqHA16713CbuFKISwn+rVq1O9evVUPxs3blwuRyOEsAtzQr/xXTUmG9TkbP3mZz9RzYrybaH1S7D1I5XUh+6CgUFQpKLtz52RvQsg8Z5K9Mo/mNE+xs0frUgldDfPqO8uOz0YTElq2WqAGjnsAeFdAoatgO+6qLmSfhmsusybb4yYTCqJX/emapUGqD8MOs5USwYajWkfOz3uhWDgD/B9F9WTeOeX0GJS+vtsma3+rAG6f6Qm3EuLm49qda/ZV3W3v7RbTSJYqJxK4nPy99PgDGWbqke7V9XKAdm9qZILspXQA/j7++Pv78/ly5cBKF26dJbv1sfExFC3bl1Gjx5N//79M73fqVOnknXtL168eJbO61AKV4DAluou4OFfoM3L9o5I5Ad7F8DGd9Trrh9A3ScffObqpSYZufGfuhvv3ck+MQoh7GbLli2ZKtemjZXWWRZC5E3mcfSxNwEdPPYatHoxe2vMZ4dOBx3eUBOhrRivfpd83RZ6fWrfnqnGew96DbR4LkXLs6lyFww3z8CpNdlL6EN3Qcw1lZSWt8L/s4UrwNDlsLCHmpH/tzGqZ+bVY6p7/eW9qlyJWmom/bJNc35OgFINoMt78M9L6oZB6cZpH3vnl6p1HqDzO6oVPrMMTlCupXrYgpOrbY5rJdlK6E0mE++88w5z5syxzHLr7e3Niy++yGuvvYY+k//Iu3XrRrdu3bJ8/uLFi+Pn55fl/RxWvSEqoT+0WN3FlLXDRXqO/garXlKv27wCzcanLBNQVyX04YegsiT0QhQ07dq1Q3f/WqKlseyQTqcjKSkpN8MSQuS2Mk3BxQv0TjDgOzVW3B6qdIHx9xPR0J3q+cJW6Po+OLvnfjyHFqvx7X5lVbfuR2iVOsOuL+F0sGptz+ps6ebu9lV7gJNLzuMF9dtu0GK1RPHJv2FBR/U7TzOpP+P2r0GTcSo5tqbGT6s/s6O/wbKRMH5rygkU936nxs2DiqPFZOvG4OCydXvttddeY+7cubz//vscPHiQgwcP8t577/HFF1/wxhtvWDvGFOrVq0dAQACdOnVi+/btNj9fnlejj5qY5NY59Q9GiLScWQe/PwNo0GjMgxlkH2UeJyTj6IUokAoVKkSZMmV44403OH36NLdv307xuHXrlr3DFELYmkdhmLxfrTFur2TezLcUjPhbNV6hg/1B8G0HuP5f1o+VGK/GjGeHKUlNwgbQbGKqCbBWpqlqXb9360Hrd6aPb3owc36NPtmLMS3l28DjCwAdXDmgkvma/WHSXmj+rPWTeVANjb0+gyKVIeoK/D5O1dHs0GLVSwCg1QvS2zgbsvWn9sMPP7BgwQJ69+5t2VanTh1KlSrFs88+y7vv2matyICAAL766isaNWpEfHw8CxYsoF27duzevZsGDRqkuk98fDzx8fGW95GRkQAYjUaM2R0Tcp95/5weJ8f0rhiq90H/72JM+38kqWTjHB0uz9TLihyxTpC1eunC9mFYMgydyYipRl+SOr0HiYmply1WEydAC/+XxFz+zhzxz8oR6wSOWS971SmvfYfmeW2+//57PvzwQ7p3786YMWPo2rWrpeVeCFFAZDQrem4yOKku+OVaqsTw2jH4pq3qJl5vUNr7JcSqxPridrXE2eW9auKznh9Dw5FZi+HkKtWI5uaX9hhvvRNU6gRHl6tJ28o2y/zxL+9R8xi5+qg10a2tRh+V1B/+Vc3CX6mD9c/xKFdv1cX/28fg7Ho1Tr7tK3D0dzVhHkDT8dDhTelpnA3ZSuhv3bpFtWopZ7WsVq2aTe/YV61alapVq1ret2jRgrNnz/LJJ59Y1st91KxZs5g5c2aK7cHBwXh4pLJ2YjaEhIRY5Tg5UfheeVoDpqO/sZZ2JBlyPgNoXqiXteWHOjklxqADjE6emd4n3XppGkWiT9Hk/Gc4JcVyzbsWu5x7o61Je31Up8QYegC6OxcJ+XNZlmKxlvzwZ5VVjlgncMx65XadYmNjc/V8GXFxceHJJ5/kySefJDQ0lKCgICZNmkR8fDwjRoxg5syZODnZoCVHCCEyo+Jjqgv+72PV8mUrx6su+N1ng4snxEerJXgv3E/gw/aDKZUbp389r8bDZ3Z5OU2DHZ+r142fVvMOpaVqN5XQ/7cGOqXMRdJk6W7fzXZjt2sPyP05CErUUDdQVk6Aje9BYhxs/0z1EmgwQg2fkGQ+W7J1Na5bty5z587l888/T7Z97ty51KlTxyqBZVaTJk3Ytm1bmp9PmzaNqVOnWt5HRkZSpkwZOnfunGxivewwGo2EhITQqVOnzK/LaCtaN7T5i3G6fZ6uZePR6mZ+ksFH5al6WUm+qVNUOE7ftoW4O2hlm6NV7YGpag/wKZVq8TTrpWkQcRj98ZXoT/yB7u4lQK0lWmjwb3RzyThB1y69j+7ORTrXKYFWLvcmvso3f1ZZ4Ih1Asesl73qZO49lheVLVuW6dOnM2zYMMaMGcP777/Piy++SOHChe0dmhCiIPP2h2Er1ao9m99XSzhf2q26ul85BNojc3x4l1Qt+4EtILCVmtl9x+ew5v9UUt96ampnSS50l2rdN7iq5c3SU6mDWhb4+km4dR4Kl8/4+CbTg4Te2t3t84J6g9UNloM/wtY5aludJ6HnJ5LM50C2EvoPP/yQHj16sG7dOssa9Dt37uTSpUv8888/Vg0wI4cOHSIgICDNz11dXXF1TXl3y9nZ2Wo/1qx5rBypPwQ2vIPTkV+h0fAcHy7P1MuK0qzTvdtw8yzcPKOeSzWEql1zP8Bts9V4K0B3cTtc3I4h+H9Qsj5U6wnVe6k1Mh/h7OyMs5OTmq302O9qHc5b5x4q4AnVe6HvOgu9h1/mYgmoC3cu4nTtGFTOhe5YjyhQf//yOUesV27XKa9+f/Hx8fz22298//337Ny5kx49erBq1SpJ5oUQeYPeoJYVC2wBvz2tfseZ+ZVVK0EF3p/9vFD55Eljp7fA2UPdDFg/UyX17f+XfmJpbp2v+xR4ZbDKlnshKNtczSr/39rUJyF+VNh+iAxTk9RVzP3fXrmi+2x1w+XqEajeG/rMy/qkgSKZbCX0bdu25b///uPLL7/k5MmTAPTv359x48bxzjvv0Lp160wdJzo6mjNnHvzDO3/+PIcOHaJw4cKULVuWadOmERYWxqJFiwD49NNPKV++PDVr1iQuLo4FCxawYcMGgoODs1MNx1N3EGx4V40PunVOLVEhLPQmo7pLeveC+g//xpn7CfwZNVNpssJO8MwWKFEz9wK8dgIO/qReDwyCyCtw4m810eGVg+qx4W01qUj1nlCtFxSvjfe9MPSb34eTf6qZ6c2c3KFKZzXZSeXO4JLFISYBddWkLDIxnhAFzp49e1i4cCG//vor5cqVY9SoUSxdulQSeSFE3lS+teqCf2QpeBRRSbxfmfT30emg/TS1Hvu6GbDlQzDGqiXTUkvqr/8Hp/4BdJmfhb1q1/sJ/ZrMJfTHV6rnKl0frBPvaJzdYdQq1duh4mO2mYivgMn2N1iyZMkUk98dPnyY7777jm+++SZTx9i3bx/t2z+Y7MHcNX7EiBEEBQURHh5OaGio5fOEhARefPFFwsLC8PDwoE6dOqxbty7ZMQo039Jq8oyzG9SMkY+9bu+I8obEBAx/T6Hn4cXoDqe+/BIA3gFQpJJqrb96FP6eCqNW595aqyHT1Tii6r0erFnafCJEX1MXkBN/w7lNcPM0bPsEtn2Ck6sPj8U/1FXX4KqWmavZT10M0hvblZGAeupZEnohCpxmzZpRtmxZnnvuORo2VOtQpza87eHJcYUQwq68iqnfTVnV6gXVUr/6FTV7fWIcdJud8vffzi/Uc9XuULRy5o5dpRsEvw4XtkFcJLilM9xX0+C4jWa3z2vcfNVShMIq7HpLpF27dmmubwsQFBSU7P0rr7zCK6+8YuOo8rl6Q+4n9L9Au2nShSUhBpYMQ392PQCaqze6IpVV4l6kEhSpqP5TLlxBzcAJcPcyzG2iJlM59BM0yPnwhQyd26zWKtU7QcdHJk7xKq5mYG04Ul0MTgfDib/gdAi6+EhMOgNU7IC+9gA1gUp6F4usCLg/H8bNMxAf9eD7EUIUCKGhobz99ttpfi7r0AshHEbTZ8DJTU2St3eB6n7f+4sHv6OjrqpZ4QFaPpf54xatBIUrwq2z6vd5zb5pl71yAO6GqmGSlTtluyqi4JE+Do6mWk911yvyMpzfrLqyWIvJBNFXVcJ795J6RF9Ts4m6FwL3wurZ4/6zeyG1pEdutXA/KvYWLH4SLu9Bc/ZgV5kJNHrq/3B2cUl/P9/SagxV8Guq1bxqd/Asars4TSYIeUO9bjRa3WRIi5vPg5lJjXEkhh0keP95OvUeiN7aY3C9iqsJZKKuQMRRCGxu3eMLIfIs08NrBKchr83ML4QQOdJwhOoOvmK8mmDPeA/6fwMGZ9jzNSQlQOkmWVuCDlRjy865ahx9egm9eTK8Kp1VHEJkkiT0jsbZDWoNgH3fwcGfs5fQXztB2Rub0G8+rNbBNCfvd8NSX/IjXTpw97uf6BcBrxJqVlKvEslfe/uDR1HrjaOJDIef+sO14+DmR9KTv3Dt3+uZn0Gz6Xg4/Ivqeh8yHfrOs05cqTn6m+rW7uINbV/N/H7ObmilGmE8fM12sQXUVQl9+GFJ6IUQgJoo78svv+TDDz8kIiIiw/Jbtmxh9uzZ7N+/37K+fd++fdPdZ9OmTUydOpVjx45RpkwZXn/9dUaOHGmdCgghRFrqPKGWils+Rk0ynBgPfebC3u/U51lpnTer0lUl9KfXgikp9d6zmubYs9sLm8pS9tS/f/pLod25cycnsQhrqT9EJfQn/4Z7d1RCnVmHf8VpxXjqo8GlVD7XGcCnJPiWUS3Z3iXUHczYW2p29nu3Ifa2ek6IAjT1+t7t5LOup0anV0m9dwk1mUmrF1Sin1U3z8KPfeFOKHj5w7AVaIUrw79ZWIHB4KSW0Piuk7pLW28wlGuV9VgyYoyD9W+p162m2LYnQHYE1IX/Vss4eiEKmPj4eGbMmEFISAguLi688sor9O3bl++//57XX38dg8HACy+8kKljxcTEULduXUaPHp3h7whQE+T26NGD8ePH8/PPP7N+/XqefvppAgIC6NJFxlwKIWysRh94yh2WDIVTq2D+AYi7o7rOV+2e9eOVbQauvhB7Ey7vg7JNU5YJPwy3L6gJjSt3zmkNRAGTpYTe19c3w8+HD8+F8cYifSUbQLHqcP2Eav1tPCZz+x39HVZOQIfGTc/K+FVtiaFQWfAtq5J3vzIqQc5sK3piwoNk/t4tiLmhuuxHX4WoiIdeX4WYa2pCuJhr6hFxBA4sUi3lLZ/P/E2J8H/hp8fVMQqVh+EroVA5MGa1ZwFQpokat74/SE2QN34bOGXQXT+r9nyjxkt5l4Rmz1r32NYQUFc9S0IvRIEyffp0vv76azp27MiOHTsYOHAgo0aNYteuXXz88ccMHDgQgyFzc7R069aNbt26ZfrcX331FeXLl2fOHLVGcfXq1dm2bRuffPKJJPRCiNxRpTMMWQq/DFK9VQFaTMre3FQGZ6jcUf0m/29N6gm9uXW+cic1lFWILMhSQr9w4UJbxSGsSadTrfTBr6vZ7jOT0J9cBb+PBc2Eqd5QttGR7t16YsjJuGwnF9Xa7l0i47KmJHXnMioCbp+HHV/A5b2w7WPY971qvW7yTPpLr13cocbMx0dCidow7PeM1wjNSIc31ezyN06p2U1bv5iz4z0s9hZs/Ui9fuy1rC8rlxvMCf31k6onhozpEqJAWLZsGYsWLaJ3794cPXqUOnXqkJiYyOHDh9FlduhSNu3cuZOOHTsm29alSxemTJmS7n7x8fHEx8db3kdGqhVAjEYjxuzc1H2Ief+cHicvccQ6gWPWS+pkJ2Vaohu0FMOSweDmR2KNARk2EKVVL13FTjgd/Q3t1GoS2/4v+U6ahtOxFeiAxKo90fLYd5Iv/qyyyF51stX5ZAy9o6rzJIS8CWH74PopKFY17bJn1sGykWBKhNpPkNRtDqxZm2uhAuqOp1dx9QioA9V7q6Xa1r+tehqsmwG7voJ2r0L9Yepu58NOrYFlI9RSI2VbwKBfsjbUIC0ehaHLu7DiGdg8G2o9rlr8rWHrHIi7C8VrQt1B1jmmtfmUVMMgYm/A1eNQuqG9IxJC5ILLly9blqurVasWrq6uvPDCCzZP5gEiIiIoUSL5jeASJUoQGRnJvXv3cHdP/cbirFmzmDlzZortwcHBeHhY54ZpSEiIVY6TlzhincAx6yV1sg+nKh+ATk9iyMZM7/NovZwTTXRFj/76CTau+IF7rsUsn/nEhtL+9nmSdM6sPaeReDELQ0RzUX74s8qq3K6TrSaTlYTeUXkVV+s7nvoHDv4EndNYeuj8Fvh1iJq5s0Yf6DsfTOms1Z5bdDqo1kNNJHJkGWx8V42J//sF1Xrf/jWo2V/NoH94CaycAFqSKj8wyLotyXWeVN/hha3wz8sweGnmJ9dLy+0Lqrs9QOe38u7ygjqdaqU/ux4iDktCL0QBkZSUhMtDK4I4OTnh5eVlx4gyNm3aNKZOnWp5HxkZSZkyZejcuTM+PjlbztNoNBISEkKnTp1wtvaKInbiiHUCx6yX1Cn/SLded3+E0J10KG3E1PjBWHz9pllwCnSVO9G51+O5HHHGHPHPyl51MvccszZJ6B1ZvSEqof93ieo6/ujY99DdsPgp1apdpRv0X6DKZHkmexvSG6DuU1CzH+z/AbZ8qCbX+20MbPsUKrRVM4eCSrz7fJmy9T6ndDro8THMb/FgDfgavXN2zPVvq5soFdpBxQ5WCdNmzAm9jKMXosDQNI2RI0fi6uoKQFxcHOPHj8fTM/nYzt9//93q5/b39+fq1avJtl29ehUfH580W+cBXF1dLfE+zNnZ2Wo/2Kx5rLzCEesEjlkvqVP+kWq9qnaH0J0YzgZjaDFBbdM0OPknAPpa/ay/BLEVOeKfVW7XyVbnstMC4SJXVOmiuktHX1Xd6h8WdgB+HgDGGKjQXrVqW3vCN2tycoWm4+C5Q9D+dXD1gatHHiTzTSdA36+sn8ybFauixvEDrH4V4qOyf6ywA3B0OaCDTm/nvLXf1mRiPCEKnBEjRlC8eHF8fX3x9fVl6NChlCxZ0vLe/LCF5s2bs379+mTbQkJCaN5cls4UQuRjVbqq5wvbHvyOvHYCbp4Gg4v63S5ENkgLvSMzOKtW611fwqGfoOr9/0gijsKP/dTkcYGt4KnFav36/MDVC9q+rCb62/aJ6o7fZJxa4s7WiXHrF9X5bl+AjbOg63tZP4amQfAb6nXdp9R8AXmdOcarxyDJaLubJkKIPMOak+BGR0dz5swZy/vz589z6NAhChcuTNmyZZk2bRphYWEsWrQIgPHjxzN37lxeeeUVRo8ezYYNG1i6dCmrVq2yWkxCCJHrilaGwhVUT9OzG1VvT/Ps9hU7gJttbpIKxyct9I6u/hD1fGoNxNxUE+Qt6qPW0yzdBAb/mjdnV8+IR2E1L8CLJ6H11Nxp5XZ2h+5qGSV2z89ei/V/a+HiNjC4qnkA8oNC5dX6qUkJarZ7IYTIgn379lG/fn3q168PwNSpU6lfvz7Tp08HIDw8nNDQUEv58uXLs2rVKkJCQqhbty5z5sxhwYIFdluy7tKtWBbuuMjOq3m8N5UQIm/T6dQQV1DL18GDhL5GH/vEJByCtNA7uhI1IaAehB9S48+PrVQzlgfUhSHLwNXbzgHmM5U7qvH8x1aoCfrGhGR+QrukRAhRP2BpNgH8ytguTmvS6VQr/YWt6iaGf217RySEyEfatWuHpqU92WpQUFCq+xw8eNCGUWXe0bC7vLf6FCU9pA1ECJFDVbqonrP/rVXd7a+fAL3zg160QmSDXJ0KgvpD1fPuryA6AorXgGErrbOsW0HUZRa4eEPYftgflPn9Dv2k1rN3L6x6FeQnMo5eCFFANS5fGIDwWLgTm4cmjRVC5D+BLVSvx9gbDxp5KrQD90J2DUvkb5LQFwS1HleTbQAUqQzD/1Bd1kX2+ARAh/vj4NfNhPNb1Rrtty+qYQ3GODVW/mHx0bDx/pj7tq/kv3FSuZXQaxr6LR9QJeJP255HCCEyqaiXKxWKeqKhY//F2/YORwiRnxmcodL91Y1OB6vnmn3tFo5wDNLlviDwKAwdpsO5zdD7c7VGvciZxk/DocVqKMMPPVN+rjOAixe4eKqHKVGtNlCoPDQak+vh5pg5oY84AqakzA8zyKoDizBsnU11IDF0OFRsa5vzCCFEFjQuV4hzN2LYe/E2XeuUsnc4Qoj8rEpXOHZ/yU+9k1rOTogckBb6gqLFZBi6HHxK2jsSx6A3QN95UKoh+AWq5QGdHlofWUuC+LsQdUUtR3L7vNreaWbeXh4wLUUqgbMHGGPh5pmMy2fHnUuw9sFEgfptc2xzHiGEyKLG5VR32L0XpIVeCJFDlTuB7n4KVr6N9JoVOSYt9EJkV4maMHZD8m2mJEiIUYlvQgwkRN9/jlETEJZtZp9Yc0pvUJPhXdqtut0Xq2rd42sa/PU8JEShFa+Fdu0E+vOb4dJeKNPYuucSQogsanI/oT8WHkV0fCJervLzSQiRTR6FoVwrOL9FDYsVIoekhV4Ia9IbwM0HvP2hSEXVVT2whbobm1+TeTNbjqM/sAjOrgcnNxL7f8ulwi3V9i0fWv9cQgiRRQG+bhRx1UgyaTKOXgiRc33mQf9voe5ge0ciHIAk9EKIzLFVQv9wV/vHXocilTnt3wtNp1cTxlzJG0tXCSEKtoo+arLTPedv2jkSIUS+51cG6jwBeknFRM7J3yIhROZYEvp/U87in10PdbWndBNo9iwAMa4l0Gre74a25SPrnEsIIXLgQUJ/y86RCCGEEA9IQi+EyJxi1dTyh/F34fYF6xzzoa729J2XbPb8pJZTAR2c/BsijlrnfEIIkU0VvVVCf/jSXeKMSXaORgghhFAkoRdCZI7BWU0ECNbpdv9oV/uilZN/XrTyg7VZt0orvRDCvoq6QQlvVxKSTBy6dMfe4QghhBCAJPRCiKzwr6Oec5rQaxr89VyKrvYptHlZPR9bCddP5eycQgiRAzodNLo/2/3uc9LtXgghRN4gCb0QIvOsNTHegUVwdkOqXe2TKVETqvUENNgq69ILIezLvB79ngsyMZ4QQoi8QRJ6IUTmBdRTz+GHsz8xXkZd7R/V5iX1fGQZ3DybvXMKIYQVNAlUCf3+i7dJSDTZORohhBBCEnohRFaUqAE6A8TegMgrWd8/s13tH1ayPlTuDJoJtn2c9XMKIYSVVCzmSSEPZ+KMJo5euWvvcIQQQghJ6IUQWeDsrma7h+x1u89sV/tHtXlFPR/+FW5fzPp5hRDCCvR6HU3KFwZk+TohhBB5gyT0Qoisye44+qx2tX9YmcZQoR2YEmH7p1k7bx6gu7iNhue/hOhruX/yhBg4tQZ2zYf4qNw/vxAOpkn5IoAk9EIIIfIGSeiFEFmTnYQ+O13tH2VupT/4U/a6+9tLUiKGv56j9J3d6Pd+Y/vzaRrcOKMS+B/7wQfl4ZcnYc3/wa+DITHe9jE8ypQEUVdz/7xC2EDT+y30e8/fIsmUzblEhBBCCCuRhF4IkTXZSeh3zc9eV/uHlWsJgS0hKQG2f5b1/e3l2Ap0d0MB0P+32jbnMN6D0yHwz8vweX2Y21Al8Gc3QFI8+JYFFy84vwV+H6cS7Nxw/RSEvAmf1IQ5VWD317lzXiFsqHqAD16uTkTFJ3IiPNLe4QghhCjgnOwdgBAin/GvBegg6orqQu5VPO2ySUZY+z/Yc79lOqtd7R/V5mX4cTvsD4JWU8G7RPaPlRs0LdnNB92NU6r1vGilnB/blKTmJDi5Ci5shcS4B5/pnSGwhZpMsHInKFoFzm2CnwfC8ZWwpjh0+1AtrG1tsbfg6G9w+BcI25/8s7X/UysllG1q/fMKkUsMeh2NyhVi06nr7Dl/i1qlfO0dkhBCiAJMWuiFEFnj6g1F7iek4f+mXS7qKvzQ+0Ey3/ZVaDYxZ+eu0A5KN1bJ684vcnas3HB2PVw9gubsyS2PimrbqVXWOfaxFfD3FDgTor4P3zLQcBQ8tRhePQ8j/oQWk6BYVZW4V2wP/b8GdOrPZOsc68QBam6D/9bC0uEwpyr885JK5nUGqNINnlgENfurcstGQswN65w3KkLNEWAtCbHoTv5N2Zub4da57C/NKBxeUxlHL4QQIo+QFnohRNYF1IWbpyH8EFTumPLzS3th6TCICgdXH+j3NVTrnvPz6nRqLP3igbD3e2j5AngWyflxbWXbpwCY6g/l0pV7FI49Cyf/gZbP5/zYR5ap51qPq54Lxapl3OJe63GIvg5rXoUNb4NnMWg4IvsxXD9JzbBfcPr8JYh5aMK/ErWg3mCoPfBBD46Kj0HEEfX35vexMGR59oZemP27FFaMB72TutFTtZt6ePtn7TgJsXA6WPVc+G8tTsZY6gPM/w78ykKF9ur4FdqBR+HsxyscimWm+wu30DQNnS16uwghhBCZIAm9ECLrAurC0eUQkUoL/b6Faiy3yQhFq8JTP+esm/2jKndS3bbDD8GuL6HDdOsd25rC9quu8HonTE2fJWLDRupeXgSXdmc8VCEjsbfgzHr1us0rULxa5vdtNh6ir8K2j1ULv2exrN9sibkBIW/ifOgnLIMHPIpCnSeg7iAIqJNyH1dv1VL/7WNqbP+Wj6Ddq1k7r9mxFbDiGdBMkJQEp9eqx99ToFQjldhX65H2TY6EWNWz4dgK1bPAGGv5SPMty81Ed4rcO4fuTigc+EE90Km/9xXbqyS/bDNwcs1e/CLfq13KFzdnPbdiEjhzLZrKJbztHZIQQogCShJ6IUTWpTYxnjEOVr+sxnUDVO+tJsBztfIPXZ1OtUgvGQK7v4EWk8G9kHXPYQ3msfO1B4JPKeJcCmMKqI8+/CCcWp2zlvETf6kbJiVqZS2ZN+swXbWoH/wJlo+C4X+oBDUjpiSV3K6bCXF3AAj3bUixzlNwqtYNDM7p71+iBvT8BFaOh02z1HKEFR/LWuwnV8FvT6tkvv5QNYzjv9Wq50PYvgePDW9DofJQtbu6YRFQV91IOLbyfhL/UFd9v7JQoy/U7EtisdpsX72a7h3b4By2F85thLMb4foJdRMp/BBs+wSc3CGwuUrum0/MWW8Dke+4OOlpULYQO87eZPf5W5LQCyGEsBtJ6IUQWWdugb19Ae7dVuOYlw6/P25aD4+9Aa1esM2ka6CStOI14doxNYt6j4/BkIf+O7t5Fo7/qV63eM6yWavSDcIPwql/cpbQH/1NPdfqn739dTro+Zlqaf9vDSx+AkavheLV097nykH4eypcOaDe+9cmscuH7Pn3Ot2rZCKZN6s3CEJ3qBs/v42F8VvBp2Tm9j0dAktHqLH4tZ+AXp+rRLpEDWj9ohpTf2q1epzbBLfPq14cu75MeSzfslCzr3qUbPDg76rRqJ5dvKBKZ/UAdexzm1Ryf24TREeoGwS3L0LL51IeXzi8JuULs+PsTfacv8XQZoH2DkcIIUQBlYd+AQsh8g33QqpV806oWopsz7cQe0Ntf/w7qNTBtufX66H9/1Qr/YEf4MZ/0P9b8Ctj2/Nm1o7PAQ0qd1HJ5v0k0VS1O4bN76mkMD4aXL2yfuyoq6orP6gx8dllcIIBC2FRH7i8B37sD2OCU36H927Dhndg73eqTq4+0P41aPw0mkmDf//J+rm7fahuEEQcgWWjYOTfGd8QOLcJlgxVPRNq9IG+81O2inv7Q6NR6hEfrRLuU/+omxb3bt9P4vtAzX7Jk/jM8PaHuk+ph6bB9ZPqz9HJJcvVF47BMo7+vIyjF0IIYT8yy70QInvM3e43zVLJfInaMG6T7ZN5s+o91c0DF28I3QlftYITf+fOudMTdRUO/aJet5qS/LOiVaFwBbU2/Nn12Tv+8T9Ud/NSjaBQuZxECi4eMHiJiivqCvz0uBqfDyppPfQLfNEI9i4ANNUqPmmvGoefkx4Rzu5qPL2rD1zaBetmpF/+4g74ZZCazb9qd/XnntH5Xb2gRm/o9xW8dAZeOAZT/oXO70CphjnrPaLTqd4MzZ+Fxk9n/zgiX2tQthDOBh0RkXFcunXP3uEIIYQooCShF0JkjzmhB5XojQnOeYKZVbUHqC7bJRuoMd1LhsCql9R4fnvZ/ZVK2Es3gbLNk3+m06mEFNSY7+ywdLfPQev8wzwKw7DfwacU3Dilut9f3g8Lu6ux7rE3VMI/4i94/NuszyKflsIV1BwLADvnqnkBUnNpL/w8UE1cV6kjDAzKfPd+M4MT+Ja23RAQUSC5ORuoW9oPgF3nb9o3GCGEEAWWJPRCiOyp8ySUbwvdP4L+36jWXnsoXF6N/zaPVd/7LSzoANdP5X4scZH3u6ajWudTSyCr9VTP/62BJGPWjn/nkmrRRqfGfluLb2kY+hu4+cHlvbDgMTXO3dkDOs6A8dugfBvrnc+sei9oPkm9XvmsWvv9YVcOql4DCdHq/E/+JDPLizzl4W73QgghhD1IQi+EyB6/sjDiT2gy1v4tn04u0PltlZR6FIWrR+GbdmoWd03L3DHMY643zlJdzDO738P2B0H8XShaBap0S71MmSYqxrg7qit5Vhz7XT0Htsz8RHKZVbw6DF4KTm7qffVeMHGPmtzQluPEO86AMk0hPlJNeGfuXRFxBBb1Vd9n2eYw6FfVVV+IPEQSeiGEEPYmk+IJIRxHpY4wYbtao/zcJvhjopq4rOcn4OaTvGzMDTX2/uJO1Rod/i9oSQ8+D/9X7ZfZ5cgS42HX/S7kLZ5TE/elRm+Aql3VzYZT/0CFtpmvn7m7fW0rdbd/VNmm8MxWlVyXbmSbczzK4Kwm5/u6NUT8C2tehaYT1GR9cXfUXAGDl4KLZ+7EI0QWNAwshF4HobdiCb97jwBfuekkhBAid0kLvRDCsXj7w9AV0OFN0Bng6HL4ug2cWQ+Hl8Bfz8PcJjC7opo1fdeXqmu3lqRmQa/WUy29d+AHWDZSJeqZ8e9SiAoH7wCo80T6Zc3d7k+uynxPgBtnIPywqlP1PpnbJzuKVcm9ZN7MtxQ8vgDQqV4O33WC2Jtqnoahv6W8GSNEHuHt5kytUr6AtNILIYSwD2mhF0I4Hr0eWk+Fcq1g+Ri1HvlPqazZXqya6s4d2EI9m5dsO/4H/PY0nPhTTcj21M/g6p32+Uym+0vVAc2ezXicd4V2anz63UuqVfrhCQbTYu5uX7E9eBbJuHx+U/ExaPsqbH5f9RAoXhOGrQR3P3tHJkS6mpQrzL+X77Ln/C361Ctl73CEEEIUMJLQCyEcV5kmahb8VS/CqdVQrKpK3gNbQJlmaSfGNfqoCeJ+HQznN8MPvWHI8rTL/7cabvwHrr7QcGTGcTm7qwT25N9qtvuMEnpNgyPL1WtrzW6fF7V9BaKvwt3Lap15j8L2jkiIDDUpX5gF286zW1rohRBC2IEk9EIIx+buBwO+U0lxVibvq9BWTfr30wC4cgAWdoVhK9SM8A/TNNj2qXrdeHTmu4dX63k/oV8F7aelX/bqMbWknMEVqvXIfB3yG70Ben1q7yiEyJLG5dSNpzPXorkRHU9RL1mJQQghRO6RMfRCiIIhOzPxl2qolsTzKaVa4L/rAjdOJy8TuhMu71HJdtMJmT92lS5qPPzVI3D7QvplzZPhVe4Ebr5ZqoIQwrYKebpQtYQakrPvgrTSCyGEyF2S0AshRHqKVVFJfZHKEHkZvu+iJtEz2/6Zeq43CLxLZP64HoVV139QwwHSomkPEnpH7m4vRD7WtIJqpZdu90IIIXKbXRP6LVu20KtXL0qWLIlOp2PlypUZ7rNp0yYaNGiAq6srlSpVIigoyOZxCiEKOL8yMHoNlKyvZl8P6gnnt8DV4/DfGkAHzSdn/bjm7vMnV6VdJuwA3LkIzp6qVV8IkefIevRCCCHsxa4JfUxMDHXr1uXLL7/MVPnz58/To0cP2rdvz6FDh5gyZQpPP/00a9eutXGkQogCz7MojPgLyreBhGj46XH441n1WfVeULRS1o9Ztbt6vrgdYtNIBI7enwyvajdZi12IPKrJ/XH0x8MjuXvPaOdohBBCFCR2nRSvW7du/H97dx4fVX3vf/w9M5lMFrISshII+05QkBhwQ6O4XCptvcXlIlKXq6LXml9bxQVcWvGqtbRK5bpQvL1WrNatQlGMolWiyBJlCYGwhS0bS1aSTGbO748Jg5GwBGYyOZPX8/GYJnNyTubzPWP55D3nnO+54oorTnn9+fPnq0+fPvrd734nSRoyZIi++OIL/f73v9fEiRy5AuBnjijp+jelt2+RCv9x9NT7835xer8vrreUNMJzHf3mDz2n7X+f2yWtb7ldHafbA51WYnSY+iREantlnVbvPKCLB7fj8hsAAM6AqWa5z8/PV05OTqtlEydO1C9+8YvjbtPY2KjGxkbv8+rqakmS0+mU03lmn6If2f5Mf09nE4zjCsYxScE5rs4/Jps0+WXZQnNl/fY1uftcJFfiSOkE9Z5oTNaBl8tWtk7uwn/INeyaVj+z7PxSIbWlMsJi1Nz7ghO+RiB0/veq/QI1pmDah8czb948Pf300yotLVVmZqaee+45jR079rjrz507Vy+88IJKSkqUkJCga665RnPmzFFYWFgHVn3qxmbEa3tlnb7eTqAHAHQcUwX60tJSJSW1bpJJSUmqrq7W4cOHFR4efsw2c+bM0aOPPnrM8o8++kgRERE+qWvZsmU++T2dTTCOKxjHJAXnuDr9mCyXKX5AH1WHpal5yZJT2qStMUXXR2uCJPeWZVr6wbtyW0O9Pxu5a6H6SCqJyFTBR3k+Ktz3Ov17dRo6ekz19fUd+nod7Y033lBubq7mz5+vrKwszZ07VxMnTlRRUZESExOPWf+vf/2r7r//fi1YsEDjxo3T5s2bddNNN8lisejZZ58NwAhOLqtvvN5YtYvr6AEAHcpUgf50zJw5U7m5ud7n1dXVSk9P12WXXabo6FO8X/RxOJ1OLVu2TJdeeqnsdvuZltppBOO4gnFMUnCOq8uNyTBkzHtRIVW7dMXAMBkDL/csdzkV8sd7JUlpE/9LqX0v6tiiT0GXe6/86MjZY8Hq2Wef1a233qrp06dL8lxCt3jxYi1YsED333//MeuvWLFC48eP1/XXXy9JysjI0HXXXaevv/66Q+tujyMT463bXaX6pmZFhAb9n1gAgE7AVN0mOTlZZWVlrZaVlZUpOjq6zaPzkuRwOORwOI5ZbrfbffbHmi9/V2cSjOMKxjFJwTmuLjWmwVdJX89XSPFSadgkz7Idn3lm1I9IUEj/CZKt8/5z3aXeKz++XrBqamrS6tWrNXPmTO8yq9WqnJwc5efnt7nNuHHj9H//939auXKlxo4dq23btmnJkiWaOnXqcV8n0JfYJXWzKzUmTHurGrRyW6XG9+t+Rq/pb8F4yYwUnONiTOYRjONiTL5/XV/rvH8htiE7O1tLfnBq67Jly5SdnR2giiSXITU2uxXEf4sB8LeWQK+ipZ6J8Ky2o/eeHza5U4d54GQqKyvlcrnavGRu06ZNbW5z/fXXq7KyUuedd54Mw1Bzc7Nuv/12PfDAA8d9nc5wiV2a3aq9surFf36jqr5un7ymvwXjJTNScI6LMZlHMI6LMZ05f11eF9C/Emtra1VcXOx9vn37dhUUFCg+Pl69evXSzJkztWfPHv3v//6vJOn222/X888/r1//+tf6+c9/rk8++UR/+9vftHjxCe7h7Ed//KRYr3xj0+Gkvbohu09AagAQBHqNk8JipfpKaddKz/3uN33g+Rmz26MLWr58uZ544gn96U9/UlZWloqLi3XPPffo8ccf18MPP9zmNp3hErvu2w/oPxas0jeVNj1140VKiu6cE/hJwXnJjBSc42JM5hGM42JMvuOvy+sCGuhXrVqlCRMmeJ8facTTpk3TwoULtW/fPpWUlHh/3qdPHy1evFj33nuv/vCHP6hnz556+eWXA3bLOovFonqXRcs3VxLoAZw+W4g08HLpu0WeIF+/X2qslqLTpPRzA10dcEYSEhJks9navGQuOTm5zW0efvhhTZ06VbfccoskacSIEaqrq9Ntt92mBx98UFar9ZhtOsMlduMHJGpM7zit2nlQC/N36aF/G+qT1/WnYLxkRgrOcTEm8wjGcTEm37yePxzbETvQRRddJMMwjnksXLhQkrRw4UItX778mG3Wrl2rxsZGbd26VTfddFOH1+2tZWCCJGnF1v1qajbHqXUAOqnBV3m+blosrX/L8/2wH0ttBBfATEJDQzV69Gjl5R29U4Pb7VZeXt5xL5mrr68/JrTbbDZJkmEY/iv2DFksFs24uL8k6bWvS3SgrinAFQEAgh1/KZ6BYSnR6mY3VNfk0qqd3KYGwBnod7Fkc0gHt0sb3/cs43R7BInc3Fy99NJLevXVV1VYWKg77rhDdXV13lnvb7zxxlaT5k2aNEkvvPCCFi1apO3bt2vZsmV6+OGHNWnSJG+w76wuGthDw9Oiddjp0p+/3B7ocgAAQY6Zls6A1WrRkFhD31RYtLyoQuP6JQS6JABm5egm9ZsgbV4qGS4pro/nWnogCEyZMkUVFRWaNWuWSktLNWrUKC1dutQ7UV5JSUmrI/IPPfSQLBaLHnroIe3Zs0c9evTQpEmT9Nvf/jZQQzhlFotFMy7qrzteW6OFK3bo1gv6KjosuE5TBQB0HhyhP0NDYz2n/n26qTzAlQAwvSOn3Uueo/MWS+BqAXzsrrvu0s6dO9XY2Kivv/5aWVlZ3p8tX77ce7mdJIWEhGj27NkqLi7W4cOHVVJSonnz5ik2NrbjCz8NE4clq39iN9U0NOsv+TsDXQ4AIIgR6M/Q4FhDVou0pbxWuw/651YEALqIgVdIlpZ/ljndHjAtq9WiOy/qJ0la8MV2HW5yBbgiAECwItCfoYgQ6exesZKk5UUVgS0GgLl16yFds0C6ep6U1PlnxwZwfD/KTFV6fLj21zXp9ZUlJ98AAIDTQKD3gQsGeK6dX17EafcAztCwH0tn/UegqwBwhkJsVt1+oeco/f98vlWNzRylBwD4HoHeBy5suX3dl8X71eCkYQMAAOma0T2VFO1QWXWj/r56T6DLAQAEIQK9DwxJjlJilEOHnS59s4Pb1wEAAMkRYtNtF3iO0s//bKuaXe4AVwQACDYEeh+wWCyaMChRkvTpJq6jBwAAHteNTVd8ZKhKDtTrH9/tDXQ5AIAgQ6D3kYsG9ZDEdfQAAOCoiNAQ3XxeH0nSnz7dKrfbCHBFAIBgQqD3kfEDEhRitWhbZZ127q8LdDkAAKCTmJrdW1FhIdpSXquPNpYGuhwAQBAh0PtIdJhdYzLiJHH7OgAAcFR0mF3TsjMkSc9/WizD4Cg9AMA3CPQ+5L2OntPuAQDA9/z8vD4Kt9u0fk+1PtvMB/8AAN8g0PvQRS2BPn8rt68DAABHxUeG6vqsXpKkeZ8WB7gaAECwIND70MCkbkqNCVNjs1v52/YHuhwAANCJ3HZBX4XarPpmx0F9zd8JAAAfIND7kMVi0UWDPUfpl2/itHsAAHBUUnSYrhnTU5LnWnoAAM4Ugd7Hjl5HX8GkNwAAoJU7Luwnm9Wif22p1Le7DgW6HACAyRHofWxcv+4KtVlVcqBe2yu5fR0AADgqPT5CV2emSuJaegDAmSPQ+1ikI0Rj+8RL8hylBwAA+L47J/STxSJ9tLFMa0sOBrocAICJEej94KJBPSRJy7l9HQAA+IH+iVH6yVmea+kfeGe9ml3uAFcEADArAr0fTGiZGO/rbQdU19gc4GoAAEBn88CVgxUbYVfhvmr9+csdgS4HAGBSBHo/6JsQqfT4cDW53Mrfym1pAABAa927OTTzisGSpGeXbdbug/UBrggAYEYEej+wWCzfm+2e0+4BAMCx/n10usZmxOuw06VH3t/A3XEAAO1GoPeTI4F+ObevAwAAbbBaLfrtj4fLbrPo48JyfbihLNAlAQBMhkDvJ+f27S5HiFV7Dh1WcXltoMsBAACd0ICkKN12QV9J0iPvb1Atc+8AANqBQO8n4aE2ndu3uyROuwcAAMd398UD1Cs+QqXVDfrdR0WBLgcAYCIEej+a0HL7uk83cT96AADQtjC7TY9PHi5JenXFDq3fUxXgigAAZkGg96OLWq6j/2bHAdU0OANcDQAA6KwuHNhDkzJT5TakB95ZJ5eb+XcAACdHoPejjIRI9U2IVLPb0JfF3L4OAAAc38P/NkRRYSH6bneV/pK/I9DlAABMgEDvZxe2nHa/nOvoAQDACSRGhenXl3vuTf/MR5tVWtUQ4IoAAJ0dgd7PuH0dAAA4VTeM7aVR6bGqbWzWo//YEOhyAACdHIHez8b2iVe43abS6gZtKq0JdDkAAKATs1otmvOTEbJZLfrn+lLlFXJvegDA8RHo/SzMbtP4/ty+DgAAnJohKdG6+bw+kqRZ721QfRP3pgcAtI1A3wEuPHLaPbevAwAAp+AXOQOUFhuuPYcO6w8fbwl0OQCATopA3wEuGuiZGG91yUEdqGsKcDUAAKCziwgN0WNXD5MkvfzFdhXuqw5wRQCAzohA3wHS4yM0LDVaLrehJ5YUBrocAABgApcMSdLlw5Llchu6669rtOfQ4UCXBADoZAj0HeTRHw2TxSK9tXq3Pt7IBDcAAODkHvnRMCVFO7S1ok6T532p9XuqAl0SAKATIdB3kDEZ8br1/L6SpPvfXqeDnHoPAABOIjkmTO/cOV6DkqJUUdOon/1Pvj7ZxIEBAIAHgb4D5V46UP0Tu6mytlGz3ufesgAA4ORSY8P15h3ZOq9/guqbXLrl1VX6y1c7A10WAKATINB3oDC7Tb/790zZrBb949u9WrJuX6BLAgAAJhAdZtefp5+ja0b3lNuQHn53veYsKZTbbQS6NABAABHoO1hmeqzuuLCfJOmhd9ersrYxwBUBAAAzsNusevqakcq9dKAk6X8+36a7X1+rBqcrwJUBAAKFQB8A/3XJAA1OjtKBuiY9+M46GQafrgMAgJOzWCz6r0sG6PdTMmW3WbR43T7d8PLX3BYXALooAn0AhIZY9ezPRslus+jDDWV6r2BvoEsCAMCv5s2bp4yMDIWFhSkrK0srV6484fqHDh3SjBkzlJKSIofDoYEDB2rJkiUdVG3n9+OzeurVn49VVFiIVu88qJ++sEI7KusCXRYAoIMR6ANkaGq0/uviAZKkWe+tV1l1Q4ArAgDAP9544w3l5uZq9uzZWrNmjTIzMzVx4kSVl5e3uX5TU5MuvfRS7dixQ2+99ZaKior00ksvKS0trYMr79zG9UvQ23eMU1psuLZX1unHf/pSq3ceCHRZAIAORKAPoDsu6qeRPWNU3dCs+//+HafeAwCC0rPPPqtbb71V06dP19ChQzV//nxFRERowYIFba6/YMECHThwQO+++67Gjx+vjIwMXXjhhcrMzOzgyju/AUlRemfGOI1Ii9HBeqeue+lrvbB8K9fVA0AXERLoArqyEJtVv/v3TF313Bf6tKhCb67arZ+dkx7osgAA8JmmpiatXr1aM2fO9C6zWq3KyclRfn5+m9u8//77ys7O1owZM/Tee++pR48euv7663XffffJZrO1uU1jY6MaG49ONFtdXS1JcjqdcjqdZzSGI9uf6e/xl7gwm/7v56N179/W6ZOiCv330k363/wdys3prx+NTJHVajlmm84+ptMVjONiTOYRjONiTL5/XV8j0AfYgKQo/b9LB2rOPzfpsQ82avyABKXFhge6LAAAfKKyslIul0tJSUmtliclJWnTpk1tbrNt2zZ98sknuuGGG7RkyRIVFxfrzjvvlNPp1OzZs9vcZs6cOXr00UePWf7RRx8pIiLizAciadmyZT75Pf4yKU5K6W/R4hKr9lU16Fd/X6+5/1ynH/V2a3Bs22cBdvYxna5gHBdjMo9gHBdjOnP19fV++b2dItDPmzdPTz/9tEpLS5WZmannnntOY8eObXPdhQsXavr06a2WORwONTSY9xr0W87vq482lmn1zoO6763v9Jebx8piOfbTdAAAugK3263ExES9+OKLstlsGj16tPbs2aOnn376uIF+5syZys3N9T6vrq5Wenq6LrvsMkVHR59RPU6nU8uWLdOll14qu91+Rr/L3/5N0v1Ol17NL9H8z7drT32zXii06fz+3fWrywZqSEqUJHONqT2CcVyMyTyCcVyMyXeOnDnmawEP9Ecmypk/f76ysrI0d+5cTZw4UUVFRUpMTGxzm+joaBUVFXmfmz382qwWPfPvmbriD5/ri+JK/d/XJZp6bu9AlwUAwBlLSEiQzWZTWVlZq+VlZWVKTk5uc5uUlBTZ7fZWp9cPGTJEpaWlampqUmho6DHbOBwOORyOY5bb7Xaf/cHmy9/lT3a7XXddMlDXn5uh5z8p1l++2qF/Fe/XF1vz9ZOzeur/XTZQPSLt3nXNMKb2CsZxMSbzCMZxMSbfvJ4/BHxSvPZOlCN5AnxycrL38cPT+MyoT0Kk7rt8sCRpzpJClez3zykZAAB0pNDQUI0ePVp5eXneZW63W3l5ecrOzm5zm/Hjx6u4uFhut9u7bPPmzUpJSWkzzKNt8ZGhmjVpqD7OvVD/NjJFhiH9fc1uTXhmuZ75aIsONwe6QgDAmQpooD8yUU5OTo532ckmypGk2tpa9e7dW+np6br66qu1YcOGjijX76ZlZ+jcvvGqb3Lp7kVrVddIpwUAmF9ubq5eeuklvfrqqyosLNQdd9yhuro67yV0N954Y6tJ8+644w4dOHBA99xzjzZv3qzFixfriSee0IwZMwI1BFPr3T1Sz19/tt6dMV5j+8Srsdmt//nXdj26xqZHPyjUxr3+OQ0UAOB/AT3l/nQmyhk0aJAWLFigkSNHqqqqSs8884zGjRunDRs2qGfPnsesb7ZZb5+YPFQ/fuErfbvrkG7680q9PPUsRYR27NvEbJbmEYzjYkzmEYzjCraZbzuLKVOmqKKiQrNmzVJpaalGjRqlpUuXevt/SUmJrNajxxjS09P14Ycf6t5779XIkSOVlpame+65R/fdd1+ghhAURqXH6o3bzlVeYbnm/LNQWyvq9H9f79L/fb1LI3vGaMo56fpRZqqiwoLrtFoACGYBv4a+vbKzs1udojdu3DgNGTJE//M//6PHH3/8mPXNOOvtzf2lP2206ZsdB3XNHz7WbYPdCm37Lj1+xWyW5hGM42JM5hGM4wqWmW87k7vuukt33XVXmz9bvnz5Mcuys7P11Vdf+bmqrsdisShnaJLO7xenuYuWaoc1VR9vKtd3u6v03e4q/eaDQl01MkXXnpOu0b3jTD9PEQAEu4AG+tOZKOeH7Ha7zjrrLBUXF7f5c7POenvurkOavnC1tlRL7x3oofnXj5LD3jGpntkszSMYx8WYzCMYxxVsM98Cx2O1WjQ41lDulZmqbnTrnbV79PrKEm2tqNNbq3frrdW71T+xm649J10/PitN3bsdO+EgACDwAhrovz9RzuTJkyUdnSjneJ/i/5DL5dK6det05ZVXtvlzs856O7ZvDy38+VhNW7BSXxTv191vfKf5U0fLEdJxh+qZzdI8gnFcjMk8gnFcwTLzLXAqundz6Jbz++rm8/po9c6DWvTNLi3+bp+Ky2v1m8WF+u+lm5QzJEk/ObunLhzYQ6EhAZ9TGQDQIuD/Ird3opzHHntMH330kbZt26Y1a9boP/7jP7Rz507dcsstgRqC35yTEa8FN52jMLtVnxZVaMZra9XU7D75hgAAAO1ksVg0JiNez/x7pr5+8BL99sfDNbJnjJwuQ/9cX6pb/3eVsp74WLPfW69vdx2SYRiBLhkAuryAX0Pf3olyDh48qFtvvVWlpaWKi4vT6NGjtWLFCg0dOjRQQ/Crc/t21yvTztHPF36jjwvL9F+vr9Vz158luy3gn8UAAIAgFR1m1w1ZvXVDVm9t3Futt9fs1nvf7lVFTaNezd+pV/N3ql+PSP3k7J6afFaa0mLDA10yAHRJAQ/0Uvsmyvn973+v3//+9x1QVecxvn+CXrxxjG59dZWWbijVvW8UaO6UUQoh1AMAAD8bmhqtoalDdf8Vg/VFcaXeXrNHH24o1daKOj39YZGe/rBI2X2768dnp+mK4cnMkg8AHahTBHqc3IUDe2j+1LP1n39ZrQ++26cQq0W/+9ko2azMPgsAAPwvxGbVRYMSddGgRNU0OPXP9aV6e81ufbXtgPK37Vf+tv2a9d56XTCghy4dmqRLhiQpPjI00GUDQFAj0JvIxYOTNO/6s3Xna2v0bsFehdiseuqnI2Ul1AMAgA4UFWbXz8ak62dj0rX7YL3eXbtHb6/Zo22VdfpoY5k+2lgmq0Ua0ztelw1L0qVDk9S7e2SgywaAoEOgN5nLhiXrj9edpbtfX6u3Vu9WiNWiJ348glAPAAAComdchO66eIBmTOivDXurtawl0Bfuq9bKHQe0cscB/WZxoQYmddOlQ5N06dBkjUyL4W8XAPABAr0JXTkiRc1uQ79YtFaLvtmlorIa/b9LB2l8/+6yWGiOAACg41ksFg1Pi9HwtBjde+lA7TpQr48Ly7RsY5m+3n5Am8tqtbmsVvM+3aqkaIfG9I6XxSIZhuQ2DLkNo+V7yWh57m75Wb8eng8DxvaJZ2JgAPgeAr1J/SgzVW63ofv+/p3WlhzSf7zytcb2iVfupQN1bt/ugS4PAAB0cenxEZo+vo+mj++jqnqnPi0q10cbS/VZUYXKqhu1eN2+U/5d/9pSqYUrdig6LEQTBifq0qFJunBgDybgA9DlEehNbPJZaRrXr7v+tHyr/vp1iVZuP6BrX/xK4/t3V+6lgzS6d1ygSwQAAFBMhF2Tz0rT5LPS1Njs0oqt+7Wtok5Wi2S1WGS1eI7wWy0WWSxq9dztNrRq5wHlFZZrf12T3ivYq/cK9irUZtW5/bp7TuMfkqTkmLA2X7u6wak9Bw9r98HD2n2w3vtVki4enKhLhyYzeR8A0yLQm1xidJge+dEw/eeFfTXv02K98c0ufVm8X18Wr9CFA3so99KBykyPDXSZAAAAkiRHiE0TBiVqwqBT3+Zn56TL5Ta0tuSglm30nMa/rbJOn2+u0OebK/Twu+s1smeMxveL18YdVn3w1wLtOdSg3QfrVd3QfNzf++GGMs18e52y+nTXFSOSNXFYspKi2/5gAAA6IwJ9kEiJCddvJo/Qf17QT89/Uqy31uzWZ5sr9NnmCuUMSdS9lw7UsNSYQJcJAABwWmxWi8ZkxGtMRrxmXjlExeW1LeG+VGt3HdJ3u6v03e4qSVZpX3mrbeMjQ5UWG66ecUceEao67NSHG0q1YW/19267t0Fn94rVFcNTdPnwZKXHRwRmsABwigj0QSY9PkL/fc1I3Tmhn/6Qt0Xvrt2jjwvL9XFhuXKGJOnfRqbookE9FBvBqWUAAMC8+id2U//Ebrrjon4qr2nQJ4Xl+mbHfh0s3a3zzx6q3gnd1DMuQmmx4Yp0tP0n739dMkAl++u1dMM+LV1fqjUlh7yP3y4p1LDUaF0xPFnZ/RI0IKmborlmH0AnQ6APUr27R+rZn43SjAn99YePt+gf3+3Vx4Vl+riwTDarRaN7xylnSKIuGZKkfj26BbpcAACA05YYFaZrx/bST89K0ZIlJboyq5fs9lML3726R+i2C/rptgv6qbSqQR9uKNU/1+/Tyu0HtGFvtTbsrZa0WZKUHB2mAUmeDxIGJkVpQGI3DUiMUkyEf4J+s8ut0uoGNbv98usBBAECfZDr16Ob5771F/fXO2v3KK+wXEVlNVq5/YBWbj+gJ5ZsUp+ESF0y2BPuz8lgIj0AANA1JceEadq4DE0bl6H9tY1atrFMH20s04a9VSqrblRpdYNKqxv0ry2VrbbrEeXQgMRu6tejm+Ii7Ip0hCjCEaJuDpsiQkPUzRGiSEeIIkNtnq+OEMmQymoaVFrl+Z1lVQ0tzxtV1vI6lbWNMgwpIsSm7eHF+vn5/ZjAD0ArBPouYkBSlH59+WD9+vLB2nWgXnmFZcrbVK6vtu3X9so6vfzFdr38xXZFh4XoggEJiqq3qH9ZjQanxslm5d72AACga+nezaFrx/bStWN7SZKqDjtVXF6jLWW12lLueRSX1WhvVYMqahpVUdOoFVv3+6WW+maLnl++TS9/uUPXntNLt5zfRz3juL4fAIG+S0qPj9BN4/vopvF9VNPg1L+2VOrjwjItL6rQgbomfbCuVJJNrz+fr4hQm4anxigzPUYje8Yqs2es0uPDZbEQ8gEAQNcRE27X6N7xGt07vtXymgantlbUaUtZjbZX1qmmoVl1Tc2qa2xWXaOrze+dLsP7O5Ojw5QY7VBydJiSY8KUFB3W6vtIu/TUXz/Uqto4rd9brYUrdugvX+3UpJEp+s8L+2lISnQgdgeAToJA38VFhdl15YgUXTkixXs7mI83lurjgq3a12BXXZNLK3cc0ModB7zbxEXYW8J9jDLTYzUiLUaJ3OIFAAB0QVFhdo1Kj9WodtwmuKnZLbdhKMxuO+m6TqdTZ3U39MB/ZOmbkmq9sHyrviiu1LsFe/VuwV5dNKiH7riwn8b2ie+UB1wMw1BlbZP2HDqsPQcPa8+heu3aX6d1xVa9d2CtnG5DDU6XGpvdanS61djsUkPL18Zmtxqb3eqTEKn/yOqln47uqSgmJmxTg9OlzWU16tujm7odZxJIBCfebXgduR1MZlqUhji3aOLlF2vXoUYVtNwK5tvdh1S4r1oH653eW+Id0SPKoeGp0RqeFqNhqTEanhattFiO5AMAAPxQaIi13dtYLBaN75+g8f0TtG53leZ/vlX/XLdPy4sqtLyoQmf1itVt5/dVenyEXG5DzW5DzS730e/dbjW7DLnchpxuQ+F2m0alx6pHlOOMx2MYhkoO1Ktg1yHtOlCvPYcOa/fBw94Q39jmrH5WaX9FG8uPVVxeq0f+sVFPf1ikn5zdUzdm99aApKgzrtvsahub9emmci3dUKrlm8pV1+RS98hQ3XvpQF17TrpCbO3/7wzmQ6DHcdmsFg1IitKApCj9+5h0SVJjs0ub9tXo292H9O2uKn23+5C2VtSqoqZRnxZV6NOio/8wx0bYNTw1RsPSojU8NUZDU6PVKz5Cdv5xAQAAOG0jesZo3vVna0dlnV761za9uXq31pYc0h2vrWn378roHqHRveM1JiNO52TEqV+Pbic9ION0ubVxb7VW7TyoVTsO6JsdB1VZ23jc9S0WKSkqTGlx4UqLDVdKtEOVu4o1etQIRThC5QixymG3KizEJofdKkeIzbMsxCabzaJPCsv0av5OFZfX6i9f7dRfvtqpcf26a9q4DF0yOLFLBdf9tY36uLBMS9eX6svi/WpyHf2wxBFi1f66Jj307nr9b/4OPXTVUF0wsEcAq0VHINCjXRwhNmWmxyozPVbK9iyrb2pW4b4abdhbpfV7qrR+T7U2l9XoUL1TXxRX6oviozPBhlgt6t09Qn17eGaC7dsjUv16dFO/HpGKjWDWVgAAgFOVkRCp3/54hH6RM1ALV2zXewV75XS5FWK1KsRmkc1qUYjV0ubzA3VN2lxeox3767Vjf73+vma3JM8BmTG947whf0RajJwut9aWHNKqHQe0audBrS05pMNOV6ta7DaLhqfFqF+PbkqLDVdaXLh6tnxNiQlvdVaC0+nUkiVbdOXonqd0e8Gp2Rn6j3N7a8XW/Xp1xQ59XFimFVv3a8XW/UqLDdcN5/bStef0MsUdAAzD0Pvf7tWnm8oV4QhRbLhdcRGhio2wKzYiVHEtX2Mj7IoNtyvEZtWeQ4f14fpSLd1QqlU7DshtHP19fRMiNXF4siYOS9bQlGi9vrJEv/94szaX1erGBSs1YVAPPXjVUPVP5DbVwYpAjzMWERqi0b3jNLr30VveNTa7tLm0VuuPhPy91dpcWqPDTpe2VtRpa0Wdlqms1e/pHhnqDfgx4XYZ8vyjZxhq+V4y5Hmulp9ZLBb1jAtv+VCgm9LiwpmVHwAAdCk9ohz61cTB+tXEwe3aruqwU2tKDmr1joNatfOACnYd0qF6pz4uLNfHheWSpFCbVc1ud6sQKR2ZJDBOYzLiNKZ3vEb2jDmlOQFO1/cvOdh9sF6vfV2iRStLtOfQYT21tEhzP96iSSNTdeGgHhqSHKU+CZGd7sh94b5qzX5vQ6u5qU6mmyNEtY3NrZYNT4vWxKHJunx4svontj6jYtq4DE0elaY/frJFr67YoU+LKvT5ls819dzeuueSAYozwYceaB8CPfzCEWLTiJ4xGtEzxrvM7Ta0r7pB2ypqta2iTlu/93VfVYP21zVpf12Tvtlx8LRfNzTEqr4Jkd878u95pMd6/vEyDEM1DU7VNDS3PJyqbnle3fK8pqFZ3Rwh6p/YTf0Tu6l3fESnawgAAABnKibcrgmDEjVhUKIkz6n0G/ZWe47E7zioVTuPnkrfKz5CY3rHaUyG58h9/x7dZA3QQZSecRG67/LBuueSAfrHt3v1av4Ord9Trb+v2e090yA0xKoBid00ODlaQ1KiNDg5WoNTopTQ7cznDGiv6ganfr9ss/43f6dcLfMX3Diut8LtNh2qd+pQfZMO1jt16LDn+0P1TlUddkryXCdvsUjnZMRr4rBkXTY0SenxJ75lYUyEXQ//21DdkNVLTyzZpI8Ly7RwxQ69s3aP/uuSAZp6bu8253EwDEOH6p3aeaBeJQfqtetAvXZU1mrPLqsOrtylET3jNDg5SpFMutep8G6gw1itFs8pWLHhOn9A6+t56hqbtb3yaMivb2r2ftpoafkfiyyyWDzPLS3PnW63dh2o19byOm3fX6emZrc2ldZoU2nNMa8fZrPp3q+WHfMJ88nYbRb17h6p/j26eUN+/0TP5QIRofxfCAAABAe7zeqdsf+W8z0Bb/fBw3KEWDvlHY3C7Db9+5h0XTO6p9buOqS31+zWhr3VKiqtUX2TSxv2VmvD3upW2yR0c2hISpRGpMXo2nN6qVf3E4fjM2EYht5Zu0dPLNnk/WDkyhHJeuiqoUqNDT/hti63oarDTh2sb1JcROhpXU7Qt0c3vTxtjFYUV+qxDzZqU2mNHv9go/7vq52686J+anK5VbLfE9537vcE+JofnA3gYdWKfxRK8vwN3qd7pIakRmtoSrSGpkZrWEq0ekQ5mAw7QEgj6BQiHSEanhaj4WkxJ1/5OFxuQ3sOHtbWitqWx5EPCGpVWdukBtfRf2TsNouiwuyKCgtRdMtXz8Pz/cG6JhVX1GpreZ0OO10qLq9VcXmttKH1a6bGhCkxOkzxkaGKa7nuKS4y1Pvc89WzLDrMLrvNclr/2DldbtU1Nqu25T62tY1O1Ta6VHe4SfvqPWc/AAAA+JLFYjnp0eDOwGKx6OxecTq7l+fyT7fb0K6D9Z6DPPtqtKm0WptKa7Rjf50qaxv1ry2N+teWSs3/bKsuH56sW87v693WVwr3VWvWe+u9Z572TYjUo1cPO+ag1vHYrBbFR55ekP+hcf0TtPi/ztebq3bpmY+KtL2yTr9667vjrp8cHaZe8RFKj49QWqxDhUVb1BSZqMJ9NSqvadS2yjptq6zT4u/2ebdJ6BaqISnRun5sL10xIuWMa8apI9AjaNisFvXqHqFe3SM0YXBiq59VVtfr7SXLdNVll6h7VLgcIdZTCtZHLhM4EuiLy2u1tbxWxRW1OlDXpL1VDdpb1dCuOkNtVtltFtlDrLLbrAq1eSaqsduOPLfI6TJawrsnxLd9u5cjQvTcpk+8n6iPSo/z2W1oAAAAzMZq9Zxd2bt7pCYOS/Yur29q1uayWhXuq9aHG0q1vKhCS9aVasm6Uo3pHadbL+irnCFJZzQf0+Fm6fHFm/Tayl3e0+vvvqS/bj6vjxwh/ptj4GRsVouuHdtLV41M0QvLt+pfWyqVGOXw/O0cH6HeLV97xkW0mgvB6XRqyeEiXXnl2bLb7aqoaVThvmpt3FetjXs9X48cPPvXlkr9a0ulJmWm6vGrhzHhdQch0KNLiAm3KylcSoxyyN6OCVu+f5nAhT+47ceBuiZtr6zT/tpGHaxv0oE6z2lRB+uaWp57roc6UNfkvQ5KkppcbjW5JM//tE9oiFVRjhBFtjzsVmnTvirVNbr0ZfF+fVm837tuz7hwb8g/q1esBidHexvUMRMMqmUCwpZtw+02n95e0DAMVR9uVk2jU7WNnvkLahuaVdPo+Vrb6PQ+rz7cpG07rVpcVSCX27O/nC7PvXOdLreaXJ776jpdbjW7DfXuHqERabEa2TNGI9Ji1DMunFO+WlQ3OLW9ok7bK+t0oK7Jc4eKnjHMCQEA6JIiQkO8fxtdN7aXikpr9PK/tundgj2eW/D9ZbUyukfo5vP76pqzeyo89NT+ZnS5De2rOqzPi8o1p8CmGmeJpFM/vb4jRYXZ9evLB+vXl5/e9j2iHOoR1aPV7fAON7lUVFajDzeU6sXPt+kf3+7V19v2679/OvKYg2zwPQI9cJracxpUs8ut2sZmOVtC6ZFHU7OhZvfR7z1f3QqxWRQV1hLcQ0O83/8wZDudTv1j8RL1P/t8rd9Xq4JdB1Ww65C2lNdq98HD2n3wsD743ulQp8pikbpHOpQc41BydJiSosM8X2M8X5NjwpQUFabo8BA1NrtVWtWg0uoGlbU8SqsaPV9bnpdXN7a6T+rJWaX95ae05u6Dh1t9kBEfGaoRaTHegD+yZ6ySos1xXZdhGCo5UK+KmkY5QmwKs1sVZm+5F++Rrz84u6Sx2aWS/fXaVukJ7tsqarW95fvK2qZjXqObI0RZfeI1rn+CxvfvrkFJUabYNwAA+Nqg5Cg9/e+Z+tXEQXo1f4f+76sS7dhfr4ffXa9nPyrS1HN7a2p2hnpEObxnbe5o6bE799dpe2W9duyvU8mBejV5z6a0qE/3CD02efgpn15vduGhNu8HJROHJev//a1AWyvqNH3hN7pubLoevGqoujGRnt+wZ4EOEGKz+u20I5tFGpISpZG94nV9Vi9JUk2DU9/trlLBrkNaW3JIBbsOthnujscwpMraRlXWNmr9nurjrhdqs7YrqDtCrIoKC1E3R4i6tXyNCrMr6nvPI+xWbd2ySaNGDFdYaIj3UgT79y5LOPJckraU1+q73VVat+eQNu2r0YG6Jn22uUKfba7wvm6PKIeGtkzY0v3IHAeRod7vu0c6FBdpVzdHSIeH24qaRq3YWqkviyv1ZfF+7Tl0+ITrWyxqCfaesygO1DWecKLHxCiH+iREKiosRKt2HtSheqfyNpUrb5PnA5OEbqHK7peg8f26a3z/hONeK2kYhuqbXJ6zKxqdqm5oVlVdo7ZWS8XlteoRE+G9Xy4AAGaSGB2mX00crDsv6q83V+3SK19u164Dh/XHT4o1//Nt6hUf8YPQfiy7zaJe8REaGl6tOdPHqVt417z0cVR6rBb/1/l6ammRFny5Xa+v3KV/banUM/+eqXP7dg90eUGJQA8Eoagwu/derZInjNW1nOJ/JK4euVPAke+/r6ah+ejR9uoGlbUcgS+tblRZVYPKahp0qN7pDfPhdpvnqH204+jR/CNH8luW9YhynNK1Y06nU0tqC3Xl2HTZ7faTrj8mI17XjfV83+B0qai0Rt/tqdK63Yf03e4qbS6rUUVNoz6rqTjxL5LnA4r4yFClxYVraEq0hqV6Zm8dmBTls3vr1jY26+tt+1sukahUUVnrOzLYbRalxoarqdmtBqdLDU63GppdRy+PMORZ5jz6R0U3R4j69ohUn4Sjj74J3ZSREKGosKP70O02tHFftefDg6379c32A6qsbdI/vt2rf3y7V5KUHh+ufj26tVwKcfT2jrWNzcf54CBEf9ywwvssOixEcZGhij0ySWREqGIj7OoR5VB23+7K7BkbsNscAQBwIpGOEN00vo+mZmfoww2l+p/Pt+nbXYc8EyPL06PT4yOU0T1SGd0j1SchQhkJnu9TY8PldjVryZIlcrRxS7iuJMxu06xJQ3Xp0CT96q1vtfvgYV330lf6+fg++tXEQT77mwoeBHqgC7BYLO061SnMblOPKMcJ7zrQ4HSpoqZR0eF2RYd1/JHttoTZbZ7rxNNjJfWW5Lmua+O+Km0pq9X+upa5DeqavN8fqGvS/rpGNTjdanK5Wz64aNDqnQe9vzfEalH/xG4a2nKLlmGpMRqaGq2Y8KNh2TAMNTa7Vd3gbAnBniBc09CsQ3WN+qzEqv99aaW+3V2l5h8k46Ep0TpvQILG9euusX3ij7kdomEYcroMNTS71Oj0BP3GZpcam92ea9m6ndolBVarxXs3if+8sJ8am10qKDmkL7fu14riShXsOqRdBw5r14HjnyVgs1q8Z1lEhtq0/1CNmix2VTd4bnNT3dCs6oZm7dxf3+b2Cd1CdeHARF0yJFHnD0ho9YEDAACdgc1q0ZUjUnTF8GSt21Olg/VO9ekeqdTYsBOeieZu//RIQS27X3ct/cUF+s0HG7Xom1165YvtWl5Urmd/NqrlbzX4AoEewGkJs9tMcSub8FCbRveO1+je8Sdc73CTS/vrGrW/tkk79te13Lu2Shv2VutQvdNz65vSGr2tPd5t0mLDZbNavMH9h0G9NaukQ5KkXvERLWdQdFd23+7q3u3Ep+VZLBaFhlgUGmKVfHgbYEeITVl9uyurb3flXjpQtY3N+mbHAVVUN3pv49jtyC0dWy6PCLMfvYbf6XRqyZIluvLKibJYbS33y3XqUMukkIfqWyaKrHdq5/46fbGlUpW1Tfr7mt36+5rdCrFaNLZPvC4enKiLByeqb49uvhscAABnyGKxaGTP2ECXYWrdHCF68qcjddmwJN3393XaWlGnn7ywQrec30eZPWM9BwgcLQcKHDbvc19OzhzsCPQAIE/w7xnquV1LZnqsrh6VJslzdHxfVYM27PXcnuVIyN9z6HCb17tbLJ7mFR1mb5kjwNOgDh8s19XjhuuCQUmd9oOQbo4QTRh0erPRhtis6t7NccIPJ5qa3Vq144A+2VSuTzaVa1tlnVZs3a8VW/frN4sLldE9QhcPTtKoXrEKtVlktVgUYrPIZrXKZrHIZj36CGnj+xCrVTabxbtuiNUim+3oz0Ntp3a7SgAA4FsXD07SR7+I06z3N+gf3+7V/3y27YTrh4ZYPXMuOUIUbrd97xbPnq8hLbd6DrFaPbeCtnqWRzpCFBNuV3R4y9cwu2IiWr62LA/RiQ7AmA+BHgBOwGLxXNOeGhuuS4cmeZdX1TtVVFYjq8UzZ0FUy5HsyNCQY64R9x7JHtPzlOYFCFahIVaN65+gcf0T9NC/DdX2yjp9sqlcn24q19fb92vH/not+HK79KX/arDbPME+NOR7D5tnksHQEM8fClUHrXqjfJVsVs8HAFaLZ+6JI99LLcssUmpsuGZPGua/ggEACBJxkaF67rqzdMXwZP199W5VHfbM0VPX1Ky6RpdqG5u9Ew82Nbt1oNlzxp+v2W0WhVpsemLDZ7LbPH8H2G1W2UNaPjSwfu/7lp8/dvWwk55RGSgEegA4DTERdo3tc+LT+HFifRIidfN5fXTzeX1U29isL7ZU6JNN5dqxv15utyGXYcjlPvpodhtyt3z1Lm9Zp9nl9q5z5GtbPLeOdHkniWybVZurDpzSGAYmcZkAAADtceWIFF05IqXNnzldbtU1eibmPRLyG5yulls+e/p9k8ut5iO3gnYbcja71ez23Pq5ttGlqsNOVTc4VX3Y8/A8b1bVYadcbs+8RE5ZVFfdeMo1z5o01FfD9zkCPQAg4Lo5QnT58BRdPrztBn86vh/+nS2N3vtweb42fu95o9Olw41OrVqzVpmZmbLabHIbnssuDEMyZLQ8P/r99ydGBAAAZ8becqtnf9zu+cgtePfXHNaSZZ/o3HHnybBYWz7sd3s/NDjyfVOzW81uz/PoTjyJL4EeABCUrFaLQlsufwjXqd0ix+l0SrsMXTkqtUtfHgEAQLCxWCyKdIQo1BqmlAhpWGp0UPR6pg8EAAAAAMCECPQAAMDv5s2bp4yMDIWFhSkrK0srV648pe0WLVoki8WiyZMn+7dAAABMiEAPAAD86o033lBubq5mz56tNWvWKDMzUxMnTlR5efkJt9uxY4d++ctf6vzzz++gSgEAMBcCPQAA8Ktnn31Wt956q6ZPn66hQ4dq/vz5ioiI0IIFC467jcvl0g033KBHH31Uffv27cBqAQAwDybFAwAAftPU1KTVq1dr5syZ3mVWq1U5OTnKz88/7naPPfaYEhMTdfPNN+tf//rXSV+nsbFRjY1Hb0FUXV0tyTPRodPpPIMRyLv9mf6eziQYxyQF57gYk3kE47gYk+9f19cI9AAAwG8qKyvlcrmUlJTUanlSUpI2bdrU5jZffPGFXnnlFRUUFJzy68yZM0ePPvroMcs/+ugjRUREtKvm41m2bJlPfk9nEoxjkoJzXIzJPIJxXIzpzNXX1/vl9xLoAQBAp1FTU6OpU6fqpZdeUkJCwilvN3PmTOXm5nqfV1dXKz09XZdddpmio6PPqCan06lly5bp0ksvDYpbHEnBOSYpOMfFmMwjGMfFmHznyJljvkagBwAAfpOQkCCbzaaysrJWy8vKypScnHzM+lu3btWOHTs0adIk7zK32y1JCgkJUVFRkfr163fMdg6HQw6H45jldrvdZ3+w+fJ3dRbBOCYpOMfFmMwjGMfFmHzzev7ApHgAAMBvQkNDNXr0aOXl5XmXud1u5eXlKTs7+5j1Bw8erHXr1qmgoMD7+NGPfqQJEyaooKBA6enpHVk+AACdGkfoAQCAX+Xm5mratGkaM2aMxo4dq7lz56qurk7Tp0+XJN14441KS0vTnDlzFBYWpuHDh7faPjY2VpKOWQ4AQFdHoAcAAH41ZcoUVVRUaNasWSotLdWoUaO0dOlS70R5JSUlslo5aRAAgPYi0AMAAL+76667dNddd7X5s+XLl59w24ULF/q+IAAAgkCXC/SGYUjyzSyDTqdT9fX1qq6uDqpJIoJxXME4Jik4x8WYzCMYxxWoMR3pSUd6FM4c/f7EgnFMUnCOizGZRzCOizH5jr96fZcL9DU1NZLEpDoAgE6npqZGMTExgS4jKNDvAQCdka97vcXoYocD3G639u7dq6ioKFksljP6XUfucbtr164zvsdtZxKM4wrGMUnBOS7GZB7BOK5AjckwDNXU1Cg1NZVryX2Efn9iwTgmKTjHxZjMIxjHxZh8x1+9vssdobdarerZs6dPf2d0dHTQ/Af+fcE4rmAckxSc42JM5hGM4wrEmDgy71v0+1MTjGOSgnNcjMk8gnFcjMk3/NHrOQwAAAAAAIAJEegBAAAAADAhAv0ZcDgcmj17thwOR6BL8algHFcwjkkKznExJvMIxnEF45hw5oLxv4tgHJMUnONiTOYRjONiTJ1fl5sUDwAAAACAYMARegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKA/A/PmzVNGRobCwsKUlZWllStXBrqkM/LII4/IYrG0egwePDjQZbXL559/rkmTJik1NVUWi0Xvvvtuq58bhqFZs2YpJSVF4eHhysnJ0ZYtWwJT7Ck62ZhuuummY963yy+/PDDFnqI5c+bonHPOUVRUlBITEzV58mQVFRW1WqehoUEzZsxQ9+7d1a1bN/30pz9VWVlZgCo+NacyrosuuuiY9+v2228PUMUn98ILL2jkyJGKjo5WdHS0srOz9c9//tP7czO+Tycbk9neI/gXvb7zCcZeL9HvzdJH6PWd/z06oqv0ewL9aXrjjTeUm5ur2bNna82aNcrMzNTEiRNVXl4e6NLOyLBhw7Rv3z7v44svvgh0Se1SV1enzMxMzZs3r82fP/XUU/rjH/+o+fPn6+uvv1ZkZKQmTpyohoaGDq701J1sTJJ0+eWXt3rfXn/99Q6ssP0+++wzzZgxQ1999ZWWLVsmp9Opyy67THV1dd517r33Xv3jH//Qm2++qc8++0x79+7VT37ykwBWfXKnMi5JuvXWW1u9X0899VSAKj65nj176sknn9Tq1au1atUqXXzxxbr66qu1YcMGSeZ8n042Jslc7xH8h17fOQVjr5fo92bpI/T6zv8eHdFl+r2B0zJ27FhjxowZ3ucul8tITU015syZE8Cqzszs2bONzMzMQJfhM5KMd955x/vc7XYbycnJxtNPP+1ddujQIcPhcBivv/56ACpsvx+OyTAMY9q0acbVV18dkHp8pby83JBkfPbZZ4ZheN4Xu91uvPnmm951CgsLDUlGfn5+oMpstx+OyzAM48ILLzTuueeewBXlA3FxccbLL78cNO+TYRwdk2EEx3sE36DXd37B2OsNg35vpj5CrzeXYOz3HKE/DU1NTVq9erVycnK8y6xWq3JycpSfnx/Ays7cli1blJqaqr59++qGG25QSUlJoEvyme3bt6u0tLTV+xYTE6OsrCzTv2/Lly9XYmKiBg0apDvuuEP79+8PdEntUlVVJUmKj4+XJK1evVpOp7PVezV48GD16tXLVO/VD8d1xGuvvaaEhAQNHz5cM2fOVH19fSDKazeXy6VFixaprq5O2dnZQfE+/XBMR5j1PYLv0OvNKZh7vUS/74zo9eYQzP0+JNAFmFFlZaVcLpeSkpJaLU9KStKmTZsCVNWZy8rK0sKFCzVo0CDt27dPjz76qM4//3ytX79eUVFRgS7vjJWWlkpSm+/bkZ+Z0eWXX66f/OQn6tOnj7Zu3aoHHnhAV1xxhfLz82Wz2QJd3km53W794he/0Pjx4zV8+HBJnvcqNDRUsbGxrdY103vV1rgk6frrr1fv3r2Vmpqq7777Tvfdd5+Kior09ttvB7DaE1u3bp2ys7PV0NCgbt266Z133tHQoUNVUFBg2vfpeGOSzPkewffo9eYUrL1eot93RvT6zv8edYV+T6CH1xVXXOH9fuTIkcrKylLv3r31t7/9TTfffHMAK8OJXHvttd7vR4wYoZEjR6pfv35avny5LrnkkgBWdmpmzJih9evXm+4azpM53rhuu+027/cjRoxQSkqKLrnkEm3dulX9+vXr6DJPyaBBg1RQUKCqqiq99dZbmjZtmj777LNAl3VGjjemoUOHmvI9Ak4Vvd686PedD72+8+sK/Z5T7k9DQkKCbDbbMbM7lpWVKTk5OUBV+V5sbKwGDhyo4uLiQJfiE0fem2B/3/r27auEhARTvG933XWXPvjgA3366afq2bOnd3lycrKampp06NChVuub5b063rjakpWVJUmd+v0KDQ1V//79NXr0aM2ZM0eZmZn6wx/+YOr36XhjaosZ3iP4Hr3enLpKr5fo94FGr+/875HUNfo9gf40hIaGavTo0crLy/Muc7vdysvLa3VNhtnV1tZq69atSklJCXQpPtGnTx8lJye3et+qq6v19ddfB9X7tnv3bu3fv79Tv2+GYeiuu+7SO++8o08++UR9+vRp9fPRo0fLbre3eq+KiopUUlLSqd+rk42rLQUFBZLUqd+vH3K73WpsbDTt+9SWI2NqixnfI5w5er05dZVeL9HvA4Ve3/nfoxMJyn4f2Dn5zGvRokWGw+EwFi5caGzcuNG47bbbjNjYWKO0tDTQpZ22//f//p+xfPlyY/v27caXX35p5OTkGAkJCUZ5eXmgSztlNTU1xtq1a421a9cakoxnn33WWLt2rbFz507DMAzjySefNGJjY4333nvP+O6774yrr77a6NOnj3H48OEAV358JxpTTU2N8ctf/tLIz883tm/fbnz88cfG2WefbQwYMMBoaGgIdOnHdccddxgxMTHG8uXLjX379nkf9fX13nVuv/12o1evXsYnn3xirFq1ysjOzjays7MDWPXJnWxcxcXFxmOPPWasWrXK2L59u/Hee+8Zffv2NS644IIAV358999/v/HZZ58Z27dvN7777jvj/vvvNywWi/HRRx8ZhmHO9+lEYzLjewT/odd3TsHY6w2Dfm+WPkKv7/zv0RFdpd8T6M/Ac889Z/Tq1csIDQ01xo4da3z11VeBLumMTJkyxUhJSTFCQ0ONtLQ0Y8qUKUZxcXGgy2qXTz/91JB0zGPatGmGYXhuZ/Pwww8bSUlJhsPhMC655BKjqKgosEWfxInGVF9fb1x22WVGjx49DLvdbvTu3du49dZbO/0fm22NR5Lx5z//2bvO4cOHjTvvvNOIi4szIiIijB//+MfGvn37Alf0KTjZuEpKSowLLrjAiI+PNxwOh9G/f3/jV7/6lVFVVRXYwk/g5z//udG7d28jNDTU6NGjh3HJJZd4G7xhmPN9OtGYzPgewb/o9Z1PMPZ6w6Dfm6WP0Os7/3t0RFfp9xbDMAzfH/cHAAAAAAD+xDX0AAAAAACYEIEeAAAAAAATItADAAAAAGBCBHoAAAAAAEyIQA8AAAAAgAkR6AEAAAAAMCECPQAAAAAAJkSgBwAAAADAhAj0ADqcxWLRu+++G+gyAACAn9DrgY5BoAe6mJtuukkWi+WYx+WXXx7o0gAAgA/Q64GuIyTQBQDoeJdffrn+/Oc/t1rmcDgCVA0AAPA1ej3QNXCEHuiCHA6HkpOTWz3i4uIkeU6Re+GFF3TFFVcoPDxcffv21VtvvdVq+3Xr1uniiy9WeHi4unfvrttuu021tbWt1lmwYIGGDRsmh8OhlJQU3XXXXa1+XllZqR//+MeKiIjQgAED9P777/t30AAAdCH0eqBrINADOMbDDz+sn/70p/r22291ww036Nprr1VhYaEkqa6uThMnTlRcXJy++eYbvfnmm/r4449bNfEXXnhBM2bM0G233aZ169bp/fffV//+/Vu9xqOPPqqf/exn+u6773TllVfqhhtu0IEDBzp0nAAAdFX0eiBIGAC6lGnTphk2m82IjIxs9fjtb39rGIZhSDJuv/32VttkZWUZd9xxh2EYhvHiiy8acXFxRm1trffnixcvNqxWq1FaWmoYhmGkpqYaDz744HFrkGQ89NBD3ue1tbWGJOOf//ynz8YJAEBXRa8Hug6uoQe6oAkTJuiFF15otSw+Pt77fXZ2dqufZWdnq6CgQJJUWFiozMxMRUZGen8+fvx4ud1uFRUVyWKxaO/evbrkkktOWMPIkSO930dGRio6Olrl5eWnOyQAAPA99HqgayDQA11QZGTkMafF+Up4ePgprWe321s9t1gscrvd/igJAIAuh14PdA1cQw/gGF999dUxz4cMGSJJGjJkiL799lvV1dV5f/7ll1/KarVq0KBBioqKUkZGhvLy8jq0ZgAAcOro9UBw4Ag90AU1NjaqtLS01bKQkBAlJCRIkt58802NGTNG5513nl577TWtXLlSr7zyiiTphhtu0OzZszVt2jQ98sgjqqio0N13362pU6cqKSlJkvTII4/o9ttvV2Jioq644grV1NToyy+/1N13392xAwUAoIui1wNdA4Ee6IKWLl2qlJSUVssGDRqkTZs2SfLMSrto0SLdeeedSklJ0euvv66hQ4dKkiIiIvThhx/qnnvu0TnnnKOIiAj99Kc/1bPPPuv9XdOmTVNDQ4N+//vf65e//KUSEhJ0zTXXdNwAAQDo4uj1QNdgMQzDCHQRADoPi8Wid955R5MnTw50KQAAwA/o9UDw4Bp6AAAAAABMiEAPAAAAAIAJcco9AAAAAAAmxBF6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQCGug///xzTZo0SampqbJYLHr33XdPus3y5ct19tlny+FwqH///lq4cKHf6wQAAKeHXg8AgP8ENNDX1dUpMzNT8+bNO6X1t2/frquuukoTJkxQQUGBfvGLX+iWW27Rhx9+6OdKAQDA6aDXAwDgPxbDMIxAFyFJFotF77zzjiZPnnzcde677z4tXrxY69ev9y679tprdejQIS1durQDqgQAAKeLXg8AgG+FBLqA9sjPz1dOTk6rZRMnTtQvfvGL427T2NioxsZG73O3260DBw6oe/fuslgs/ioVAIBTZhiGampqlJqaKqu1a09vczq9XqLfAwA6N3/1elMF+tLSUiUlJbValpSUpOrqah0+fFjh4eHHbDNnzhw9+uijHVUiAACnbdeuXerZs2egywio0+n1Ev0eAGAOvu71pgr0p2PmzJnKzc31Pq+qqlKvXr20a9cuRUdHB7AyAAA8qqurlZ6erqioqECXYlr0ewBAZ+avXm+qQJ+cnKyysrJWy8rKyhQdHX3cT+wdDoccDscxy6Ojo2nwAIBOhVPDT6/XS/R7AIA5+LrXm+pCvezsbOXl5bVatmzZMmVnZweoIgAA4Ev0egAATl1AA31tba0KCgpUUFAgyXOrmoKCApWUlEjynD534403ete//fbbtW3bNv3617/Wpk2b9Kc//Ul/+9vfdO+99waifAAAcBL0egAA/CeggX7VqlU666yzdNZZZ0mScnNzddZZZ2nWrFmSpH379nkbviT16dNHixcv1rJly5SZmanf/e53evnllzVx4sSA1A8AAE6MXg8AgP90mvvQd5Tq6mrFxMSoqqqKa+oAAJ0Cvcn32KcAgM7EX33JVNfQAwAAAAAADwI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCEAh7o582bp4yMDIWFhSkrK0srV6484fpz587VoEGDFB4ervT0dN17771qaGjooGoBAMDpoN8DAOB7AQ30b7zxhnJzczV79mytWbNGmZmZmjhxosrLy9tc/69//avuv/9+zZ49W4WFhXrllVf0xhtv6IEHHujgygEAwKmi3wMA4B8BDfTPPvusbr31Vk2fPl1Dhw7V/PnzFRERoQULFrS5/ooVKzR+/Hhdf/31ysjI0GWXXabrrrvupJ/yAwCAwKHfAwDgHwEL9E1NTVq9erVycnKOFmO1KicnR/n5+W1uM27cOK1evdrb0Ldt26YlS5boyiuvPO7rNDY2qrq6utUDAAB0DPo9AAD+ExKoF66srJTL5VJSUlKr5UlJSdq0aVOb21x//fWqrKzUeeedJ8Mw1NzcrNtvv/2Ep+DNmTNHjz76qE9rBwAAp4Z+DwCA/wR8Urz2WL58uZ544gn96U9/0po1a/T2229r8eLFevzxx4+7zcyZM1VVVeV97Nq1qwMrBgAA7UW/BwDg1ATsCH1CQoJsNpvKyspaLS8rK1NycnKb2zz88MOaOnWqbrnlFknSiBEjVFdXp9tuu00PPvigrNZjP59wOBxyOBy+HwAAADgp+j0AAP4TsCP0oaGhGj16tPLy8rzL3G638vLylJ2d3eY29fX1xzRxm80mSTIMw3/FAgCA00K/BwDAfwJ2hF6ScnNzNW3aNI0ZM0Zjx47V3LlzVVdXp+nTp0uSbrzxRqWlpWnOnDmSpEmTJunZZ5/VWWedpaysLBUXF+vhhx/WpEmTvI0eAAB0LvR7AAD8I6CBfsqUKaqoqNCsWbNUWlqqUaNGaenSpd6Jc0pKSlp9Qv/QQw/JYrHooYce0p49e9SjRw9NmjRJv/3tbwM1BAAAcBL0ewAA/MNidLFz16qrqxUTE6OqqipFR0cHuhwAAOhNfsA+BQB0Jv7qS6aa5R4AAAAAAHgQ6AEAAAAAMCECPQAAAAAAJkSgBwAAAADAhAj0AAAAAACYEIEeAAAAAAATItADAAAAAGBCBHoAAAAAAEyIQA8AAAAAgAkR6AEAAAAAMCECPQAAAAAAJkSgBwAAAADAhAj0AAAAAACYEIEeAAAAAAATItADAAAAAGBCBHoAAAAAAEyIQA8AAAAAgAkR6AEAAAAAMCECPQAAAAAAJkSgBwAAAADAhAj0AAAAAACYEIEeAAAAAAATItADAAAAAGBCBHoAAAAAAEyIQA8AAAAAgAkR6AEAAAAAMCECPQAAAAAAJkSgBwAAAADAhAj0AAAAAACYEIEeAAAAAAATItADAAAAAGBCBHoAAAAAAEyIQA8AAAAAgAkR6AEAAAAAMCECPQAAAAAAJkSgBwAAAADAhAj0AAAAAACYEIEeAAAAAAATItADAAAAAGBCBHoAAAAAAEyIQA8AAAAAgAkR6AEAAAAAMCECPQAAAAAAJkSgBwAAAADAhAj0AAAAAACYEIEeAAAAAAATItADAAAAAGBCBHoAAAAAAEyIQA8AAAAAgAkR6AEAAAAAMCECPQAAAAAAJkSgBwAAAADAhAj0AAAAAACYEIEeAAAAAAATItADAAAAAGBCBHoAAAAAAEyIQA8AAAAAgAkR6AEAAAAAMCECPQAAAAAAJhTwQD9v3jxlZGQoLCxMWVlZWrly5QnXP3TokGbMmKGUlBQ5HA4NHDhQS5Ys6aBqAQDA6aDfAwDgeyGBfPE33nhDubm5mj9/vrKysjR37lxNnDhRRUVFSkxMPGb9pqYmXXrppUpMTNRbb72ltLQ07dy5U7GxsR1fPAAAOCX0ewAA/MNiGIYRqBfPysrSOeeco+eff16S5Ha7lZ6errvvvlv333//MevPnz9fTz/9tDZt2iS73X5ar1ldXa2YmBhVVVUpOjr6jOoHAMAXgr030e8BAF2dv/pSwE65b2pq0urVq5WTk3O0GKtVOTk5ys/Pb3Ob999/X9nZ2ZoxY4aSkpI0fPhwPfHEE3K5XMd9ncbGRlVXV7d6AACAjkG/BwDAfwIW6CsrK+VyuZSUlNRqeVJSkkpLS9vcZtu2bXrrrbfkcrm0ZMkSPfzww/rd736n3/zmN8d9nTlz5igmJsb7SE9P9+k4AADA8dHvAQDwn4BPitcebrdbiYmJevHFFzV69GhNmTJFDz74oObPn3/cbWbOnKmqqirvY9euXR1YMQAAaC/6PQAApyZgk+IlJCTIZrOprKys1fKysjIlJye3uU1KSorsdrtsNpt32ZAhQ1RaWqqmpiaFhoYes43D4ZDD4fBt8QAA4JTQ7wEA8J+AHaEPDQ3V6NGjlZeX513mdruVl5en7OzsNrcZP368iouL5Xa7vcs2b96slJSUNps7AAAILPo9AAD+E9BT7nNzc/XSSy/p1VdfVWFhoe644w7V1dVp+vTpkqQbb7xRM2fO9K5/xx136MCBA7rnnnu0efNmLV68WE888YRmzJgRqCEAAICToN8DAOAfAb0P/ZQpU1RRUaFZs2aptLRUo0aN0tKlS70T55SUlMhqPfqZQ3p6uj788EPde++9GjlypNLS0nTPPffovvvuC9QQAADASdDvAQDwj4Dehz4QuC8tAKCzoTf5HvsUANCZBN196AEAAAAAwOkj0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATKhTBPp58+YpIyNDYWFhysrK0sqVK09pu0WLFslisWjy5Mn+LRAAAJwRej0AAL4X8ED/xhtvKDc3V7Nnz9aaNWuUmZmpiRMnqry8/ITb7dixQ7/85S91/vnnd1ClAADgdNDrAQDwj4AH+meffVa33nqrpk+frqFDh2r+/PmKiIjQggULjruNy+XSDTfcoEcffVR9+/btwGoBAEB70esBAPCPgAb6pqYmrV69Wjk5Od5lVqtVOTk5ys/PP+52jz32mBITE3XzzTef9DUaGxtVXV3d6gEAADpGR/R6iX4PAOiaAhroKysr5XK5lJSU1Gp5UlKSSktL29zmiy++0CuvvKKXXnrplF5jzpw5iomJ8T7S09PPuG4AAHBqOqLXS/R7AEDXFPBT7tujpqZGU6dO1UsvvaSEhIRT2mbmzJmqqqryPnbt2uXnKgEAwOk6nV4v0e8BAF1TSCBfPCEhQTabTWVlZa2Wl5WVKTk5+Zj1t27dqh07dmjSpEneZW63W5IUEhKioqIi9evXr9U2DodDDofDD9UDAICT6YheL9HvAQBdU0CP0IeGhmr06NHKy8vzLnO73crLy1N2dvYx6w8ePFjr1q1TQUGB9/GjH/1IEyZMUEFBAafXAQDQydDrAQDwn4AeoZek3NxcTZs2TWPGjNHYsWM1d+5c1dXVafr06ZKkG2+8UWlpaZozZ47CwsI0fPjwVtvHxsZK0jHLAQBA50CvBwDAPwIe6KdMmaKKigrNmjVLpaWlGjVqlJYuXeqdPKekpERWq6ku9QcAAN9DrwcAwD8shmEYgS6iI1VXVysmJkZVVVWKjo4OdDkAANCb/IB9CgDoTPzVl/g4HAAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMqFME+nnz5ikjI0NhYWHKysrSypUrj7vuSy+9pPPPP19xcXGKi4tTTk7OCdcHAACBR68HAMD3Ah7o33jjDeXm5mr27Nlas2aNMjMzNXHiRJWXl7e5/vLly3Xdddfp008/VX5+vtLT03XZZZdpz549HVw5AAA4FfR6AAD8w2IYhhHIArKysnTOOefo+eeflyS53W6lp6fr7rvv1v3333/S7V0ul+Li4vT888/rxhtvPOn61dXViomJUVVVlaKjo8+4fgAAzlSw96aO7vVS8O9TAIC5+KsvBfQIfVNTk1avXq2cnBzvMqvVqpycHOXn55/S76ivr5fT6VR8fHybP29sbFR1dXWrBwAA6Bgd0esl+j0AoGsKaKCvrKyUy+VSUlJSq+VJSUkqLS09pd9x3333KTU1tdUfCt83Z84cxcTEeB/p6elnXDcAADg1HdHrJfo9AKBrCvg19GfiySef1KJFi/TOO+8oLCyszXVmzpypqqoq72PXrl0dXCUAADhdp9LrJfo9AKBrCgnkiyckJMhms6msrKzV8rKyMiUnJ59w22eeeUZPPvmkPv74Y40cOfK46zkcDjkcDp/UCwAA2qcjer1EvwcAdE0BPUIfGhqq0aNHKy8vz7vM7XYrLy9P2dnZx93uqaee0uOPP66lS5dqzJgxHVEqAAA4DfR6AAD8J6BH6CUpNzdX06ZN05gxYzR27FjNnTtXdXV1mj59uiTpxhtvVFpamubMmSNJ+u///m/NmjVLf/3rX5WRkeG9/q5bt27q1q1bwMYBAADaRq8HAMA/Ah7op0yZooqKCs2aNUulpaUaNWqUli5d6p08p6SkRFbr0RMJXnjhBTU1Nemaa65p9Xtmz56tRx55pCNLBwAAp4BeDwCAfwT8PvQdjfvSAgA6G3qT77FPAQCdSVDehx4AAAAAAJweAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQI9AAAAAAAmBCBHgAAAAAAEyLQAwAAAABgQgR6AAAAAABMiEAPAAAAAIAJEegBAAAAADAhAj0AAAAAACZEoAcAAAAAwIQ6RaCfN2+eMjIyFBYWpqysLK1cufKE67/55psaPHiwwsLCNGLECC1ZsqSDKgUAAKeDXg8AgO8FPNC/8cYbys3N1ezZs7VmzRplZmZq4sSJKi8vb3P9FStW6LrrrtPNN9+stWvXavLkyZo8ebLWr1/fwZUDAIBTQa8HAMA/LIZhGIEsICsrS+ecc46ef/55SZLb7VZ6erruvvtu3X///cesP2XKFNXV1emDDz7wLjv33HM1atQozZ8//6SvV11drZiYGFVVVSk6Otp3AwEA4DQFe2/q6F4vBf8+BQCYi7/6UojPftNpaGpq0urVqzVz5kzvMqvVqpycHOXn57e5TX5+vnJzc1stmzhxot599902129sbFRjY6P3eVVVlSTPDgUAoDM40pMC/Bm7X3REr5fo9wCAzs1fvT6ggb6yslIul0tJSUmtliclJWnTpk1tblNaWtrm+qWlpW2uP2fOHD366KPHLE9PTz/NqgEA8I/9+/crJiYm0GX4VEf0eol+DwAwB1/3+oAG+o4wc+bMVp/yHzp0SL1791ZJSUnQ/dEUKNXV1UpPT9euXbs4rdEH2J++xz71Lfan71VVValXr16Kj48PdCmmRb/3L/5/73vsU99if/oe+9S3/NXrAxroExISZLPZVFZW1mp5WVmZkpOT29wmOTm5Xes7HA45HI5jlsfExPAfpo9FR0ezT32I/el77FPfYn/6ntUa8Llqfa4jer1Ev+8o/P/e99invsX+9D32qW/5utcH9C+H0NBQjR49Wnl5ed5lbrdbeXl5ys7ObnOb7OzsVutL0rJly467PgAACBx6PQAA/hPwU+5zc3M1bdo0jRkzRmPHjtXcuXNVV1en6dOnS5JuvPFGpaWlac6cOZKke+65RxdeeKF+97vf6aqrrtKiRYu0atUqvfjii4EcBgAAOA56PQAA/hHwQD9lyhRVVFRo1qxZKi0t1ahRo7R06VLvZDglJSWtTksYN26c/vrXv+qhhx7SAw88oAEDBujdd9/V8OHDT+n1HA6HZs+e3eZpeTg97FPfYn/6HvvUt9ifvhfs+7Sje70U/Pu0o7E/fY996lvsT99jn/qWv/ZnwO9DDwAAAAAA2i/4Zt8BAAAAAKALINADAAAAAGBCBHoAAAAAAEyIQA8AAAAAgAkFZaCfN2+eMjIyFBYWpqysLK1cufKE67/55psaPHiwwsLCNGLECC1ZsqSDKjWP9uzTl156Seeff77i4uIUFxennJyck74HXU17/xs9YtGiRbJYLJo8ebJ/CzSh9u7TQ4cOacaMGUpJSZHD4dDAgQP5//73tHd/zp07V4MGDVJ4eLjS09N17733qqGhoYOq7fw+//xzTZo0SampqbJYLHr33XdPus3y5ct19tlny+FwqH///lq4cKHf6zQTer3v0et9j37vW/R636Pf+07Aer0RZBYtWmSEhoYaCxYsMDZs2GDceuutRmxsrFFWVtbm+l9++aVhs9mMp556yti4caPx0EMPGXa73Vi3bl0HV955tXefXn/99ca8efOMtWvXGoWFhcZNN91kxMTEGLt37+7gyjun9u7PI7Zv326kpaUZ559/vnH11Vd3TLEm0d592tjYaIwZM8a48sorjS+++MLYvn27sXz5cqOgoKCDK++c2rs/X3vtNcPhcBivvfaasX37duPDDz80UlJSjHvvvbeDK++8lixZYjz44IPG22+/bUgy3nnnnROuv23bNiMiIsLIzc01Nm7caDz33HOGzWYzli5d2jEFd3L0et+j1/se/d636PW+R7/3rUD1+qAL9GPHjjVmzJjhfe5yuYzU1FRjzpw5ba7/s5/9zLjqqqtaLcvKyjL+8z//0691mkl79+kPNTc3G1FRUcarr77qrxJN5XT2Z3NzszFu3Djj5ZdfNqZNm0aD/4H27tMXXnjB6Nu3r9HU1NRRJZpKe/fnjBkzjIsvvrjVstzcXGP8+PF+rdOsTqXJ//rXvzaGDRvWatmUKVOMiRMn+rEy86DX+x693vfo975Fr/c9+r3/dGSvD6pT7puamrR69Wrl5OR4l1mtVuXk5Cg/P7/NbfLz81utL0kTJ0487vpdzens0x+qr6+X0+lUfHy8v8o0jdPdn4899pgSExN18803d0SZpnI6+/T9999Xdna2ZsyYoaSkJA0fPlxPPPGEXC5XR5XdaZ3O/hw3bpxWr17tPU1v27ZtWrJkia688soOqTkY0ZuOj17ve/R636Pf+xa93vfo94Hnq94U4suiAq2yslIul0tJSUmtliclJWnTpk1tblNaWtrm+qWlpX6r00xOZ5/+0H333afU1NRj/oPtik5nf37xxRd65ZVXVFBQ0AEVms/p7NNt27bpk08+0Q033KAlS5aouLhYd955p5xOp2bPnt0RZXdap7M/r7/+elVWVuq8886TYRhqbm7W7bffrgceeKAjSg5Kx+tN1dXVOnz4sMLDwwNUWeDR632PXu979Hvfotf7Hv0+8HzV64PqCD06nyeffFKLFi3SO++8o7CwsECXYzo1NTWaOnWqXnrpJSUkJAS6nKDhdruVmJioF198UaNHj9aUKVP04IMPav78+YEuzZSWL1+uJ554Qn/605+0Zs0avf3221q8eLEef/zxQJcGoAPQ688c/d736PW+R7/vnILqCH1CQoJsNpvKyspaLS8rK1NycnKb2yQnJ7dr/a7mdPbpEc8884yefPJJffzxxxo5cqQ/yzSN9u7PrVu3aseOHZo0aZJ3mdvtliSFhISoqKhI/fr182/Rndzp/DeakpIiu90um83mXTZkyBCVlpaqqalJoaGhfq25Mzud/fnwww9r6tSpuuWWWyRJI0aMUF1dnW677TY9+OCDslr57Li9jteboqOju/TReYle7w/0et+j3/sWvd736PeB56teH1R7PTQ0VKNHj1ZeXp53mdvtVl5enrKzs9vcJjs7u9X6krRs2bLjrt/VnM4+laSnnnpKjz/+uJYuXaoxY8Z0RKmm0N79OXjwYK1bt04FBQXex49+9CNNmDBBBQUFSk9P78jyO6XT+W90/PjxKi4u9v6xJEmbN29WSkpKl2/wp7M/6+vrj2niR/6A8swLg/aiNx0fvd736PW+R7/3LXq979HvA89nvaldU+iZwKJFiwyHw2EsXLjQ2Lhxo3HbbbcZsbGxRmlpqWEYhjF16lTj/vvv967/5ZdfGiEhIcYzzzxjFBYWGrNnz+ZWNj/Q3n365JNPGqGhocZbb71l7Nu3z/uoqakJ1BA6lfbuzx9i1ttjtXeflpSUGFFRUcZdd91lFBUVGR988IGRmJho/OY3vwnUEDqV9u7P2bNnG1FRUcbrr79ubNu2zfjoo4+Mfv36GT/72c8CNYROp6amxli7dq2xdu1aQ5Lx7LPPGmvXrjV27txpGIZh3H///cbUqVO96x+5lc2vfvUro7Cw0Jg3bx63rfseer3v0et9j37vW/R636Pf+1agen3QBXrDMIznnnvO6NWrlxEaGmqMHTvW+Oqrr7w/u/DCC41p06a1Wv9vf/ubMXDgQCM0NNQYNmyYsXjx4g6uuPNrzz7t3bu3IemYx+zZszu+8E6qvf+Nfh8Nvm3t3acrVqwwsrKyDIfDYfTt29f47W9/azQ3N3dw1Z1Xe/an0+k0HnnkEaNfv35GWFiYkZ6ebtx5553GwYMHO77wTurTTz9t89/FI/tx2rRpxoUXXnjMNqNGjTJCQ0ONvn37Gn/+8587vO7OjF7ve/R636Pf+xa93vfo974TqF5vMQzOjwAAAAAAwGyC6hp6AAAAAAC6CgI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJgQgR4AAAAAABMi0AMAAAAAYEIEegAAAAAATIhADwAAAACACRHoAQAAAAAwIQI9AAAAAAAmRKAHAAAAAMCECPQAAAAAAJjQ/weCf7NZwgAyoQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import models, visualizations,utils\n",
    "reload(models)\n",
    "reload(visualizations)\n",
    "reload(utils)\n",
    "\n",
    "from models import LSTMModel, train_and_evaluate_model\n",
    "from visualizations import plot_prices_comparision, plot_metrics\n",
    "from utils import get_company_list_from_directory,map_company_names_to_predictions \n",
    "\n",
    "\n",
    "directory = \"data/probar\"  # Replace with your actual directory path\n",
    "companies = get_company_list_from_directory(directory)\n",
    "\n",
    "\n",
    "# Entrenar el modelo LSTM Configuración buena \n",
    "input_size      = X_train_tensor.shape[2]  # Número de características de entrada\n",
    "output_size     = y_train.shape[1]        # Número de empresas (salida)\n",
    "\n",
    "\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "dropout_prob = 0.65\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 1e-4\n",
    "optimizer_choice = 'adam'\n",
    "critierion='MSE'\n",
    "patience=10\n",
    "\n",
    "\n",
    "model = LSTMModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout_prob=dropout_prob,\n",
    "\n",
    ")\n",
    "file_name = f\"{ventana}_{horizonte}_{hidden_size}_{num_layers}_{dropout_prob}_{num_epochs}_{learning_rate}_{weight_decay}_{optimizer_choice}_{critierion}\"\n",
    "\n",
    "print(f\"Entrenando modelo LSTM con configuración {file_name}\")\n",
    "# train_model(model_lstm, train_loader, test_loader, num_epochs, learning_rate)\n",
    "# lstm_preds, lstm_actuals = evaluate_model(model_lstm, test_loader)\n",
    "\n",
    "trained_model, preds, actuals,metrics = train_and_evaluate_model(\n",
    "    model, train_loader, test_loader, num_epochs, learning_rate,weight_decay,optimizer_choice,critierion,patience\n",
    ")\n",
    "\n",
    "# # Guardar predicciones con Pickle\n",
    "# with open('lstm_preds.pkl', 'wb') as f:\n",
    "#     pickle.dump(lstm_preds, f)\n",
    "\n",
    "plot_metrics(metrics,'LSTM',str(len(companies)),guardar=True,file_name=file_name)\n",
    "plot_prices_comparision(actuals,preds,'LSTM',scaler_y,len(companies),companies,guardar=True,file_name=file_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo RNN...\n",
      "Epoch 1/15, Train Loss: 0.2305, Valid Loss: 0.1656, Train RMSE: 0.4804, Valid RMSE: 0.4070, Train R²: 0.7692, Valid R²: -110.6133\n",
      "Epoch 2/15, Train Loss: 0.3501, Valid Loss: 0.7020, Train RMSE: 0.5929, Valid RMSE: 0.8378, Train R²: 0.6485, Valid R²: -309.6143\n",
      "Epoch 3/15, Train Loss: 0.2242, Valid Loss: 0.9437, Train RMSE: 0.4745, Valid RMSE: 0.9714, Train R²: 0.7748, Valid R²: -385.8154\n",
      "Epoch 4/15, Train Loss: 0.3463, Valid Loss: 0.0441, Train RMSE: 0.5895, Valid RMSE: 0.2100, Train R²: 0.6525, Valid R²: -31.1422\n",
      "Epoch 5/15, Train Loss: 0.3688, Valid Loss: 0.1295, Train RMSE: 0.6085, Valid RMSE: 0.3599, Train R²: 0.6298, Valid R²: -66.9784\n",
      "Epoch 6/15, Train Loss: 0.2218, Valid Loss: 0.1001, Train RMSE: 0.4717, Valid RMSE: 0.3163, Train R²: 0.7775, Valid R²: -40.7170\n",
      "Epoch 7/15, Train Loss: 0.1615, Valid Loss: 0.0186, Train RMSE: 0.4020, Valid RMSE: 0.1363, Train R²: 0.8384, Valid R²: -8.8974\n",
      "Epoch 8/15, Train Loss: 0.1355, Valid Loss: 0.0662, Train RMSE: 0.3684, Valid RMSE: 0.2572, Train R²: 0.8643, Valid R²: -32.8328\n",
      "Epoch 9/15, Train Loss: 0.1341, Valid Loss: 0.3055, Train RMSE: 0.3665, Valid RMSE: 0.5527, Train R²: 0.8657, Valid R²: -132.4645\n",
      "Epoch 10/15, Train Loss: 0.1317, Valid Loss: 0.3619, Train RMSE: 0.3625, Valid RMSE: 0.6016, Train R²: 0.8686, Valid R²: -156.0121\n",
      "Epoch 11/15, Train Loss: 0.1331, Valid Loss: 0.0486, Train RMSE: 0.3651, Valid RMSE: 0.2204, Train R²: 0.8667, Valid R²: -22.8897\n",
      "Epoch 12/15, Train Loss: 0.3642, Valid Loss: 0.0402, Train RMSE: 0.6044, Valid RMSE: 0.2006, Train R²: 0.6347, Valid R²: -28.1850\n",
      "Epoch 13/15, Train Loss: 0.1102, Valid Loss: 0.1320, Train RMSE: 0.3317, Valid RMSE: 0.3634, Train R²: 0.8899, Valid R²: -72.8705\n",
      "Epoch 14/15, Train Loss: 0.1332, Valid Loss: 0.0690, Train RMSE: 0.3654, Valid RMSE: 0.2627, Train R²: 0.8665, Valid R²: -34.6605\n",
      "Epoch 15/15, Train Loss: 0.2640, Valid Loss: 0.8654, Train RMSE: 0.5145, Valid RMSE: 0.9303, Train R²: 0.7353, Valid R²: -555.2029\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA14AAAHWCAYAAABnpFhuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8qElEQVR4nO3dd3xUVfrH8c9MekICgYSEQOi9dwQFUYEgNiyoiEgUdXVBpYiCrgg2FgUrrq67P0FXXRELugpIBAsKCNJRem+hQ4BA2szvj8NMiAmQQCZ3yvf9eg1z586dO89wksw8c855js3pdDoRERERERERj7FbHYCIiIiIiIi/U+IlIiIiIiLiYUq8REREREREPEyJl4iIiIiIiIcp8RIREREREfEwJV4iIiIiIiIepsRLRERERETEw5R4iYiIiIiIeJgSLxEREREREQ9T4iUiIhIgatasybXXXmt1GCIiAUmJl4iIlIkpU6Zgs9ncl+DgYKpWrUpqaiq7du0qcOzHH39Mp06duPzyy2nSpAn//ve/LYq6ZGrWrFngNZ556dmzp9XhiYiIhYKtDkBERALLM888Q61atTh16hQLFy5kypQp/Pzzz6xevZrw8HAAOnTowI8//khISAjLly+ndevWdOvWjZo1a1obfDG0bNmS4cOHF9qflJRkQTQiIuItlHiJiEiZuvrqq2nbti0A9957L3FxcYwfP56vvvqKW2+9FYBatWq5j3c6ne5eI6vl5ubicDgIDQ096zFVq1blzjvvLMOoRETEF2iooYiIWKpz584AbNq0qdB9x44dY8CAATzyyCPUqFHjnOdxzV+aPXs2LVu2JDw8nMaNG/P5558XOvbIkSMMGTKE5ORkwsLCqFu3LuPHj8fhcLiP2bp1KzabjQkTJvDqq69Sp04dwsLC+OOPPy7yFUNqairlypVj8+bNpKSkEBUVRVJSEs888wxOp7PAsSdOnGD48OHuWBs0aMCECRMKHQfwwQcf0L59eyIjI4mNjaVLly7Mnj270HE///wz7du3Jzw8nNq1a/P+++9f9GsSEZFzU+IlIiKW2rp1KwCxsbEF9p88eZLevXtTt25dXnrppWKda8OGDdx2221cffXVjBs3juDgYPr06UNaWpr7mMzMTC6//HI++OAD7rrrLl5//XUuvfRSRo0axbBhwwqdc/Lkybzxxhvcf//9TJw4kYoVK54zhpycHA4cOFDocvLkyQLH5eXl0bNnTxISEnjxxRdp06YNTz/9NE8//bT7GKfTyfXXX88rr7xCz549efnll2nQoAEjRowoFOvYsWPp378/ISEhPPPMM4wdO5bk5GTmzp1b4LiNGzdyyy230L17dyZOnEhsbCypqan8/vvvxfo/FhGRC+QUEREpA5MnT3YCzu+++865f/9+544dO5yffvqpMz4+3hkWFubcsWOH+9jMzExnt27dnP369XPm5OQU6/w1atRwAs7PPvvMve/o0aPOKlWqOFu1auXe9+yzzzqjoqKc69evL/D4kSNHOoOCgpzbt293Op1O55YtW5yAMyYmxrlv374SxVDUZdy4ce7jBgwY4AScDz30kHufw+FwXnPNNc7Q0FDn/v37nU6n0zl9+nQn4HzuuecKPM8tt9zitNlszo0bNzqdTqdzw4YNTrvd7rzxxhudeXl5BY51OByF4vvpp5/c+/bt2+cMCwtzDh8+vFivUURELox6vEREpEx169aN+Ph4kpOTueWWW4iKiuKrr76iWrVq7mOee+455s6dy44dO+jWrRtdu3ZlwYIF5z13UlISN954o/t2TEwMd911F8uWLSM9PR2AadOm0blzZ2JjYwv0SHXr1o28vDx++umnAue8+eabiY+PL/br69ChA2lpaYUuffv2LXTs4MGD3ds2m43BgweTnZ3Nd999B8CMGTMICgri4YcfLvC44cOH43Q6mTlzJgDTp0/H4XAwevRo7PaCb+1/nhvXuHFj9/BOgPj4eBo0aMDmzZuL/RpFRKTkVFxDRETK1Jtvvkn9+vU5evQo7777Lj/99BNhYWEFjnn++ed5/vnnS3zuunXrFko06tevD5ghjYmJiWzYsIGVK1eeNZnat29fgdtnFvoojri4OLp163be4+x2O7Vr1z5rrADbtm0jKSmJ6OjoAsc1atTIfT+Y+XF2u53GjRuf93mrV69eaF9sbCyHDx8+72NFROTCKfESEZEy1b59e3dVw969e3PZZZdxxx13sG7dOsqVK+fx53c4HHTv3p3HHnusyPtdyY9LRESEx2MqS0FBQUXudxZRrENEREqPEi8REbFMUFAQ48aN44orrmDSpEmMHDnyos63ceNGd/l5l/Xr1wO41wCrU6cOx48fL1avlCc5HA42b95cINH7c6w1atTgu+++49ixYwV6vdauXeu+H8xrcjgc/PHHH7Rs2bJsXoCIiJSI5niJiIilunbtSvv27Xn11Vc5derURZ1r9+7dfPHFF+7bGRkZvP/++7Rs2ZLExEQAbr31VhYsWMC3335b6PFHjhwhNzf3omIoiUmTJrm3nU4nkyZNIiQkhKuuugqAXr16kZeXV+A4gFdeeQWbzcbVV18NmJ5Du93OM888U6Akvuu8IiJiPfV4iYiI5UaMGEGfPn2YMmUKDzzwwAWfp379+gwcOJDFixeTkJDAu+++y969e5k8eXKB5/rqq6+49tprSU1NpU2bNpw4cYJVq1bx6aefsnXrVuLi4i44hl27dvHBBx8U2l+uXDl69+7tvh0eHs6sWbMYMGAAHTp0YObMmXzzzTc88cQT7vln1113HVdccQVPPvkkW7dupUWLFsyePZsvv/ySIUOGUKdOHcDMbXvyySd59tln6dy5MzfddBNhYWEsXryYpKQkxo0bd8GvR0RESocSLxERsdxNN91EnTp1mDBhAvfdd99Z5yGdT7169XjjjTcYMWIE69ato1atWkydOpWUlBT3MZGRkfz444+88MILTJs2jffff5+YmBjq16/P2LFjKV++/EW9luXLl9O/f/9C+2vUqFEg8QoKCmLWrFk8+OCDjBgxgujoaJ5++mlGjx7tPsZut/PVV18xevRopk6dyuTJk6lZsyYvvfQSw4cPL3D+Z555hlq1avHGG2/w5JNPEhkZSfPmzYuMRUREyp7NqTEIIiLiB2rWrEnTpk35+uuvrQ7lvFJTU/n00085fvy41aGIiEgZ0RwvERERERERD1PiJSIiIiIi4mFKvERERERERDxMc7xEREREREQ8TD1eIiIiIiIiHqbES0RERERExMO0jlcJORwOdu/eTXR0NDabzepwRERERETEIk6nk2PHjpGUlITdfu4+LSVeJbR7926Sk5OtDkNERERERLzEjh07qFat2jmPUeJVQtHR0YD5z42JibE4GsjJyWH27Nn06NGDkJAQq8MJeGoP76M28S5qD++jNvE+ahPvovbwPt7UJhkZGSQnJ7tzhHNR4lVCruGFMTExXpN4RUZGEhMTY/kPnqg9vJHaxLuoPbyP2sT7qE28i9rD+3hjmxRnCpKKa4iIiIiIiHiYEi8REREREREPU+IlIiIiIiLiYUq8REREREREPEyJl4iIiIiIiIcp8RIREREREfEwJV4iIiIiIiIepsRLRERERETEw5R4iYiIiIiIeJgSLxEREREREQ9T4iUiIiIiIuJhAZt4vfnmm9SsWZPw8HA6dOjAokWLrA5JRERERET8VEAmXlOnTmXYsGE8/fTTLF26lBYtWpCSksK+ffusDk1ERERERPyQzel0Oq0Ooqx16NCBdu3aMWnSJAAcDgfJyck89NBDjBw58pyPzcjIoHz58hw9epSYmJiyCPesPvx1G18t38XBgwepVKkSdpvJo222/GNc2zZshfadyXbGTlsRx9mKeZxrb1GPLW48rp9IJ84i9hU+jvMe5zzHY/P3F/WrUPA4Z+F9zj/f53S3h81mO+vrLO12OfPYov6/zzz6YtqmOEr6GBsX8iTFP9TpcLBnzx4SE6tgt9uL/rkq1I6u2wXv56z3n3HOP5+7iGPO+ZzF/Pkv9ISFdp/757kYpyj6d+KsxxbvvE6ngyOHj1KhQvkCP98lias4z130+Yp3YKC9KzqdTjIyMoiJiTlvm0jZOLNN7HbzXpL/Nxqw2Qq8J9go+B5Q4O+5jT8de8a5/nSbIs5z5vv/mefm9OPOd24wv3tOp/ndOvNvnhPX79uf9zkL/C117YOCfyPPdk6c+fe7zu46Z4G/p4X2nXnOM5/TScaxY8RER3v8d+RCzl/SR5zr80iRnxHO8fmiOJ8PC7zPF3rcmXcVjOFc9zmdTg7s388Hg64iLiay0GssSyXJDYLLKCavkZ2dzZIlSxg1apR7n91up1u3bixYsKDQ8VlZWWRlZblvZ2RkAJCTk0NOTo7nAz6HLfuP8+uWw4CdjRmHLY1FzqT28D52OLjX6iDEzcb2ExlWByEF2ODEMauDkALUJt7Fxp7M41YHIQXYyTyVTU5EiKVRlCQfCLjE68CBA+Tl5ZGQkFBgf0JCAmvXri10/Lhx4xg7dmyh/bNnzyYy0toMu+IJGFCv4PccZ+vVKer+s+4vqteoqMeU4Nvz4sRV1Dc2Z+udOee+onqOijr+Is5dnOMv9P/hrPuK6qUr6viSnrcEj7FCafY+nOtn48/HFKeNi/WzdgHnK7T/XA86y7mLUuzjSvs5LuJ8F+NCnqPEj1EHkXiSq+el4K5C79Nn9tpwluP/vK+4xxfu+S/euf78t7uov28FeszOcazt9D9nO7bA7Ys41rXvbPsvlje8r57r815JP4MUdfvP7e7pc/7684+EBhXxJGUoMzOz2McGXOJVUqNGjWLYsGHu2xkZGSQnJ9OjRw/LhxqCybLT0tLo3r07ISHWZvyi9vBGahPvovbwPmoT76M28S5qD+/jTW3iGg1XHAGXeMXFxREUFMTevQWHHe3du5fExMRCx4eFhREWFlZof0hIiOUNfSZviyfQqT28j9rEu6g9vI/axPuoTbyL2sP7eEOblOT5A66qYWhoKG3atGHOnDnufQ6Hgzlz5tCxY0cLIxMREREREX8VcD1eAMOGDWPAgAG0bduW9u3b8+qrr3LixAnuvvtuq0MTEfFfOSexrfqcFtunYVt2ABr2gpgqVkclIiJSJgIy8brtttvYv38/o0ePJj09nZYtWzJr1qxCBTdERKQU7P0Dlr4HK/5L8Kmj1ASY8T3MGAZVWkKDq6F+itlWOXMREfFTAZl4AQwePJjBgwdbHYaIiH/KzoTfv4AlU2DnIvduZ/lktoQ2pGbIQey7l8Ke5ebywziIrmISsPo9odblEGpt5VgREZHSFLCJl4iIeED6apNsrfwEso6afbYgM6ywTSq5yZexata3JPfqhf3UIdgwG9bPgk3fw7E95rFLpkBwuEm+GvQ0iVhMkoUvSkRE5OIp8RIRkYuTfQJWf24Spl2/5e+vUAPaDICW/SD6dNXYMxeajE6A1v3NJecUbP3ZJGHrZ8HRHbDhW3NhKCQ2Pz0ksacZkmgPuNpQIiLi45R4iYjIhdmzApa8Z3q3so+ZffZgaHgttEk1PVbFTZBCwqFeN3Pp9RLs/T0/Cdv5G6SvNJcfx0O5RKjfA+pfDbUvh9Aoj71EERGR0qLES0REii/rGKz+zPRu7V6Wvz+2lkm2Wt4B5Spf3HPYbJDY1Fy6PArH958ekjjTDEk8ng5L3zeX4HCo1cX0hNVPgfLVLu65RUREPESJl4iInN/uZSbZWvUpZB83++wh0Og6k3DV7Oy54X/l4qFVP3PJzcofkrhuFhzdbpKyDbPhGyCxmekJq98TklppSKKIiHgNJV4iIlK0Uxmw+lOTcO1Zkb+/Yp383q2ouLKNKTgM6l5lLle/CPvW5A9J3LEI0leZy08vQlTl/CGJda7QkEQREbGUEi8REcnndMKupbBksimYkXPC7A8KhcY3QOsBUPMy71hvy2aDhMbm0nkYnDgAG9LMkMSNc+HEPlj2gbkEhUGtzqeHJPaECslWRy9StGPpBE1L5fL9uwg69E+IiIWIChBe4Yzr2D/drgDh5SEoxLq4ReS8lHiJiAicOmqKZCx5D/auyt8fV9/0bjW/HaIqWRZesUTFQcu+5pKbDdt+OT0kcSYc2QYbvzOXGY9CQtP8JKxqGw1JFO/x7RPYty+gAsC27SV7bGi5IhKyP1+XL/q+4NDSiV9EzkqJl4hIoHI6TcXAJVNMwYzck2Z/UBg06W0SruodvaN3q6SCQ83wwjpXQM+/w/51pids/bew41fYu9pc5k2AqHiol2KKc9S5EsLKWR29BKotP8Hqz3Da7CxLvpfmrdsQnH0MTh2Bk0fOuD5a8Larqmj2cXPJ2Fny5w6JLEbSVsR1eHlTlVREzkuJl4hIoDl55HTv1hTY93v+/viGp3u3boPIihYF5wE2G1RuaC6XDYUTB2FjmukN2zgHTuyH5R+YS1CoKRRSv6dZvLlCdaujl0CRlwPfPAqAo8097Mi7jGZNekFIMYYP5uWaZMydmB3OT8z+nKQVuD6av9B5Tqa5HNtd8tiDw8+enFVuDC36qkdNBCVeIiKBwek0PT1LpsDvX0DuKbM/OBya3GQSruT2vtm7VVJRlaDF7eaSmw3b55uesHUz4fAW2DTHXGaOgMpNTE9Yg6tPD0kMsjp68VcL34ID6yAyDsflo2DuL8V/bFCw+bm+kOHAjrw/JW0luD6VATjN35Pj6eZSlF9ehW5jTRXUQPgbI3IWSrxEpLCjO2HBm/nf/Gv+i+/KPAQrp5qEa//a/P2Vm5zu3epjJuoHquBQqN3VXFJegAPr80vV71hoegT3/Q4/vwyRcVCvh+kJq3MlhEVbHb34i6O74Ie/m+3uz5jhe2XFHmR6uC+kl9vhMD1mZ+tdyzwAK6bCoc3wSX+o3glSnjNfYogEICVeIlLYD3+HZf+Bhf+ASnXhkr+aoSKhkVZHJsXhdML2Bad7t6ZDXpbZHxwBTW82CVe1tvrm+c9sNohvYC6XPmKS1o3fmZ6wjXNOf4j8yFzsIaa6Y+dhZgFnkYsx+2+mgmhyB/O3Ni/P6oiKx24/XXXxHF/eXP44/PIazJ9kepf/dSU06wNXPa3qohJwlHiJSGE7fjXX9hA4uBG+GQZzn4N290L7+6BcZWvjk6KdOAgr/gtL3zM9Ny4JzaBtqvmwU5bfpPu6yIrQ/FZzycsxyaxrSOKhTbD5e7Pvnm8hqaXV0Yqv2vwD/P452OzQa4JJZnwl8SqOsGi48m/Q5m6Y+6z5G7VqGvzxFXT8K1w2DMJjrI4yMGQdg01zoWpbKF/V6mgCksYPiUhBmYfyP7Q/vNRUhKtQHU4eMovSvtIEvhxkFq4V6zmdsGUefDoQXm4Is5807RcSBa3vgvvmwgPzTNKspOvCBYWYnq2U583vxeDfoM5VZm7L1DvNGmIiJZWbDTMeM9vt7oUqza2Nx5PKV4Ub34b7f4Aal5me+J9fgddbweL/MwVCxDMyD8H34+CVpvDJXfC/R6yOKGCpx0tECtq52FxXqmcSrksehHb3wdqvYcEkc79rUdo6V0GnwVD7Cg1bK2snDsDyj8xwwkOb8vdXaWGGEja9Rd8ie1JcPbjlXTNs6tAmmJYK/b/QArZSMr/mF9TgiietjqZsJLWC1K9h3QxIG50/qmLRO9D9WajXXe8npeVYunnf/m2yWWbAZfdS62IKcEq8RKQg1zDD5A75+4KCzbpOTXrDjkUw/w2TiLmqvyU0hY6DzPyh4DArog4MDgds/ckkW2u+BkeO2R9azgwjbDPAfKiRshFRAW7/CP59FWydB7Ofgqv/bnVU4iuO7oIfxpvtHs+an6dAYbNBw2tMsZrfJsMP40zxn4/6mEI3PZ6DxGZWR+m7Dm+FX143X5C65vgmNDPv09MfgMyDphfMn5YN8RFKvESkoB2LzHVy+6LvT24Pt/0HDm0x5Y+XfWAWop3+IHw31swBa3uP/qCXpv3r4Y8vYfmHpty5S1Lr071bN2vRX6tUbgg3/hOm9jO9F1VaQMu+VkclvmD2k6cLalwCzW+3OhprBIVAh/vNPMp5E+DXf5o5b293hlb94Iq/QUwVq6P0HfvXwbyXzRw65+l5gskdoPOj+T2Jc58zC2wf3AiRZ3mfF49R4iUi+fJyYdcSs322xMulYi3o9SJcMcr0wPz6Tzi2x0yenjcRWvYzwxQr1fF42H7H6TTf/v7xpalKuP+M+XSh0eZDSpsB5kO+WK/RtaZy24/jzdyJ+AZQtbXVUYk32/yDWU/PZodrJmjJjogKpper3b3w3Rjzf7PsA1j9uakw2ukhCI2yOkrvtXuZed9d8zXgNPvqXAmdh0ONSwsO3YyraxKvA+vP/z4vpU6Jl4jk27sacjIhrDzENSjeYyJi4bKhcMkg82a54A1IXwWL/wWL/22Gk3QcDNUv0bj9c3E6zf//H1+ay5lVCe0hZvhNk97Q5EZ9APFGl4+EPSth/UxTbOP+H6FcvNVRiTfKzYYZI8x2u/s0pO5MsTWhzxSzhMm3T5g5xT+MM1/uXfk3U2pfi5jn2/qLSbg2zcnf1/Bas8zF2dZKq1TPJP4HNpRJiFKQEi8RyecqrJHcruTfwAaHQovbTG/Mlp/MhN4Ns81csLVfmzeBjoOg0Q1mzpiYZGvP8vxk69Dm/PuCQk3xksY3mAV7A3mRY19gt8NN/4R/XQUHN8C0AXDXlyq2IYUt/If5YiUqHq54wupovFNyexiYZr7M+24MHNlmqukufNsswFy7q9URWsfpNOsLzptolrMAsAVBs1vMl6CVG5378XH1zfXBjZ6NU4qkTz8ikq+owholZbNB7cvNZf86WPAmrPjYDGH89B4oXx0ueQBa9Q/MqntOJ+xaCn98YZKtI9vz7wsOh7rdoHFvqJ8SmP8/viy8vCm28a8rYdsv5hv7Xi9ZHZV4k6O74McXzXb3ACuoUVI2GzS9yYya+PWf8NME2LsK3r8B6qWYgiTxxRyZ4Q8cebDmfybhSl9p9gWFQqs7odPDZvh/ccTVNddnjqqQMqPES0TyuRKvau1K53zxDeD61+HKp+C3/4NF/4Kj280H0h/+btaZ6vAAVEgunefzVg6H6U109Wxl7My/LzgC6vcwPVv1epjFRsV3xdeHm96Bj/ua8thVWpgPRiJQsKBGiwAtqFFSwWFw6cNm3vCP4817yYZvTa9Pm1ToOsq/h/Xm5cDKT8yaZwdPDw8MiYK2d5th/CUtPlKpnrk+tMXM69YIlDKl/20RMY6lm94Xm/3sY8MvVLl46DrSTJJeOdX0gh1Yb4YjLnzLzFvqOMi/ChI48mD7QpNorfnKFB5xCS1nerQa32B6uDRny7807AVdn4AfXoCvh0J8I6hWyr9T4ns2fV+woIbmvJZMVCVT0Kn9fZD2NKz7xiRhKz+BLsOhw4MQEm51lKUn56QpMPLLa3B0h9kXXt58WdnhgQuvHBxTFUIizXzuI9tUAKuMKfESEcNVRr5yE88NcQuJMN9QtrrLfFu54A0zH2z1p+ZS41LzDV79nr5Z5Ssv1wwx++NLMyTkxL78+8JioMHVJtmqc6X5vxD/1WWEGQ609uvTxTZ+gOgEq6MSq5xZUKP9/SqocTHi6kHfj2DLPNODuGeFmQe2+F3o9rRZXsOXk9pTGSahXPAmnNhv9kVVhk6DzVItFzsqwm43yVb6KvMFqBKvMqXES0QM9/yuUhpmeC52uxleV7+HedNc8Cas/swkLdt+gUp1TVWrFn0hNNLz8VyMvByzeO4fX5pSvpkH8u8LL28qTDW+wUwG1+LSgcNuh95vwb83wIF18MldMOB/pgiNBJ6Fb5phYlGVzdA4uXi1OsN9P8CqT2DOM2YY+2cDTfGSlBdMJV1fknnIjABZ9E84ddTsK1/dDLNsdWfpfllXqd7pxGuD+UJQyowSLxEx3AsnX0RhjQtRpYWZE9NtjJlAvWSyqbb0zTCz0GO7gabksjf1FuRmw5Yf4Y/psPYbOHk4/76IimZdp8Y3QM0u+qAdyMJj8ott7FgIs0bCtS9bHZWUtaM74cfTRVa6P6OCGqXJbjdz5Rpdb5Lbea+YQk7vpph93cdCxdpWR3luGXvMsPvfJpv5f2ASo87DoFkfz1RGdVc2VEn5sqbES0QgN8uUNQfrFlSMSTJvkl1GwPIPTS/YkW3w00tmjHuzW808sITG1sSXcwo2f296ttbOgKyj+fdFxkGj604nW501WVnyxdWFm/8FH91mhg9VaWEWv5bA8e3pghrVO6qghqeERpr3jlZ3wffPw7L/mLm162aaoZ2Xj/C+JTkObYFfXoXlH0FettmX2By6PGpGSnhyvbK40wU2tJZXmdOnAxExw/3yss26MrHFLEnrKWHloMNfoN29Zn7M/EmwcxEs/8Bc6lxlxrrXvsLz4/hzTpq5aH98CetmQfax/PvKJZhvVBvfADU6aVFPObv6KXDlk6YHd8ajZp0dq77gkLK1aa7pGbcFQS8V1PC46ARTSbfDAzD7b2Zh4YVvmi/zLn/cvK9YPQph7x+mQuHqT8HpMPuqd4TOj0Ldq8rmZ6SSq6S8Eq+ypsRLRM4oI9/eez4Y2INMUtP4BjMMcsEkU7Bi0xxzqdzE9IA1u6V0505ln4ANaSbZWv9t/tAPgOik/JiSO/hmARCxRudHzRcca/4HU/ubYhslLQMtviU3G2Y8Zrbb3w+JTa2NJ5AkNIb+n5svzmY/Bfv+gG9HweJ/meGeDa8t+/e6XUtg3svmC0WXut2g83Dz5V1ZciVemQfMUHlv6w30Y0q8ROSMwhpe+i18cntIfh8Ob4WFb8PS92Hf7/DlX2HOWPOhpu09F15eN+uYSbL++NIkXbkn8+8rXx0aX28WNa7aRsmWXBibzRTbOLAR9q8xxTZSv1bBFX92ZkGNK1RQwxJ1u0Gtrma0xNzn4dBmU2W0eidIed7zS5g4nbD1Z7Po8ebvT++0maHpnYdBUivPPv/ZhJUzZeUzdpm/SWVRVEsAJV4i4nRaV1ijpGJrwtV/N2uCLZliinEc2w1znzVvbC3vMNUQi1Me99RRM3zwjy/Nt6J5WQWfp3Fv07OV1Mp7egHFt4VFw+0fwr+uMMNnZ4www6LE/xzdCT++aLZ7PGsqnIo1goLNMiZNbzbzhedPgu3zze9hs1vhqtFQIbl0n9PpNF/mzZtoftfBDDdtfitcNhTiG5Tu812ISnVPJ17rlXiVISVeIoHuyHY4vhfswZDU0upoiieiAlw2xCRZf0yH+W+YNZMW/xsW/x806GXmgVXvWPBxJw+bwhh/fGm+fXRNaAbzJuRKthKbKdkSz6hUB25+Fz68BZa+Z37n2t5jdVRS2r59wixQW70jNL/N6mgEzBcfV/4N2txtvqxb8V9Tiv6PL6HjX+GyYRe/hqUjz7wnzXsZ9q42+4LCoHV/6PQwxNa46JdRauLqmeq8qmxYppR4iQQ6V29XlRa+t6hvcKj5BrFZH7OW1oI3Yf0sWPeNuSS1xtb+L1Q/uIig/74HW38ER27+4+Mb5s/ZqtxYyZaUjXrdzLfsc8aaOUCVG/vemkNydpvmmg/zKqjhncpXhRvfzi/AsXWeKXax9D9wxRPQekDJK9PmZsPKqeY8hzaZfaHlzJcqHQdBdGLpv46L5SoprwIbZUqJl0ig2+kjwwzPxWaDWl3MZf96M7dixceweynB0/9CgVH0CU1NotXoeqjc0KqIJdBdNtQU2/hjuim28ZcfzZIK4ttys8wQUlBBDW+X1NIsar5uJqQ9lb9+5KJ3oPuzUK/7+ZPm7Ewz53j+62bYHkB4BbjkQdP+FzrvuCyosqEllHiJBDp3RUM/GeMdXx+uew2ufAoW/x/O5R9yNMtGdIc7CWp6o1lXScRqNhv0/of50LPvdzPhP3UGhIRbHZlcjAVvmg/wKqjhG2w2aNjLJFm/TYYfxsH+tfBRH6jdFXo8Z4ae/9mpo2Zo+4J/mMqAYJYY6fSQmU8WFl2Wr+LCuNbyOrQZ8nK1/mQZUXkukUCWdRzST49D9+Uer6JExUHXx8kdtIQfGz6D49IhSrrEu4RGmWIb4RVMqekZw82kfPFNR3aYBd/BfGBXQQ3fERQCHe6Hh5eZ5CkoFDb/AG93hi8HwbE95rgTB2DOs/BKM5jzjEm6KlSHa16GR1aax/pC0gUQUw2CI8CRA0e2WR1NwFB6KxLIdi8FZ575A1y+qtXRiASeirWgz2T44GZY9gFUaQnt77M6KrkQ7oIanczcU/E9ERVM0tzuXvhuDPz+BSz7gODVn9M2qinBq1ebNgaIa2BKwje92SRuvsZuN8MN964yPe/FqQYsF009XiKBzF1G3kvX7xIJBHWuhG5jzfaskbD1F2vjkZLbOAfWfGUKalyjgho+L7Ym9JkCA9OgWjtsOZlUPbIIW06m+XLktg/grwuhxe2+mXS5uEaBqLJhmVHiJRLIlHiJeIdOD0HTW0zVzWkDzDpQ4htys2DmY2a7w18goYm18UjpSW4PA9PIvfHfbK94Gbl9p8H9P5gFkO1+8BFalQ3LnB/81IjIBXE6z6hoqMRLxFI2G1z/BiQ0gxP7TbGNnJNWRyXFsWCSKahRLsEs7i7+xWbD2bg3y2rcj7P2Ff7Vm1npdIENJV5lRomXSKA6uNEsKBwcAYnNrY5GREIjTbGNiIqwexl8PUzFNrzdkR3wowpqiI/SUMMyp8RLJFC5ysgntfLtMeoi/iS2him2YbPDio/g139aHZGcy7ejIPck1LjULOQu4ktcPV4n9psvYsXjlHiJBCpX4qVhhiLexbV+EJhKeVvmWRqOnMXG72DN/0xBjV4qqCE+KKwcRJ9euP3ARmtjCRBKvEQClbuwhp+t3yXiDy75KzS71Sz3MG0AHNludURyptwsmOEqqPEAJDS2Nh6RC6XhhmVKiZdIIDp5BPavNdvq8RLxPjYbXP86VGkBmQfh436QnWl1VOIy/w04tEkFNcT3qbJhmVLiJRKIdv5mrivWhqg4a2MRkaKFRMBtH0JkJUhfCf97RMU2vMGR7fDTBLPd43kIj7E2HpGL4a5suN7aOAKEEi+RQOSe36VhhiJerUIy9HnPzCNa9Qks/IfVEcm3T5wuqHEZNLvF6mhELo57qKHmeJUFJV4igUjrd4n4jlqdIeUFsz37Kdj8g6XhBLQNZxbUeEkFNcT3uYYaHtoMebnWxhIAlHiJBBpHXv5Qw2pKvER8Qoe/QIu+p4tt3A2Ht1kdUeDJzYKZI8z2JQ+qoIb4h5hqZj3PvGw4or8rnqbESyTQ7PsDso9DaDRUbmR1NCJSHDYbXPuKWXfv5CEV27DC/NdNr0C5RLj8caujESkddjtUqmO2NdzQ45R4iQQa1/yuam3BHmRtLCJSfCERcNsHEBUPe1fBV4NVbKOsHNkOP0002z2eU0EN8S9xrgIbqmzoaT6TeD3//PN06tSJyMhIKlSoUOQx27dv55prriEyMpLKlSszYsQIcnMLjlf94YcfaN26NWFhYdStW5cpU6Z4PngRb7JjsblWYQ0R31O+mim2YQ+G1Z+ZsubiebNGqaCG+C9VNiwzPpN4ZWdn06dPHx588MEi78/Ly+Oaa64hOzub+fPn89577zFlyhRGjx7tPmbLli1cc801XHHFFSxfvpwhQ4Zw77338u2335bVyxCxnruiYTtr4xCRC1PzUuj5d7P93dOwaa618fi7DWmw9mtTUOOaCSqoIf7H1eOloYYe5zOJ19ixYxk6dCjNmjUr8v7Zs2fzxx9/8MEHH9CyZUuuvvpqnn32Wd58802ys7MBePvtt6lVqxYTJ06kUaNGDB48mFtuuYVXXnmlLF+KiHWO74fDWwAbVG1rdTQicqHa3Qut7gSnwxTbOLTF6oj8U24WzHzMbF/yoObFin/SUMMyE2x1AKVlwYIFNGvWjISEBPe+lJQUHnzwQX7//XdatWrFggUL6NatW4HHpaSkMGTIkLOeNysri6ysLPftjIwMAHJycsjJySndF3EBXDF4Qyzi/e1h2zqfYMAZ35Dc4Cjw0jhLk7e3SaBRe5SiHn8naO8f2HcvxfnxHeQOmAmhUSU+jdrk7Ow/v0rQoc04yyWQe+nwMvubqTbxLn7fHjE1CAE4sY+cYwcgvLzVEZ2XN7VJSWLwm8QrPT29QNIFuG+np6ef85iMjAxOnjxJREREofOOGzeOsWPHFto/e/ZsIiMjSyv8i5aWlmZ1CHIGb22PxrumUg/Y5khgxYwZVodTpry1TQKV2qN0hMfexeX7NhG+7w/2/asPv9UcdMFD4dQmBUVk7efKNRMAWFLpJnbNmVfmMahNvIs/t0ePkFgicg6z4H/vcziqjtXhFJs3tElmZvErzFqaeI0cOZLx48ef85g1a9bQsGHDMoqosFGjRjFs2DD37YyMDJKTk+nRowcxMdZXNcrJySEtLY3u3bsTEhJidTgBz9vbI+j9fwBQrePNVG3Ry+Joyoa3t0mgUXuUPtuOejg/6E3VI4tIjE3B0emREj1ebVK0oGl3YXdm46hxKS36PUOLMpzbpTbxLoHQHkGH/wVb59GpQWWczb3/84E3tYlrNFxxWJp4DR8+nNTU1HMeU7t27WKdKzExkUWLFhXYt3fvXvd9rmvXvjOPiYmJKbK3CyAsLIywsLBC+0NCQixv6DN5WzyBzivbIzcbdi8DILhmJ/C2+DzMK9skgKk9SlHty6DXi/D1UIK+f46gpJZQr9t5H/ZnapMzbEiD9TPAHoz9monYQ0MtCUNt4l38uj3i6sPWeQQf3uRTnw+8oU1K8vyWJl7x8fHEx8eXyrk6duzI888/z759+6hcuTJguh9jYmJo3Lix+5gZfxpelZaWRseOHUslBhGvlr4K8rIgomL+Yoki4h/a3gO7l8PS9+Cze+C+7/V7fqFyTsGMEWa7wwMqqCGBwV3ZUAU2PMlnqhpu376d5cuXs337dvLy8li+fDnLly/n+PHjAPTo0YPGjRvTv39/VqxYwbfffsvf/vY3Bg0a5O6xeuCBB9i8eTOPPfYYa9eu5R//+AeffPIJQ4cOtfKliZQNdxn59iqHLOKPer0E1drDqaPwcT/IOmZ1RL5p/hum+mt0Feg60upoRMqGey0vlZT3JJ9JvEaPHk2rVq14+umnOX78OK1ataJVq1b89ttvAAQFBfH1118TFBREx44dufPOO7nrrrt45pln3OeoVasW33zzDWlpabRo0YKJEyfy73//m5SUFKte1sVZ/y1B/+1D3LHfrY5EfMGZiZeI+J/gMLj1fSiXCPvXwPQHwem0OirfcngbzDMFNejxHIRFWxuPSFlx9Xgd2gSOPGtj8WM+U9VwypQpTJky5ZzH1KhRo9BQwj/r2rUry5YtK8XILLRhNvbN31OzwglghNXRiLfbudhcJ3ewNg4R8ZyYKnDbf2ByL1jzP5NEdNH7Q7HNGgW5p6BmZ2h6s9XRiJSd8skQHG5+/o9sg4rFq7EgJeMzPV5ShDapAFQ5sgSO77M2FvFuR3dCxi6wBUFSK6ujERFPSm4P10w023Ofh/XfWhuPr1g/G9Z9A/Zg6DVBQ7IlsNjtUPH0vFANN/QYJV6+LLEZjqQ22MnDvvK/Vkcj3sw1zDCx2QUtsCoiPqbNAFNwAyd8dq8+SJ1PzimY+ZjZvuRBqGzdMjYillGBDY9T4uXjHK0HAGBf9h9wOCyORrzWjtNLLWiYoUjg6Dkeki+BrAz4+A44Vfy1ZgLO/NfzC2pc/rjV0YhYw5V4HVhvbRx+TImXj3M27k1OUCS2I1th8/dWhyPeyp14qbCGSMAIDjXFNqKT4MA6+OIBfUFXlMNbYd7poZkpz6ughgQuVTb0OCVevi4kkh2xncz2ksnWxiLeKTsT0leabSVeIoElOgFu+wCCQs38pZ9esjoi7zPrCVNQoFYXaHKT1dGIWEdDDT1OiZcf2Bp3pdlYOwOOpVsbjHif3cvAkWuG0JRPtjoaESlr1drAta+Y7R9eMO8VYqz/Nr+gxtUvqaCGBLZKdc318b1mPUApdUq8/MCxiGo4qrUHZx4s+8DqcMTb7DxjmKE+VIgEplZ3Qrv7zPbn98N+zeEoWFDjryqoIRIeY9YBBA039BAlXn7C0eous7H0PY3hl4Jc87uqaZihSEDrOQ5qXArZx+DjvvpG+5fXzPyu6CS4/DGroxHxDhpu6FFKvPyEs9ENEF4ejmyHTXOtDke8hdOZX0peFQ1FAltQCPR5D2KqwsGNpucrUL+oO7wVfn7ZbKc8p4IaIi6qbOhRSrz8RUgEtOhrtlVkQ1wObYbMgxAUBlWaWx2NiFitXPzpYhthsH4W/DDO6oisMWuUCmqIFMVd2VA9Xp6gxMuftLnbXK+bCRl7rI1FvINrmGFSKwgOszYWEfEOVVvDda+Z7Z9exLb2G2vjKWvrZsG6GaagRq8Jmvsqcqa4+ub6oOZ4eYISL39SuSFU73i6yMZ/rI5GvIF7mGE7a+MQEe/Ssi90eBCAoP/9leiTuywOqIzknMwvqNFxEMQ3sDYeEW8Td7qy4cFN4MizNhY/pMTL37h6vZa8p18YOWPhZM3vEpE/6fEs1OyMLfsEHTe9iP3752D7Qv9+7/jlNTiyzRTU6KKCGiKFlE82Q5HzskzdAClVSrz8TeMbICIWMnbCxjlWRyNWOpUB+/4w26poKCJ/FhQCfabgrFCDiJzDBM1/Fd5NgZfqwmf3wapP4eRhq6MsPYe2wM+n1zNLeR7Cylkbj4g3sgflr+el4YalTomXvwkJhxZ3mG0V2Qhsu34DnFChBkQnWB2NiHijqDhy7/2BJTUewNHkJlMd9+QhWPUJfDYQXqwD714NP78K+9aYSqm+yl1Q43JocqPV0Yh4L9dwQ1U2LHXBVgcgHtBmACx801SsOroLyle1OiKxgoYZikhxhEWzs2Inmvfqhd1uM4uur58F62fD/jWwfb65fPc0lK8O9VPMpWZn82WfL1g3E9bPBHuICmqInI8qG3qMEi9/FN/ALJK57RdTZKPrSKsjEiu4C2tomKGIFFNQMNToZC7dn4HD22DDbFj/LWz5CY5uh8X/MpeQSNN7VD8F6vXw3i/5ck7CzMfNdsdBEF/f2nhEvJ0qG3qMEi9/1eZuk3gtfR86P2reTCVwOByw8zezrcRLRC5UbA1of5+5ZJ8wydf6b83l2G7Ti7R+pjk2oVl+b1jVNmauiDdwFdSIqQpdRlgdjYj301BDj9GncX/V+HqYWREydsHGNGhwtdURSVnavxayMiAkCio3sToaEfEHoVHmvaTB1Wau197V+UMSdy6GvavMZd4EiKwEdbtD/R5Q5yqIqGBNzIe2wLyXzbYKaogUj2uo4fG9plBXeIy18fgRJV7+KjgMWt4BCybBb5OVeAUa1zDDam3U2ykipc9mg8Rm5tJlBJw4ABu/Mz1hG+dA5kFY+bG52ILMGpOu3rC4+mU3x2rWSFMWu3ZXaNy7bJ5TxNeFx0C5RDieDgc3mB5sKRX6RObP2txtEq+NaXBkB1RItjoiKSs7F5trFdYQkbIQFQctbjeXvBzz5Y9rSOKBdbDtZ3NJewpia0K9FNMbVuMyzxXoWDfT9MjZQ+Dql1RQQ6Qk4uqZxOuAEq/SpHLy/iyurqk65XSYIhsSONw9XprfJSJlLCgEal5mFmgevAgeWWESnzpXQVAoHN4Ki/4JH9wML9aG/94BS96DjD2lF4MKaohcHNdaXqpsWKrU4+Xv2qTC1nmmyEaXxzTsLBCcOJhfiahaW2tjERGJrQkd7jeXrOOw5UfTE7ZhNhzbA+u+MReAxOZQv6cZkpjUGuwX+P3wz6+qoIbIxXBXNlTiVZr0KdzfNbrOTHI+tgc2fAsNr7E6IvG0nafX74prAJEVrY1FRORMYeXM+1DDa0yBjvSVpjjH+lmwa4m5nb4SfnoRouLPKNBxpVncuTgObYafXzHbKS+ooIbIhYjTWl6eoMTL3wWHQct+MP91U2RDiZf/cy+c3M7aOEREzsVmgyotzOXyEXB8/+kCHbNg01w4sR9WfGQu9uAzCnT0NMOgzjZna6aroMYV0PiGsn1NIv7CNdTw4CZw5HnP8hA+TolXIGiTahKvjd+ZxTBja1gdkXiSO/FSYQ0R8SHl4qFlX3PJy4HtC/KHJB5Yb4bNb50Hs/8GsbVOD0nsATUuNV8ygimoseFbU1CjlwpqiFywCtUhKMx8iXF0hxkyLBdNiVcgqFQHal1uxtUvfR+uesrqiMRT8nLMcB1Q4iUivisoBGp1MZeU583wQdeQxG2/wOEt8Otb5hJazpSLr58CP71kHt9pcP5QKREpOXuQ+fy47w8z3FCJV6lQVcNA0fZuc73sA/PhXPzT3tWQexLCK+QvgCgi4usq1oZLHoC7psNjW+C2D6FVf7PWUPZxWPs1fPUQHNkOMdVUUEOkNKiyYalTj1egaHCNmah8PN18Y9joOqsjEk9wDTOs1u7Cq4GJiHizsHLQ6FpzcThOF+j41gwxPLABrnsVQqOsjlLE97l6jVXZsNQo8QoUwaHQ6k5T6em3yUq8/JVr/S4NMxSRQGC3Q1JLc+n6uNXRiPgXV0l59XiVGn0lHkha32WuN801C1iK/3EX1tDCySIiInIRKqmkfGlT4hVIKtY25XVxwpL3rI5GSlvGblN5yGaHqm2sjkZERER8WdzpOV7H0+FUhrWx+AklXoFGRTb8l6u3K6GJFgwVERGRixNeHsolmG3N8yoVSrwCTYNe5pfoxD5Y+43V0Uhp0vpdIiIiUprcww03WhuHn1DiFWiCQkyRDYAlUywNRUrZTiVeIiIiUopcww3V41UqlHgFotYDABts/t4sSim+L+cU7F5utqu1szQUERER8ROqbFiqlHgFotgaUPcqs60iG/5hz3Jw5EBUZa0uLyIiIqVDlQ1LlRKvQNUm1Vwv/xBysy0NRUqBe/2u9mCzWRuLiIiI+AfXUMNDm8yC5XJRlHgFqvo9oVwinNgPa7+2Ohq5WFq/S0REREpbhRoQFAq5p8ySNXJRlHgFqqAQaN3fbC+ZbG0scnGcTlU0FBERkdJnD4KKdcy2hhteNCVegaz1XYANtvwEBzdZHY1cqMNbzfIA9hCo0tLqaERERMSfqLJhqVHiFcgqVId63c22Ssv7rp2LzXVSSwgJtzQUERER8TOqbFhqlHgFujZ3m+vlH0JulrWxyIVxFdaopvldIiIiUsrclQ3XWxuHH1DiFejq9YDoJMg8CGv+Z3U0ciHOrGgoIiIiUpriTideBzdaG4cfUOIV6IKCzyiyMcXSUOQCZB2Dvb+bbRXWEBERkdJW6fQcr2N7zOcOuWBKvMQU2bDZYes8jd/1NbuWgtMB5ZMhporV0YiIiIi/iagAUZXNtj4nXhQlXgLlq5khh6BeL1+j9btERETE0zTcsFQo8RLjzCIbOaesjUWKzz2/S8MMRURExENciZd6vC6KEi8x6nWHmGpw8rCKbPgKhyO/lLx6vERERMRTVNmwVCjxEsMedHpBZWDJZGtjkeI5uAFOHYHgCEhoanU0IiIi4q801LBUKPGSfK37gy0Itv0C+9dZHY2cj2uYYdU2EBRibSwiIiLiv85MvBwOa2PxYUq8JF9MEtTvabZVZMP7af0uERERKQsVakBQKOSegqM7rI7GZ/lE4rV161YGDhxIrVq1iIiIoE6dOjz99NNkZ2cXOG7lypV07tyZ8PBwkpOTefHFFwuda9q0aTRs2JDw8HCaNWvGjBkzyupl+IY2qeZ6+UeQc9LSUOQ8dmh+l4iIiJQBexBUrG22D6rAxoXyicRr7dq1OBwO/vnPf/L777/zyiuv8Pbbb/PEE0+4j8nIyKBHjx7UqFGDJUuW8NJLLzFmzBjeeecd9zHz58+nb9++DBw4kGXLltG7d2969+7N6tWrrXhZ3qnuVWZNqFNH4I8vrY5GzibzEBw4PRy0mhIvERER8TB3ZUPN87pQPpF49ezZk8mTJ9OjRw9q167N9ddfz6OPPsrnn3/uPubDDz8kOzubd999lyZNmnD77bfz8MMP8/LLL7uPee211+jZsycjRoygUaNGPPvss7Ru3ZpJkyZZ8bK8kz0IWg8w27+pyIbX2vmbua5UF6IqWRuLiIiI+D9VNrxowVYHcKGOHj1KxYoV3bcXLFhAly5dCA0Nde9LSUlh/PjxHD58mNjYWBYsWMCwYcMKnCclJYXp06ef9XmysrLIyspy387IyAAgJyeHnJycUno1F84VQ6nG0uw2gn8Yh23HQnJ2r4L4hqV3bj/nkfYogn3bQoIAR9V25HnBz6E3K6s2keJRe3gftYn3UZt4F7WHYYutTTDgOLDe8s8e3tQmJYnBJxOvjRs38sYbbzBhwgT3vvT0dGrVqlXguISEBPd9sbGxpKenu/edeUx6evpZn2vcuHGMHTu20P7Zs2cTGRl5MS+jVKWlpZXq+drFtCTp6BK2f/EMq6vdWarnDgSl3R5/1mnDTOKBlYfD2aZ5isXi6TaRklF7eB+1ifdRm3iXQG+P2BP76AJk7VrNbC/57OENbZKZmVnsYy1NvEaOHMn48ePPecyaNWto2DC/x2XXrl307NmTPn36cN9993k6REaNGlWglywjI4Pk5GR69OhBTEyMx5//fHJyckhLS6N79+6EhJReSXHbpnD4+FZqH/uV6t3/D0IiSu3c/sxT7VGAI5fg1Q8C0KTnPTSp3Mgzz+MnyqRNpNjUHt5HbeJ91CbeRe1x2skj8PIzROQcptdVnSEs2rJQvKlNXKPhisPSxGv48OGkpqae85jatWu7t3fv3s0VV1xBp06dChTNAEhMTGTv3r0F9rluJyYmnvMY1/1FCQsLIywsrND+kJAQyxv6TKUeT/3uUKE6tiPbCVn/NbS8o/TOHQA8+vOx5w/IOQFhMYRUaQp2n5iqaTlv+50NdGoP76M28T5qE+8S8O0REg9R8XBiPyEZ2yCpldUReUWblOT5Lf3EFh8fT8OGDc95cc3Z2rVrF127dqVNmzZMnjwZ+58+bHbs2JGffvqpwDjLtLQ0GjRoQGxsrPuYOXPmFHhcWloaHTt29PAr9UF2u4pseKsdi8x1tbZKukRERKTsVFJlw4vhE5/aXElX9erVmTBhAvv37yc9Pb3A3Kw77riD0NBQBg4cyO+//87UqVN57bXXCgwTfOSRR5g1axYTJ05k7dq1jBkzht9++43Bgwdb8bK8X6v+YA+GnYtg7+9WRyMursQruYO1cYiIiEhgiVNlw4vhE4lXWloaGzduZM6cOVSrVo0qVaq4Ly7ly5dn9uzZbNmyhTZt2jB8+HBGjx7N/fff7z6mU6dOfPTRR7zzzju0aNGCTz/9lOnTp9O0aVMrXpb3i06ABr3Mtnq9vMeOX821Fk4WERGRsuRKvLSI8gXxiaqGqamp550LBtC8eXPmzZt3zmP69OlDnz59SimyAND2bljzFaycCt2fgVDvqeQYkI7thSPbABtUbWt1NCIiIhJINNTwovhEj5dYqFZXiK0JWRnw++fnOVg8bufpYYaVG0O49VU1RUREJIC4e7w2gsNhbSw+SImXnJvdDm1SzbaGG1pPwwxFRETEKhVqgD0Eck9Cxk6ro/E5Srzk/FreaX7Jdv0G6ausjiawqbCGiIiIWCUoGCqeXurpgOZ5lZQSLzm/cvHQ8BqzrV4v6+Rmwe7lZls9XiIiImIFd2VDJV4lpcRLiqft3eZ65SeQddzaWALVnpWQlwWRlfK/bRIREREpS6pseMGUeEnx1OxiPuxnH4PVn1kdTWByz+/qADabtbGIiIhIYKqkHq8LpcRLiufMIhtLNNzQEq6KhtXaWRuHiIiIBK64+ub6oErKl5QSLym+lv0gKBR2L8ufayRlw+lUYQ0RERGxXlxdc52xS9NPSkiJlxRfVBw0us5sL5liaSgB5+gOOLYH7MGQ1MrqaERERCRQRcRCZJzZVq9XiSjxkpJpc7rIxqppkHXM2lgCiau3K7E5hEZaG4uIiIgENg03vCBKvKRkal4GlepC9nFY9anV0QQO9zBDlZEXERERi7mGGx5Yb20cPkaJl5SMzaYiG1ZwVzRU4iUiIiIWU2XDC6LES0quxR2myMaeFbBrqdXR+L/sE5C+ymyrsIaIiIhYzT3UUIlXSSjxkpKLqgSNbzDb6vXyvN3LwJkH0UlQvprV0YiIiEigcy2ifGAjOBzWxuJDlHjJhXEX2fgMTmVYG4u/0zBDERER8SYVaoA9BHJPmrLyUixKvOTC1OgEcQ0g54SpcCieo/W7RERExJsEBUPF2mZbww2LTYmXXJg/F9lwOi0Nx29p4WQRERHxRnEqsFFSSrzkwrW4HYLCTOEHFdnwjIOb4OQhCA6HxGZWRyMiIiJiVHKVlFfiVVxKvOTCRVaEJr3N9pJ3LQ3Fb7nmdyW1guBQa2MRERERcVFlwxJT4iUXx1VkY/XncOqotbH4IxXWEBEREW+koYYlpsRLLk71SyC+IeRkwspPrI7G/+xcbK6rKfESERERL+Iaapixy6w5KuelxEsujs2W3+v1m4pslKqTR2DfGrOtHi8RERHxJpEVITLObB/caG0sPkKJl1y8FreZ4g/7foedv1kdjf/Y9RvghNhaUK6y1dGIiIiIFKThhiWixEsuXkQsNLnJbC+ZbG0s/mTH6WGGKiMvIiIi3kiVDUtEiZeUjrZnFNk4ecTSUPyGu7BGO2vjEBERESmKq8dLlQ2LRYmXlI5q7aByE8g9CSunWh2N73Pk5Q/bVI+XiIiIeCNXSfkD662Nw0co8ZLSYbNBm1SzrSIbF2/fGsg+BqHloHJjq6MRERERKaySq8drEzgc1sbiA5R4SelpfisER8D+NfnD5OTC7Fxkrqu2AXuQtbGIiIiIFCW2BtiDzbJCx3ZbHY3XU+IlpSeiAjS92WwvmWJlJL5vx+nES8MMRURExFsFhUDF2mZbww3PS4mXlC5XkY3fv4CTh62NxZe5C2so8RIREREv5hpueEBreZ2PEi8pXVXbQEIzyD0FKz62OhrfdOIAHNpstqu1tTYWERERkXOJO11SXpUNz0uJl5Qumw3appptFdm4MK5hhvENzfBNEREREW+lyobFpsRLSl+zWyEkEg6sg+0LrI7G97iHGba3Ng4RERGR89FQw2JT4iWlLzwmv8jGb5OtjcUXqbCGiIiI+ArXIsoZOyH7hLWxeDklXuIZriIbf3wJmYesjcWX5OXA7qVmu5p6vERERMTLRVaEyEpm+6B6vc5FiZd4RlJrSGwOeVmw4r9WR+M70leawiQRsVCprtXRiIiIiJyfe7ihCmycixIv8QybLb/XS0U2is81zLBae7Dr11NERER8gLuyoXq8zqVEn+xycnJYt26d+/aCBSqcIOfQrA+EljPlRbf9YnU0vsE9v0vDDEVERMRHqLJhsZQo8RowYADXXXcdTzzxBADDhw/3SFDiJ8KiodktZltFNopHiZeIiIj4Gg01LJYSJV6rV69m/fr1hISE8Oabb3oqJvEnbU4PN1zzFZw4aG0s3u7oTlMRyBZk5siJiIiI+AJXZcODG8HhsDYWL1aixKtKlSoAjB07ll9++YUtW7Z4JCjxI0ktoUpLyMuG5R9aHY13c/V2JTaFsHLWxiIiIiJSXLE1wR4MOZlwbLfV0XitEiVel156Kbm5uQC8/fbbdOhQeJ2hkydPlk5k4j9cRTaWTFGRjXPZudhcq4y8iIiI+JKgEIitZbY13PCsSpR4jR49muDgYABiYmKYPn26+76srCwmTpxIrVq1SjVA8QNNb4HQaDi0CbbOszoa77XjV3OthZNFRETE15w53FCKVKLEKzs7m1GjRtG2bVs6derkTrwmT55MrVq1ePXVVxk6dKgn4hRfFlYOmvcx2yqyUbSck7BnhdlWYQ0RERHxNXEqsHE+wSU5+KmnnuKf//wn3bp1Y/78+fTp04e7776bhQsX8vLLL9OnTx+CgoI8Fav4sjZ3w2/vwpr/wfH9UC7e6oi8y+7l4MiFcolQobrV0YiIiIiUjLuyoUrKn02JerymTZvG+++/z6effsrs2bPJy8sjNzeXFStWcPvttyvpkrOr0hyqtgFHjopsFMU9zLCdWXxaRERExJdoqOF5lSjx2rlzJ23atAGgadOmhIWFMXToUGz6oCjF0eaMIhsqNVqQe/0uze8SERERH+RaRPnoDsjOtDYWL1WixCsvL4/Q0FD37eDgYMqVU9lrKaamN0FYDBzeAlt+tDoa7+F0qrCGiIiI+LbIihBR0Wyr16tIJZrj5XQ6SU1NJSwsDIBTp07xwAMPEBUVVeC4zz//vPQiFP8RGgXNb4XF/4Ylk6HOFVZH5B0Ob4HMAxAUClVaWB2NiIiIyIWJq2e+TD64wUwzkQJKlHgNGDCgwO0777yzVIORANDmbpN4rf0Gju+DcpWtjsh6rmGGVVpCcJiloYiIiIhcMFfidUA9XkUpUeI1ebJKgctFSmwK1dqZxYKXfQCdh1kdkfXcwwxVRl5ERER8mCobnlOJ5niJlApXkY2l76nIBsCOxeZaiZeIiIj4MndlQ63lVRSfSbyuv/56qlevTnh4OFWqVKF///7s3r27wDErV66kc+fOhIeHk5yczIsvvljoPNOmTaNhw4aEh4fTrFkzZsyYUVYvQVya3Ahh5eHwVtj8vdXRWOtUBuz73WxXU+IlIiIiPsxV2fDARlM8TArwmcTriiuu4JNPPmHdunV89tlnbNq0iVtuucV9f0ZGBj169KBGjRosWbKEl156iTFjxvDOO++4j5k/fz59+/Zl4MCBLFu2jN69e9O7d29Wr15txUsKXKGR0OJ2s70kwIev7loCTodZNDmmitXRiIiIiFy42JpgD4acE5Cx+7yHBxqfSbyGDh3KJZdcQo0aNejUqRMjR45k4cKF5OTkAPDhhx+SnZ3Nu+++S5MmTbj99tt5+OGHefnll93neO211+jZsycjRoygUaNGPPvss7Ru3ZpJkyZZ9bICV5tUc712BhxLtzQUS2n9LhEREfEXQSEm+QINNyxCiYpreItDhw7x4Ycf0qlTJ0JCQgBYsGABXbp0KbDOWEpKCuPHj+fw4cPExsayYMEChg0rWMwhJSWF6dOnn/W5srKyyMrKct/OyMgAICcnx530WckVgzfEUiIV6xFUrT32nYvI++09HJf5R5GNkrZH0PZfsQN5Vdrg8LU29BE++zvip9Qe3kdt4n3UJt5F7VEyQRXrYD+4kby963AkX+qR5/CmNilJDD6VeD3++ONMmjSJzMxMLrnkEr7++mv3fenp6dSqVavA8QkJCe77YmNjSU9Pd+8785j09LP3uIwbN46xY8cW2j979mwiIyMv5uWUqrS0NKtDKLFke0tas4isBe+QdrQ+2HymA/a8itUeTge9ti3ADszbls3R/Zpv6Em++Dviz9Qe3kdt4n3UJt5F7VE8jY8GUQ/YtjSNVfsSPfpc3tAmmZmZxT7W0sRr5MiRjB8//pzHrFmzhoYNGwIwYsQIBg4cyLZt2xg7dix33XUXX3/9NTabzWMxjho1qkAvWUZGBsnJyfTo0YOYmBiPPW9x5eTkkJaWRvfu3d29fz4j5wqcr39C5KkDXNMwAmedq6yO6KKVqD32ryVkeSbOkEguvel+MyZaSp1P/474IbWH91GbeB+1iXdRe5SMbfkh+GYGNaNzSe7VyyPP4U1t4hoNVxyWftIbPnw4qamp5zymdu3a7u24uDji4uKoX78+jRo1Ijk5mYULF9KxY0cSExPZu3dvgce6bicmJrqvizrGdX9RwsLCCAsrvKhtSEiI5Q19Jm+Lp1hCQqBFX/j1LYKX/wca9rQ6olJTrPbYsxQAW9U2hIRFlEFUgc0nf0f8mNrD+6hNvI/axLuoPYqpsukwsR/chN3D/1/e0CYleX5LE6/4+Hji4+Mv6LGO0+s/ueZfdezYkSeffJKcnBz3f0BaWhoNGjQgNjbWfcycOXMYMmSI+zxpaWl07NjxIl6FXJS2d8Ovb8G6mZCxJ7Aq+7kLa6iMvIiIiPgJV0n5o9shO9NUsxbAR6oa/vrrr0yaNInly5ezbds25s6dS9++falTp447abrjjjsIDQ1l4MCB/P7770ydOpXXXnutwDDBRx55hFmzZjFx4kTWrl3LmDFj+O233xg8eLBVL03iG0D1TuDMg2X/sTqasrXjV3OtioYiIiLiL6IqQYTp9ODQJmtj8TI+kXhFRkby+eefc9VVV9GgQQMGDhxI8+bN+fHHH93DAMuXL8/s2bPZsmULbdq0Yfjw4YwePZr777/ffZ5OnTrx0Ucf8c4779CiRQs+/fRTpk+fTtOmTa16aQKm1wvgl9dh23xrYykrmYfyy6xWa2dtLCIiIiKlqVI9c31AJeXP5BOz+Zs1a8bcuXPPe1zz5s2ZN2/eOY/p06cPffr0Ka3QpDQ07g1L34et8+A/N0Hfj6DOlVZH5Vk7F5vrSvUgsqK1sYiIiIiUprj6sHOREq8/8YkeL/FzwaHQbxrU7Q65J+Gj22DtN1ZH5VkaZigiIiL+Kq6uudYiygUo8RLvEBIBt38Eja6HvGyY2h9WfWp1VJ6jwhoiIiLirzTUsEhKvMR7BIfCLZOh+e2m2MZn95ohiP4mLxd2LTHbSrxERETE37gqGx7cCE6ntbF4ESVe4l2CgqH3W9D2HsAJXz0EC9+yOqrStXc15GRCWHmIa2B1NCIiIiKlK7Ym2IIg+zgc22N1NF5DiZd4H7sdrnkZOp4u8z9rJPw0wdqYSpN7mGE781pFRERE/ElwqEm+QMMNz6BPfeKdbDbo8Rx0HWVuz30WvhvrH93VKqwhIiIi/s413PDAemvj8CJKvMR72WzQdSR0f9bc/vllmPk4OBzWxnWxdp7u8dL6XSIiIuKv3JUNN1obhxdR4iXe79KH4ZqJZnvRP+F/D4Ejz9qYLlTGHjiyHWx2qNrG6mhEREREPEOVDQtR4iW+od290Pttk7As+8BUPMzLsTqqknP1dlVuAuEx1sYiIiIi4inuoYZKvFyUeInvaNkX+kwBewj8/rlZ6yvnlNVRlcyZhTVERERE/FXc6R6vozsg56S1sXgJJV7iWxrfYBZaDg6H9TPhv7dB9gmroyo+d+KlwhoiIiLixyIrQXgFwAkHN1kdjVdQ4iW+p34P6DcNQqJg8w/wn5vg1FGrozq/nFOwZ7nZ1sLJIiIi4s9sNlU2/BMlXuKbanWBu740ixDvWAjvXQ+Zh6yO6tz2rIC8bIiKh9haVkcjIiIi4lmu4YaqbAgo8RJfltwOUv9nurL3LIcp18CxvVZHdXbuMvLtzbdAIiIiIv6s0umS8iqwASjxEl9XpQXcPROiq8C+P2ByTziyw+qoiuZeOFnDDEVERCQAuIYaHlTiBUq8xB/ENzDJV4XqcGgzTL7a+yZxOp0qrCEiIiKBJe6MtbycTmtj8QJKvMQ/VKxlkq9KdU3Z0slXw741VkeV78h2OL4X7MGQ1NLqaEREREQ8L7YW2IIg+zgcS7c6Gssp8RL/Ub6aSb4qNzFJzuResHuZ1VEZrt6uKi0gJMLaWERERETKQnAoxNY02xpuqMRL/Ey5ypD6NSS1hpOHTLXD7QutjuqM+V0aZigiIiIBxD3cUCXllXiJ/4msaErN17gUsjLgPzea9b6spMIaIiIiEojclQ1VUl6Jl/in8Bjo9ynUuRJyMuHDW2HdLGtiyToOe38329WUeImIiEgAUWVDNyVe4r9CI6Hvx9DwWsjLgqn9YPXnZR/H7qXgzIOYalC+atk/v4iIiIhVNNTQTYmX+LfgMOgzBZr1AUcufDYQln1QtjFomKGIiIgEqkqnE68jOyDnpLWxWEyJl/i/oBC48Z/QegA4HfDlIFj0r7J7/h2LzbUSLxEREQk0UXEQXh5wmvVWA5gSLwkM9iC47jW45K/m9oxH4edXPP+8DgfsdC2crMRLREREAozNlj/PK8CHGyrxksBhs0HKC9DlMXP7uzEw9znPrqR+cCOcPAzBEZDY3HPPIyIiIuKtXMMNA7yyoRIvCSw2G1z5JHQbY27/9BJ8+4Tnki/X/K6qrc2QRxEREZFAE3e6pHyAVzZU4iWB6bKh0GuC2V74D/jfI+DIK/3ncQ0zrNau9M8tIiIi4gs01BBQ4iWBrP19cMM/wGaHpe/BF3+BvJzSfY4drvldHUr3vCIiIiK+4syhhp6c4uHllHhJYGvVD27+P7AHw6ppMC0VcrNK59wnD8P+tWZbhTVEREQkUFWsZb7ozj4Gx/daHY1llHiJNL0JbvsQgsJg7dfw39shO/Piz7tzibmuWNuUUhUREREJRMFhEFvTbAfwcEMlXiIADXpCv08gJBI2zYUPboZTGRd3TvfCyRpmKCIiIgHOPdwwcAtsKPEScandFfp/AWExsH0+vH8DZB668PO5Ey8NMxQREZEAF3c68ToYuCXllXiJnKn6JTDgfxBREXYvhSnXwvF9JT+PIw92nR5qWE2Jl4iIiAQ4V+KloYYi4pbUEu6eAeUSYN/vMPlqOLqrZOfYvwayj0NoNFRu5JEwRURERHyGhhoq8RIpUuVGcPdMKJ9susQn94RDm4v9cLt7/a62YA/yUJAiIiIiPsLV43VkO+ScsjYWiyjxEjmbSnVM8lWxjvkjMbkX7F9XrIfadi42GyqsISIiIgJR8RBeHnDCoU1WR2MJJV4i51Ih2SRflRvDsT1m2OGeled9mG3Xb2YjuZ2HAxQRERHxATZbwA83VOIlcj7RCZD6DVRpCZkH4b1rYcfisx4elnMU2+EtgA2qti2zMEVERES8mruyoRIvETmbyIow4CtIvgROHTWl5rf8VOShsSdOl0mt3AgiKpRdjCIiIiLeLE49XiJSHOHlof/nZr2vnBPwYR9YP7vQYRVPnP5jUk3DDEVERETcNNRQRIotNAr6ToUGvSD3FHx8B/zxZYFDKrp6vFRYQ0RERCTfmYsoO53WxmIBJV4iJRUSDre+D01uAkcOTEuF5f819+VlUyFzi9lW4iUiIiKSr2JtsNkhKwOO77U6mjIXbHUAIj4pKARu/jeERsKyD2D6A5CTiS2+CUHOHJwRFbFVqmN1lCIiIiLeIzgMKtSAw1vMcMPoRKsjKlPq8RK5UPYguO4N6PCAuf3NMOyzRwHgrNrWlE0VERERkXwBXNlQiZfIxbDboeff4bJh5ubupQA4q7W3MioRERER7xRX31wHYIENJV4iF8tmg25Pw5VPuXc5k5V4iYiIiBRSqa65DsDES3O8REpLl0fJjanGxoWzqJPc0epoRERERLxPAA81VOIlUoqcTW5m3bYI6mh+l4iIiEhhrqGGR7ZDzilTLTpAaKihiIiIiIiUjah4CCsPTgcc2mx1NGVKiZeIiIiIiJQNmw3iTs/zCrDhhkq8RERERESk7FQ6Pc8rwAps+FzilZWVRcuWLbHZbCxfvrzAfStXrqRz586Eh4eTnJzMiy++WOjx06ZNo2HDhoSHh9OsWTNmzJhRRpGLiIiIiIi7wIYSL+/22GOPkZSUVGh/RkYGPXr0oEaNGixZsoSXXnqJMWPG8M4777iPmT9/Pn379mXgwIEsW7aM3r1707t3b1avXl2WL0FEREREJHAFaGVDn0q8Zs6cyezZs5kwYUKh+z788EOys7N59913adKkCbfffjsPP/wwL7/8svuY1157jZ49ezJixAgaNWrEs88+S+vWrZk0aVJZvgwRERERkcDlHmq4EZxOa2MpQz5TTn7v3r3cd999TJ8+ncjIyEL3L1iwgC5duhAaGurel5KSwvjx4zl8+DCxsbEsWLCAYcOGFXhcSkoK06dPP+vzZmVlkZWV5b6dkZEBQE5ODjk5ORf5qi6eKwZviEXUHt5IbeJd1B7eR23ifdQm3kXt4QExyQTb7NiyjpJzZBeUSyjRw72pTUoSg08kXk6nk9TUVB544AHatm3L1q1bCx2Tnp5OrVq1CuxLSEhw3xcbG0t6erp735nHpKenn/W5x40bx9ixYwvtnz17dpEJoFXS0tKsDkHOoPbwPmoT76L28D5qE++jNvEuao/S1S0kjqjsffz6zYccjG54QefwhjbJzMws9rGWJl4jR45k/Pjx5zxmzZo1zJ49m2PHjjFq1KgyiizfqFGjCvSSZWRkkJycTI8ePYiJiSnzeP4sJyeHtLQ0unfvTkhIiNXhBDy1h/dRm3gXtYf3UZt4H7WJd1F7eEZQxvuw6TsuqVcJZ+teJXqsN7WJazRccViaeA0fPpzU1NRzHlO7dm3mzp3LggULCAsLK3Bf27Zt6devH++99x6JiYns3bu3wP2u24mJie7roo5x3V+UsLCwQs8LEBISYnlDn8nb4gl0ag/vozbxLmoP76M28T5qE++i9ihl8Q1g03cEH94MF/j/6g1tUpLntzTxio+PJz4+/rzHvf766zz33HPu27t37yYlJYWpU6fSoUMHADp27MiTTz5JTk6O+z8gLS2NBg0aEBsb6z5mzpw5DBkyxH2utLQ0OnbsWIqvSkREREREzikAF1H2iTle1atXL3C7XLlyANSpU4dq1aoBcMcddzB27FgGDhzI448/zurVq3nttdd45ZVX3I975JFHuPzyy5k4cSLXXHMNH3/8Mb/99luBkvMiIiIiIuJhAbiIsk+Vkz+X8uXLM3v2bLZs2UKbNm0YPnw4o0eP5v7773cf06lTJz766CPeeecdWrRowaeffsr06dNp2rSphZGLiIiIiASYuPrm+sg2yM0697F+wid6vP6sZs2aOIuo+d+8eXPmzZt3zsf26dOHPn36eCo0ERERERE5n3KVISwGsjLg0Gao3MjqiDzOb3q8RERERETER9hsUOn0PK8AGW6oxEtERERERMqea7jhgfXWxlFGlHiJiIiIiEjZc1c23GhtHGVEiZeIiIiIiJS9AKtsqMRLRERERETKnnuo4QYoonCev1HiJSIiIiIiZa9ibcAGWUfhxH6ro/E4JV4iIiIiIlL2QsKhQnWzHQDDDZV4iYiIiIiINQKosqESLxERERERsUbc6QIbAVDZUImXiIiIiIhYI4AWUVbiJSIiIiIi1tBQQxEREREREQ9zDTU8sg1ys6yNxcOUeImIiIiIiDXKJUBoNDgdcGiL1dF4lBIvERERERGxhs2W3+vl58MNlXiJiIiIiIh13JUN/bvAhhIvERERERGxTiVXj5d/l5RX4iUiIiIiItbRUEMREREREREPO3OoodNpbSwepMRLRERERESsU7E2YINTR+HEAauj8RglXiIiIiIiYp2QCKiQbLb9eLihEi8REREREbFWXH1z7ceVDZV4iYiIiIiItdyVDZV4iYiIiIiIeEZcXXN90H9LyivxEhERERERa7mGGmqOl4iIiIiIiIe4hhoe3ga52dbG4iFKvERERERExFrRiRBaDpx5cHiL1dF4hBIvERERERGxls2Wv5Cynw43VOIlIiIiIiLW8/PKhkq8RERERETEeq4eLz+tbKjES0RERERErKehhiIiIiIiIh525lBDp9PaWDxAiZeIiIiIiFivUh3ABqeOQOZBq6MpdUq8RERERETEeiERUCHZbPvhcEMlXiIiIiIi4h38uLKhEi8REREREfEO7sqGSrxEREREREQ8I049XiIiIiIiIp6loYYiIiIiIiIe5urxOrwVcrMtDaW0KfESERERERHvEF0FQsuBMw8Ob7E6mlKlxEtERERERLyDzQaV6pptPxtuqMRLRERERES8h59WNlTiJSIiIiIi3iOuvrlWj5eIiIiIiIiHaKihiIiIiIiIh7nX8loPTqe1sZQiJV4iIiIiIuI9KtYBbHDqCGQetDqaUqPES0REREREvEdoJJRPNtt+NNxQiZeIiIiIiHiXuNPzvPyosqESLxERERER8S7uyobrrY2jFCnxEhERERER7+KubLjR2jhKkRIvERERERHxLn64iLISLxERERER8S6VTideh7ZAbra1sZQSJV4iIiIiIuJdYpIgJAqceXB4q9XRlAqfSbxq1qyJzWYrcPn73/9e4JiVK1fSuXNnwsPDSU5O5sUXXyx0nmnTptGwYUPCw8Np1qwZM2bMKKuXICIiIiIixWGz+V1lQ59JvACeeeYZ9uzZ47489NBD7vsyMjLo0aMHNWrUYMmSJbz00kuMGTOGd955x33M/Pnz6du3LwMHDmTZsmX07t2b3r17s3r1aitejoiIiIiInI1ruKGfVDYMtjqAkoiOjiYxMbHI+z788EOys7N59913CQ0NpUmTJixfvpyXX36Z+++/H4DXXnuNnj17MmLECACeffZZ0tLSmDRpEm+//XaR583KyiIrK8t9OyMjA4CcnBxycnLOGmteXh65ubk4nc4Leq3FlZubS3BwMMePHyc42Kea0y+drz1sNhvBwcEEBQVZEF1gcv2enuv3VcqO2sP7qE28j9rEu6g9rGOvWIcgwLFvPXln/P97U5uUJAab09OZQSmpWbMmp06dIicnh+rVq3PHHXcwdOhQ94fbu+66i4yMDKZPn+5+zPfff8+VV17JoUOHiI2NpXr16gwbNowhQ4a4j3n66aeZPn06K1asKPJ5x4wZw9ixYwvt/+ijj4iMjCzyMdHR0URHR2O3+1SHopQRh8PBsWPHOHbsmNWhiIiIiHitpMMLabf1HxyMqsfP9Z+yOpwiZWZmcscdd3D06FFiYmLOeazPdJE8/PDDtG7dmooVKzJ//nxGjRrFnj17ePnllwFIT0+nVq1aBR6TkJDgvi82Npb09HT3vjOPSU9PP+vzjho1imHDhrlvZ2RkkJycTI8ePYr8z927dy8ZGRnEx8cTGRmJzWa74NdcHE6nkxMnThAVFeXx55LzO197OJ1OMjMz2b9/P/Xr1y/08yilLycnh7S0NLp3705ISIjV4QQ8tYf3UZt4H7WJd1F7WCg9Gf7vH1R0HKRXr17u3d7UJq7RcMVhaeI1cuRIxo8ff85j1qxZQ8OGDQskP82bNyc0NJS//OUvjBs3jrCwMI/FGBYWVuT5Q0JCCjV0Xl4ex44dIyEhgUqVKnkspjM5HA5ycnKIiIhQD5sXKE57REVFYbfb2bdvH1WqVNGwwzJS1O+sWEft4X3UJt5HbeJd1B4WSGgIgO3kIUKyMyCq4Odrb2iTkjy/pYnX8OHDSU1NPecxtWvXLnJ/hw4dyM3NZevWrTRo0IDExET27t1b4BjXbde8sLMdc7Z5YyXlGuN5tiGIIi6un5GcnBwlXiIiIiJFCY2E8slwdIepbBhVNh0bnmJp4hUfH098fPwFPXb58uXY7XYqV64MQMeOHXnyySfJyclxZ55paWk0aNCA2NhY9zFz5swpMMcrLS2Njh07XtwL+RMN+ZPz0c+IiIiISDFUqmsSrwMboPolVkdzUXxibNqCBQt49dVXWbFiBZs3b+bDDz9k6NCh3Hnnne6k6o477iA0NJSBAwfy+++/M3XqVF577bUCQxQfeeQRZs2axcSJE1m7di1jxozht99+Y/DgwVa9NBEREREROZu4+ubaD0rK+0TiFRYWxscff8zll19OkyZNeP755xk6dGiBNbrKly/P7Nmz2bJlC23atGH48OGMHj3aXUoeoFOnTnz00Ue88847tGjRgk8//ZTp06fTtGlTK16W36tZsyavvvqq1WGIiIiIiK+KO72W18GN1sZRCnyiqmHr1q1ZuHDheY9r3rw58+bNO+cxffr0oU+fPqUVml8437C3p59+mjFjxpT4vIsXLyYqKuoCozK6du3Kjz/+CJgEvHr16tx9992MHDnSHfdf//pXNmzYwM6dO+nWrRtvvPHGRT2niIiIiHiJSnXN9YEN1sZRCnwi8RLP2rNnj3t76tSpjB49mnXr1rn3lStXzr3tdDrJy8sr1mLNFzp/78/uu+8+nnnmGbKyspg7dy73338/FSpU4MEHHwTg1VdfJTQ0lKysLBISEnjhhReIjo4ulecWEREREQu5hhoe3gJ5ORDku5UlfWKooS9zOp1kZud69HIyO6/I/cVdGzsxMdF9KV++PDabzX177dq1REdHM3PmTNq0aUNYWBg///wzmzZt4oYbbiAhIYFy5crRrl07vvvuuwLn/fNQQ5vNxr///W9uvPFGIiMjqVevHl999dV544uMjCQxMZEaNWpw991307x5c9LS0tz3h4aGkpuby1//+lclXSIiIiL+JCYJQqLAkQuHt1odzUVRj5eHnczJo/Hoby157j+eSSEytHSaeOTIkUyYMIHatWsTGxvLjh076NWrF88//zxhYWG8//77XHfddaxbt47q1auf9Txjx47lxRdf5KWXXuKNN96gX79+bNu2jYoVK543BqfTyc8//8zatWupV6+ee/+ePXu47777GDBggIaRioiIiPgTmw0q1YH0lWa4YVy98z/GS6nHS4rlmWeeoXv37tSpU4eKFSvSokUL/vKXv9C0aVPq1avHs88+S506dc7bg5Wamkrfvn2pW7cuL7zwAsePH2fRokXnfMw//vEPypUrR1hYGF26dMHhcPDwww+77+/ZsycrV65k4sSJXHLJJWzevLlUXrOIiIiIeAE/qWyoHi8PiwgJ4o9nUjx2fofDwbGMY0THRGO3F8yjI0JKb2Hetm3bFrh9/PhxxowZwzfffMOePXvIzc3l5MmTbN++/Zznad68uXs7KiqKmJgY9u3bd87H9OvXjyeffJLDhw/z9NNP06lTJzp16uS+f8WKFRfwikRERETEJ7grG/p2gQ0lXh5ms9lKbbhfURwOB7mhQUSGBhdKvErTn6sTPvroo6SlpTFhwgTq1q1LREQEt9xyC9nZ2ec8j2txaxebzYbD4TjnY8qXL0/duqaizSeffELdunW55JJL6Nat2wW8EhERERHxKe7Khr5dUl6Jl1yQX375hdTUVG688UbA9IBt3brV489brlw5HnnkER599FGWLVt23lL4IiIiIuLj/GSooeZ4yQWpV68en3/+OcuXL2fFihXccccd5+25Ki1/+ctfWL9+PZ999lmZPJ+IiIiIWKhSHXN98hBkHrI2lougxEsuyMsvv0xsbCydOnXiuuuuIyUlhdatW5fJc1esWJG77rqLMWPGlFmyJyIiIiIWCY2CmGpm24cXUtZQQykgNTWV1NRU9+2uXbsWuR5YzZo1mTt3boF9gwYNKnD7z0MPizrPkSNHzhnPDz/8UOT+t99++5yPExERERE/ElcPMnaa4YZVyubL/tKmHi8REREREfFuflDZUImXiIiIiIh4t0qnEy8frmyoxEtERERERLybq8fLhysbKvESERERERHv5kq8Dm+BvBxrY7lASrxERERERMS7RSdBSCQ4cuHINqujuSBKvERERERExLvZ7VCpLgA2Hy2wocRLRERERES83+nhhraDvllgQ4mXiIiIiIh4v0pKvERERERERDzLvZaXEi8JcF27dmXIkCHu2zVr1uTVV18952NsNhvTp0+/6OcurfOIiIiIiJdyDTU8pMRLfNR1111Hz549i7xv3rx52Gw2Vq5cWeLzLl68mPvvv/9iwytgzJgxtGzZstD+PXv2cPXVV5fqc/3ZlClTsNls2Gw27HY7VapU4bbbbmP79u3uY7788ktuvPFGOnbsSKdOndiyZYtHYxIREREJGK7iGpkHCck9ZnEwJafESxg4cCBpaWns3Lmz0H2TJ0+mbdu2NG/evMTnjY+PJzIysjRCPK/ExETCwsI8/jwxMTHs2bOHXbt28dlnn7Fu3Tr69Onjvv/qq6/miy++YMGCBTRu3JhZs2Z5PCYRERGRgBAaBTFVASh3Kt3iYEpOiZenOZ2QfcKzl5zMovc7ncUK8dprryU+Pp4pU6YU2H/8+HGmTZvGwIEDOXjwIH379qVq1apERkbSrFkz/vvf/57zvH8earhhwwa6dOlCeHg4jRs3Ji0trdBjHn/8cerXr09kZCS1a9fmqaeeIifHLJI3ZcoUxo4dy4oVK9w9T66Y/zzUcNWqVVx55ZVERERQqVIl7r//fo4fP+6+PzU1ld69ezNhwgSqVKlCpUqVGDRokPu5zsZms5GYmEiVKlXo1KkTAwcOZNGiRWRkZAAQGhoKwDfffMPOnTu5++67z3k+ERERESmB08MNo7P2WBxIyQVbHYDfy8mEF5I8dno7UOFsdz6x23wzcB7BwcHcddddTJkyhSeffBKbzQbAtGnTyMvLo2/fvhw/fpw2bdrw+OOPExMTwzfffEP//v2pU6cO7du3P+9zOBwObrrpJhISEvj11185evRogflgLtHR0UyZMoWkpCRWrVrFfffdR3R0NI899hi33XYbq1evZtasWXz33XcAlC9fvtA5Tpw4QUpKCh07dmTx4sXs27ePe++9l8GDBxdILr///nuqVKnC999/z8aNG7ntttto2bIl991333lfD8C+ffv44osvCAoKIigoyP06J0yYwJ49e5g+fTrh4eHFOpeIiIiIFEOlerD5B8qd8r3ESz1eAsA999zDpk2b+PHHH937Jk+ezM0330z58uWpWrUqjz76KC1btqR27do89NBD9OzZk08++aRY5//uu+9Yu3Yt77//Pi1atKBLly688MILhY7729/+RqdOnahZsybXXXcdjz76qPs5IiIiKFeuHMHBwSQmJpKYmEhEREShc3z00UecOnWK999/n6ZNm3LllVcyadIk/vOf/7B37173cbGxsUyaNImGDRty7bXXcs011zBnzpxzvo6jR49Srlw5oqKiSEhI4Pvvv2fQoEFERZkE9/XXX2fixIn8/vvvdO3alTfeeKNY/z8iIiIiUgyne7zKqcdLCgmJND1PHuJwOMg4doyY6Gjs9j/l0SHFn1/VsGFDOnXqxLvvvkvXrl3ZuHEj8+bN45lnngEgLy+PF154gU8++YRdu3aRnZ1NVlZWsedwrVmzhuTkZJKS8nv/OnbsWOi4qVOn8vrrr7Np0yaOHz9Obm4uMTExxX4drudq0aKFOxkCuPTSS3E4HKxbt46EhAQAmjRp4u6pAqhSpQqrVq0657mjo6NZunQpOTk5zJw5kw8//JDnn3/eff+QIUO45557iImJKdweIiIiInJxXImXD/Z4KfHyNJutWMP9LpjDASF55jku8oP+wIEDeeihh3jzzTeZPHkyderU4fLLLwfgpZde4rXXXuPVV1+lWbNmREVFMWTIELKzs0vjVQCwYMEC+vXrx9ixY0lJSaF8+fJ8/PHHTJw4sdSe40whISEFbttsNhwOxzkfY7fbqVvXVNRp1KgRmzZt4sEHH+Q///mPR2IUERERkTOcXkQ5KnsfeY5cIOTcx3sRfSUvbrfeeit2u52PPvqI999/n3vuucc93+uXX37hhhtu4M4776RFixbUrl2b9evXF/vcjRo1YseOHezZk//txMKFCwscM3/+fGrUqMGTTz5J27ZtqVevHtu2bStwTGhoKHl5eed9rhUrVnDixAn3vl9++QW73U6DBg2KHXNxjBw5kqlTp7J06dJSPa+IiIiIFCGmKs7gCOzOPDiy7fzHexElXuJWrlw5brvtNkaNGsWePXtITU1131evXj3S0tKYP38+a9as4S9/+UuB+VLn061bN+rXr8+AAQNYsWIF8+bN48knnyxwTL169di+fTsff/wxmzZt4vXXX+eLL74ocEzNmjXZsmULy5cv58CBA2RlZRV6rn79+hEeHs6AAQNYvXo133//PQ899BD9+/d3DzMsLcnJydx4442MHj26VM8rIiIiIkWw28m78V/8WH8MxFSzOpoSUeIlBQwcOJDDhw+TkpJSYD7W3/72N1q3bk1KSgpdu3YlMTGR3r17F/u8drudL774gpMnT9K+fXvuvffeAnOjAK6//nqGDh3K4MGDadmyJfPnz+epp54qcMzNN99Mz549ueKKK4iPjy+ypH1kZCTffvsthw4dol27dtxyyy1cddVVTJo0qWT/GcU0dOhQvvnmGxYtWuSR84uIiIhIPmf9nhyJqg3Bnl/DtTTZnM5iLvYkAGRkZFC+fHmOHj1aqOjDqVOn2LJlC7Vq1SqzMuIOh4OMjAwVc/ASxW0PK35WAlVOTg4zZsygV69eheb1SdlTe3gftYn3UZt4F7WH9/GmNjlXbvBn+qQuIiIiIiLiYUq8REREREREPEyJl4iIiIiIiIcp8RIREREREfEwJV4eoHolcj76GREREREJLEq8SpGrqkpmZqbFkYi3c/2MWF2JR0RERETKRrDVAfiToKAgKlSowL59+wCznpTNZvPoczocDrKzszl16pTKyXuB87WH0+kkMzOTffv2UaFCBYKCgiyIUkRERETKmhKvUpaYmAjgTr48zel0cvLkSSIiIjye5Mn5Fbc9KlSo4P5ZERERERH/p8SrlNlsNqpUqULlypXJycnx+PPl5OTw008/0aVLFw1b8wLFaY+QkBD1dImIiIgEGCVeHhIUFFQmH66DgoLIzc0lPDxciZcXUHuIiIiISFE0KUhERERERMTDlHiJiIiIiIh4mBIvERERERERD9McrxJyLXybkZFhcSRGTk4OmZmZZGRkaE6RF1B7eB+1iXdRe3gftYn3UZt4F7WH9/GmNnHlBK4c4VyUeJXQsWPHAEhOTrY4EhERERER8QbHjh2jfPny5zzG5ixOeiZuDoeD3bt3Ex0d7RXrZmVkZJCcnMyOHTuIiYmxOpyAp/bwPmoT76L28D5qE++jNvEuag/v401t4nQ6OXbsGElJSdjt557FpR6vErLb7VSrVs3qMAqJiYmx/AdP8qk9vI/axLuoPbyP2sT7qE28i9rD+3hLm5yvp8tFxTVEREREREQ8TImXiIiIiIiIhynx8nFhYWE8/fTThIWFWR2KoPbwRmoT76L28D5qE++jNvEuag/v46ttouIaIiIiIiIiHqYeLxEREREREQ9T4iUiIiIiIuJhSrxEREREREQ8TImXiIiIiIiIhynx8mFvvvkmNWvWJDw8nA4dOrBo0SKrQwpY48aNo127dkRHR1O5cmV69+7NunXrrA5LTvv73/+OzWZjyJAhVocS0Hbt2sWdd95JpUqViIiIoFmzZvz2229WhxWw8vLyeOqpp6hVqxYRERHUqVOHZ599FtXcKjs//fQT1113HUlJSdhsNqZPn17gfqfTyejRo6lSpQoRERF069aNDRs2WBNsADhXe+Tk5PD444/TrFkzoqKiSEpK4q677mL37t3WBRwAzvc7cqYHHngAm83Gq6++WmbxlZQSLx81depUhg0bxtNPP83SpUtp0aIFKSkp7Nu3z+rQAtKPP/7IoEGDWLhwIWlpaeTk5NCjRw9OnDhhdWgBb/Hixfzzn/+kefPmVocS0A4fPsyll15KSEgIM2fO5I8//mDixInExsZaHVrAGj9+PG+99RaTJk1izZo1jB8/nhdffJE33njD6tACxokTJ2jRogVvvvlmkfe/+OKLvP7667z99tv8+uuvREVFkZKSwqlTp8o40sBwrvbIzMxk6dKlPPXUUyxdupTPP/+cdevWcf3111sQaeA43++IyxdffMHChQtJSkoqo8gukFN8Uvv27Z2DBg1y387Ly3MmJSU5x40bZ2FU4rJv3z4n4Pzxxx+tDiWgHTt2zFmvXj1nWlqa8/LLL3c+8sgjVocUsB5//HHnZZddZnUYcoZrrrnGec899xTYd9NNNzn79etnUUSBDXB+8cUX7tsOh8OZmJjofOmll9z7jhw54gwLC3P+97//tSDCwPLn9ijKokWLnIBz27ZtZRNUgDtbm+zcudNZtWpV5+rVq501atRwvvLKK2UeW3Gpx8sHZWdns2TJErp16+beZ7fb6datGwsWLLAwMnE5evQoABUrVrQ4ksA2aNAgrrnmmgK/K2KNr776irZt29KnTx8qV65Mq1at+Ne//mV1WAGtU6dOzJkzh/Xr1wOwYsUKfv75Z66++mqLIxOALVu2kJ6eXuDvV/ny5enQoYPe673E0aNHsdlsVKhQwepQApbD4aB///6MGDGCJk2aWB3OeQVbHYCU3IEDB8jLyyMhIaHA/oSEBNauXWtRVOLicDgYMmQIl156KU2bNrU6nID18ccfs3TpUhYvXmx1KAJs3ryZt956i2HDhvHEE0+wePFiHn74YUJDQxkwYIDV4QWkkSNHkpGRQcOGDQkKCiIvL4/nn3+efv36WR2aAOnp6QBFvte77hPrnDp1iscff5y+ffsSExNjdTgBa/z48QQHB/Pwww9bHUqxKPESKWWDBg1i9erV/Pzzz1aHErB27NjBI488QlpaGuHh4VaHI5gvJNq2bcsLL7wAQKtWrVi9ejVvv/22Ei+LfPLJJ3z44Yd89NFHNGnShOXLlzNkyBCSkpLUJiLnkJOTw6233orT6eStt96yOpyAtWTJEl577TWWLl2KzWazOpxi0VBDHxQXF0dQUBB79+4tsH/v3r0kJiZaFJUADB48mK+//prvv/+eatWqWR1OwFqyZAn79u2jdevWBAcHExwczI8//sjrr79OcHAweXl5VocYcKpUqULjxo0L7GvUqBHbt2+3KCIZMWIEI0eO5Pbbb6dZs2b079+foUOHMm7cOKtDE3C/n+u93ru4kq5t27aRlpam3i4LzZs3j3379lG9enX3e/22bdsYPnw4NWvWtDq8Iinx8kGhoaG0adOGOXPmuPc5HA7mzJlDx44dLYwscDmdTgYPHswXX3zB3LlzqVWrltUhBbSrrrqKVatWsXz5cvelbdu29OvXj+XLlxMUFGR1iAHn0ksvLbTEwvr166lRo4ZFEUlmZiZ2e8GPAUFBQTgcDosikjPVqlWLxMTEAu/1GRkZ/Prrr3qvt4gr6dqwYQPfffcdlSpVsjqkgNa/f39WrlxZ4L0+KSmJESNG8O2331odXpE01NBHDRs2jAEDBtC2bVvat2/Pq6++yokTJ7j77rutDi0gDRo0iI8++ogvv/yS6Oho9/j78uXLExERYXF0gSc6OrrQ/LqoqCgqVaqkeXcWGTp0KJ06deKFF17g1ltvZdGiRbzzzju88847VocWsK677jqef/55qlevTpMmTVi2bBkvv/wy99xzj9WhBYzjx4+zceNG9+0tW7awfPlyKlasSPXq1RkyZAjPPfcc9erVo1atWjz11FMkJSXRu3dv64L2Y+dqjypVqnDLLbewdOlSvv76a/Ly8tzv9RUrViQ0NNSqsP3a+X5H/pz8hoSEkJiYSIMGDco61OKxuqyiXLg33njDWb16dWdoaKizffv2zoULF1odUsACirxMnjzZ6tDkNJWTt97//vc/Z9OmTZ1hYWHOhg0bOt955x2rQwpoGRkZzkceecRZvXp1Z3h4uLN27drOJ5980pmVlWV1aAHj+++/L/K9Y8CAAU6n05SUf+qpp5wJCQnOsLAw51VXXeVct26dtUH7sXO1x5YtW876Xv/9999bHbrfOt/vyJ95ezl5m9OpJepFREREREQ8SXO8REREREREPEyJl4iIiIiIiIcp8RIREREREfEwJV4iIiIiIiIepsRLRERERETEw5R4iYiIiIiIeJgSLxEREREREQ9T4iUiIiIiIuJhSrxERETKkM1mY/r06VaHISIiZUyJl4iIBIzU1FRsNluhS8+ePa0OTURE/Fyw1QGIiIiUpZ49ezJ58uQC+8LCwiyKRkREAoV6vEREJKCEhYWRmJhY4BIbGwuYYYBvvfUWV199NREREdSuXZtPP/20wONXrVrFlVdeSUREBJUqVeL+++/n+PHjBY559913adKkCWFhYVSpUoXBgwcXuP/AgQPceOONREZGUq9ePb766ivPvmgREbGcEi8REZEzPPXUU9x8882sWLGCfv36cfvtt7NmzRoATpw4QUpKCrGxsSxevJhp06bx3XffFUis3nrrLQYNGsT999/PqlWr+Oqrr6hbt26B5xg7diy33norK1eupFevXvTr149Dhw6V6esUEZGyZXM6nU6rgxARESkLqampfPDBB4SHhxfY/8QTT/DEE09gs9l44IEHeOutt9z3XXLJJbRu3Zp//OMf/Otf/+Lxxx9nx44dREVFATBjxgyuu+46du/eTUJCAlWrVuXuu+/mueeeKzIGm83G3/72N5599lnAJHPlypVj5syZmmsmIuLHNMdLREQCyhVXXFEgsQKoWLGie7tjx44F7uvYsSPLly8HYM2aNbRo0cKddAFceumlOBwO1q1bh81mY/fu3Vx11VXnjKF58+bu7aioKGJiYti3b9+FviQREfEBSrxERCSgREVFFRr6V1oiIiKKdVxISEiB2zabDYfD4YmQRETES2iOl4iIyBkWLlxY6HajRo0AaNSoEStWrODEiRPu+3/55RfsdjsNGjQgOjqamjVrMmfOnDKNWUREvJ96vEREJKBkZWWRnp5eYF9wcDBxcXEATJs2jbZt23LZZZfx4YcfsmjRIv7v//4PgH79+vH0008zYMAAxowZw/79+3nooYfo378/CQkJAIwZM4YHHniAypUrc/XVV3Ps2DF++eUXHnroobJ9oSIi4lWUeImISECZNWsWVapUKbCvQYMGrF27FjAVBz/++GP++te/UqVKFf773//SuHFjACIjI/n222955JFHaNeuHZGRkdx88828/PLL7nMNGDCAU6dO8corr/Doo48SFxfHLbfcUnYvUEREvJKqGoqIiJxms9n44osv6N27t9WhiIiIn9EcLxEREREREQ9T4iUiIiIiIuJhmuMlIiJymkbfi4iIp6jHS0RERERExMOUeImIiIiIiHiYEi8REREREREPU+IlIiIiIiLiYUq8REREREREPEyJl4iIiIiIiIcp8RIREREREfEwJV4iIiIiIiIe9v+Sa0nU2Xm0KQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from models import RNNModel\n",
    "model = RNNModel(input_size, hidden_size, output_size)\n",
    "\n",
    "print(\"Entrenando modelo RNN...\")\n",
    "# train_model(model_lstm, train_loader, test_loader, num_epochs, learning_rate)\n",
    "# RNN_preds, RNN_actuals = evaluate_model(model_lstm, test_loader)\n",
    "\n",
    "# # Guardar predicciones con Pickle\n",
    "# with open('lstm_preds.pkl', 'wb') as f:\n",
    "#     pickle.dump(lstm_preds, f)\n",
    "\n",
    "\n",
    "\n",
    "trained_model, preds, actuals,metrics = train_and_evaluate_model(\n",
    "    model, train_loader, test_loader, num_epochs, learning_rate\n",
    ")\n",
    "\n",
    "# # Guardar predicciones con Pickle\n",
    "# with open('lstm_preds.pkl', 'wb') as f:\n",
    "#     pickle.dump(lstm_preds, f)\n",
    "\n",
    "plot_metrics(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Portfolio Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bibliograpghy**\n",
    "\n",
    "- Wong, Albert, et al. “Short-Term Stock Price Forecasting Using Exogenous Variables and Machine Learning Algorithms.” ArXiv.org, 2023, arxiv.org/abs/2309.00618. Accessed 7 Dec. 2024.\n",
    "- Gabriel, and Marcel Otoboni. “Previs\\~Ao Dos Pre\\C{C}Os de Abertura, M\\'Inima E M\\'Axima de \\'Indices de Mercados Financeiros Usando a Associa\\C{C}\\~Ao de Redes Neurais LSTM.” ArXiv.org, 2021, arxiv.org/abs/2108.10065. Accessed 7 Dec. 2024.\n",
    "- Orsel, Ogulcan E., and Sasha S. Yamada. “Comparative Study of Machine Learning Models for Stock Price Prediction.” ArXiv.org, 31 Jan. 2022, arxiv.org/abs/2202.03156."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
